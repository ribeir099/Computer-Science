{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP ( Multi-layer Perceptron )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1881,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as libs que serão usadas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1882,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as funções que serão usadas no código\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>menopause</th>\n",
       "      <th>tumor-size</th>\n",
       "      <th>inv-nodes</th>\n",
       "      <th>node-caps</th>\n",
       "      <th>deg-malig</th>\n",
       "      <th>breast</th>\n",
       "      <th>breast-quad</th>\n",
       "      <th>irradiat</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40-49</td>\n",
       "      <td>premeno</td>\n",
       "      <td>15-19</td>\n",
       "      <td>0-2</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>right</td>\n",
       "      <td>left_up</td>\n",
       "      <td>no</td>\n",
       "      <td>recurrence-events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50-59</td>\n",
       "      <td>ge40</td>\n",
       "      <td>15-19</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>right</td>\n",
       "      <td>central</td>\n",
       "      <td>no</td>\n",
       "      <td>no-recurrence-events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50-59</td>\n",
       "      <td>ge40</td>\n",
       "      <td>35-39</td>\n",
       "      <td>0-2</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>left</td>\n",
       "      <td>left_low</td>\n",
       "      <td>no</td>\n",
       "      <td>recurrence-events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40-49</td>\n",
       "      <td>premeno</td>\n",
       "      <td>35-39</td>\n",
       "      <td>0-2</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>right</td>\n",
       "      <td>left_low</td>\n",
       "      <td>yes</td>\n",
       "      <td>no-recurrence-events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40-49</td>\n",
       "      <td>premeno</td>\n",
       "      <td>30-34</td>\n",
       "      <td>3-5</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>left</td>\n",
       "      <td>right_up</td>\n",
       "      <td>no</td>\n",
       "      <td>recurrence-events</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age menopause tumor-size inv-nodes node-caps  deg-malig breast  \\\n",
       "0  40-49   premeno      15-19       0-2       yes          3  right   \n",
       "1  50-59      ge40      15-19       0-2        no          1  right   \n",
       "2  50-59      ge40      35-39       0-2        no          2   left   \n",
       "3  40-49   premeno      35-39       0-2       yes          3  right   \n",
       "4  40-49   premeno      30-34       3-5       yes          2   left   \n",
       "\n",
       "  breast-quad irradiat                 Class  \n",
       "0     left_up       no     recurrence-events  \n",
       "1     central       no  no-recurrence-events  \n",
       "2    left_low       no     recurrence-events  \n",
       "3    left_low      yes  no-recurrence-events  \n",
       "4    right_up       no     recurrence-events  "
      ]
     },
     "execution_count": 1883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando a base de dados\n",
    "\n",
    "df = pd.read_csv('breast-cancer.csv', sep=',')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1884,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 286 entries, 0 to 285\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   age          286 non-null    object\n",
      " 1   menopause    286 non-null    object\n",
      " 2   tumor-size   286 non-null    object\n",
      " 3   inv-nodes    286 non-null    object\n",
      " 4   node-caps    286 non-null    object\n",
      " 5   deg-malig    286 non-null    int64 \n",
      " 6   breast       286 non-null    object\n",
      " 7   breast-quad  286 non-null    object\n",
      " 8   irradiat     286 non-null    object\n",
      " 9   Class        286 non-null    object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 22.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Verificando se possui valores nulos\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deg-malig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>286.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.048951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.738217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        deg-malig\n",
       "count  286.000000\n",
       "mean     2.048951\n",
       "std      0.738217\n",
       "min      1.000000\n",
       "25%      2.000000\n",
       "50%      2.000000\n",
       "75%      3.000000\n",
       "max      3.000000"
      ]
     },
     "execution_count": 1885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando a distribuicao dos dados\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1886,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: ['40-49', '50-59', '60-69', '30-39', '70-79', '20-29']\n",
      "menopause: ['premeno', 'ge40', 'lt40']\n",
      "tumor-size: ['15-19', '35-39', '30-34', '25-29', '40-44', '10-14', '0-4', '20-24', '45-49', '50-54', '5-9']\n",
      "inv-nodes: ['0-2', '3-5', '15-17', '6-8', '9-11', '24-26', '12-14']\n",
      "node-caps: ['yes', 'no', '?']\n",
      "deg-malig: [3, 1, 2]\n",
      "breast: ['right', 'left']\n",
      "breast-quad: ['left_up', 'central', 'left_low', 'right_up', 'right_low', '?']\n",
      "irradiat: ['no', 'yes']\n",
      "Class: ['recurrence-events', 'no-recurrence-events']\n"
     ]
    }
   ],
   "source": [
    "# Verificando os dados unicos das colunas\n",
    "\n",
    "def unique_values(df):\n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"{column}: {list(unique_values)}\")\n",
    "\n",
    "unique_values(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processando a base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277, 10)"
      ]
     },
     "execution_count": 1887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo dados indesejados\n",
    "\n",
    "df = df[df['node-caps'] != '?']\n",
    "df = df[df['breast-quad'] != '?']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1889,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "tumor-size: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
      "inv-nodes: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "node-caps: [1, 0]\n",
      "deg-malig: [3, 1, 2]\n",
      "breast: [1, 0]\n",
      "irradiat: [0, 1]\n",
      "Class: [1, 0]\n",
      "breast_quad_breast-quad_central: [0.0, 1.0]\n",
      "breast_quad_breast-quad_left_low: [0.0, 1.0]\n",
      "breast_quad_breast-quad_left_up: [1.0, 0.0]\n",
      "breast_quad_breast-quad_right_low: [0.0, 1.0]\n",
      "breast_quad_breast-quad_right_up: [0.0, 1.0]\n",
      "menopause_menopause_ge40: [0.0, 1.0]\n",
      "menopause_menopause_lt40: [0.0, 1.0]\n",
      "menopause_menopause_premeno: [1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Transformando os dados categóricos em numéricos\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['Class'] = label_encoder.fit_transform(df['Class'])\n",
    "df['node-caps'] = label_encoder.fit_transform(df['node-caps'])\n",
    "df['breast'] = label_encoder.fit_transform(df['breast'])\n",
    "df['irradiat'] = label_encoder.fit_transform(df['irradiat'])\n",
    "# df['breast-quad'] = label_encoder.fit_transform(df['breast-quad'])\n",
    "# df['menopause'] = label_encoder.fit_transform(df['menopause'])\n",
    "\n",
    "ordinal_encoder_age = OrdinalEncoder(categories=[df[\"age\"].unique()])\n",
    "ordinal_encoder_tumor = OrdinalEncoder(categories=[df[\"tumor-size\"].unique()])\n",
    "ordinal_encoder_nodes = OrdinalEncoder(categories=[df[\"inv-nodes\"].unique()])\n",
    "# ordinal_encoder_quad = OrdinalEncoder(categories=[df[\"breast-quad\"].unique()])\n",
    "# ordinal_encoder_menopause = OrdinalEncoder(categories=[df[\"menopause\"].unique()])\n",
    "\n",
    "df.loc[:, \"age\"] = ordinal_encoder_age.fit_transform(df.loc[:, \"age\"].values.reshape(-1, 1)).flatten()\n",
    "df.loc[:,\"tumor-size\"] = ordinal_encoder_tumor.fit_transform(df.loc[:, \"tumor-size\"].values.reshape(-1, 1)).flatten()\n",
    "df.loc[:,\"inv-nodes\"] = ordinal_encoder_nodes.fit_transform(df.loc[:, \"inv-nodes\"].values.reshape(-1, 1)).flatten()\n",
    "# df.loc[:,\"breast-quad\"] = ordinal_encoder_quad.fit_transform(df.loc[:, \"breast-quad\"].values.reshape(-1, 1)).flatten()\n",
    "# df.loc[:,\"menopause\"] = ordinal_encoder_menopause.fit_transform(df.loc[:, \"menopause\"].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "onehot_encoder_breast_quad = OneHotEncoder(sparse=False)\n",
    "onehot_encoder_menopause = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Ajuste e transforme os atributos usando o OneHotEncoder\n",
    "breast_quad_encoded = onehot_encoder_breast_quad.fit_transform(df[['breast-quad']])\n",
    "menopause_encoded = onehot_encoder_menopause.fit_transform(df[['menopause']])\n",
    "\n",
    "# Crie novos nomes de colunas para os atributos codificados\n",
    "breast_quad_columns = [f'breast_quad_{col}' for col in onehot_encoder_breast_quad.get_feature_names_out(['breast-quad'])]\n",
    "menopause_columns = [f'menopause_{col}' for col in onehot_encoder_menopause.get_feature_names_out(['menopause'])]\n",
    "\n",
    "# Adicione as novas colunas ao DataFrame original\n",
    "df[breast_quad_columns] = breast_quad_encoded\n",
    "df[menopause_columns] = menopause_encoded\n",
    "\n",
    "# Drop das colunas originais\n",
    "df = df.drop(['breast-quad', 'menopause'], axis=1)\n",
    "\n",
    "unique_values(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1891,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                                  0.654831\n",
       "tumor-size                           0.445974\n",
       "inv-nodes                             2.55107\n",
       "node-caps                            1.491267\n",
       "deg-malig                           -0.089498\n",
       "breast                               0.094479\n",
       "irradiat                             1.332411\n",
       "Class                                0.917675\n",
       "breast_quad_breast-quad_central      3.222552\n",
       "breast_quad_breast-quad_left_low     0.485427\n",
       "breast_quad_breast-quad_left_up      0.682279\n",
       "breast_quad_breast-quad_right_low    3.038736\n",
       "breast_quad_breast-quad_right_up     2.364244\n",
       "menopause_menopause_ge40              0.22647\n",
       "menopause_menopause_lt40             7.279533\n",
       "menopause_menopause_premeno         -0.152891\n",
       "dtype: object"
      ]
     },
     "execution_count": 1891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando a existencia de outliers\n",
    "\n",
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1892,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                                  0.654831\n",
       "tumor-size                           0.445974\n",
       "inv-nodes                            1.629511\n",
       "node-caps                            1.491267\n",
       "deg-malig                           -0.089498\n",
       "breast                               0.094479\n",
       "irradiat                             1.332411\n",
       "Class                                0.917675\n",
       "breast_quad_breast-quad_central      3.222552\n",
       "breast_quad_breast-quad_left_low     0.485427\n",
       "breast_quad_breast-quad_left_up      0.682279\n",
       "breast_quad_breast-quad_right_low    3.038736\n",
       "breast_quad_breast-quad_right_up     2.364244\n",
       "menopause_menopause_ge40              0.22647\n",
       "menopause_menopause_lt40             7.279533\n",
       "menopause_menopause_premeno         -0.152891\n",
       "dtype: object"
      ]
     },
     "execution_count": 1892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo os outliers\n",
    "\n",
    "df[\"inv-nodes\"] = np.sqrt(df[\"inv-nodes\"].astype(float))\n",
    "\n",
    "df.skew()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1893,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "281    0\n",
       "282    0\n",
       "283    0\n",
       "284    0\n",
       "285    0\n",
       "Name: Class, Length: 277, dtype: int64"
      ]
     },
     "execution_count": 1893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separando os atributos de entrada do atributo de classificação\n",
    "\n",
    "X = df.loc[:, \"age\":\"irradiat\"]\n",
    "y = df.loc[:, \"Class\"]\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1894,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 1.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.2       , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.2       , 0.1       , 0.        , ..., 0.5       , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.6       , 0.2       , 0.70710678, ..., 0.5       , 1.        ,\n",
       "        0.        ],\n",
       "       [0.2       , 0.        , 0.        , ..., 0.5       , 1.        ,\n",
       "        0.        ],\n",
       "       [0.2       , 0.4       , 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 1894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padronizando os dados\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "X = standard_scaler.fit_transform(X)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "# transformer = RobustScaler().fit(X)\n",
    "# X = transformer.transform(X)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1895,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os dados de treino e teste\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1896,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando random search para encontrar os melhores hipermarametros\n",
    "\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50,),(100,),(150,)],  # Vary the number of neurons in each hidden layer\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],  # Activation functions for the hidden layers\n",
    "    'solver': ['sgd', 'adam'],  # Solvers for weight optimization\n",
    "    'alpha': uniform(loc=0, scale=0.0001),  # L2 penalty (regularization term)\n",
    "    'batch_size': [\"auto\", 16, 32, 64, 128],  # Size of minibatches for stochastic optimizers\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],  # Learning rate schedule\n",
    "    'learning_rate_init': uniform(loc=0.001, scale=0.1),  # Initial learning rate\n",
    "    'power_t': uniform(loc=0.1, scale=0.5),  # Exponent for inverse scaling learning rate\n",
    "    'max_iter': randint(200, 10000),  # Maximum number of iterations\n",
    "    'shuffle': [True, False],  # Whether to shuffle samples in each iteration\n",
    "    'tol': uniform(loc=1e-5, scale=1e-3),  # Tolerance for optimization convergence\n",
    "    'verbose': [True, False],  # Whether to print progress messages\n",
    "    'warm_start': [True, False],  # Reuse the solution of the previous call to fit as initialization\n",
    "    'momentum': uniform(loc=0.1, scale=0.9),  # Momentum for SGD\n",
    "    'nesterovs_momentum': [True, False],  # Whether to use Nesterov's momentum\n",
    "    'early_stopping': [True, False],  # Whether to use early stopping to terminate training\n",
    "    'validation_fraction': uniform(loc=0.1, scale=0.3),  # Fraction of training data to set aside as validation set\n",
    "    'beta_1': uniform(loc=0.1, scale=0.9),  # Exponential decay rate for estimates of the first moment vector in adam\n",
    "    'beta_2': uniform(loc=0.1, scale=0.9),  # Exponential decay rate for estimates of the second moment vector in adam\n",
    "    'epsilon': uniform(loc=1e-8, scale=1e-6),  # Value for numerical stability in adam\n",
    "    'n_iter_no_change': randint(5, 20),  # Number of iterations with no improvement to wait before early stopping\n",
    "    'max_fun': randint(10, 50)  # Only used when solver='lbfgs'. Maximum number of loss function calls.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63047543\n",
      "Iteration 2, loss = 0.59915050\n",
      "Iteration 3, loss = 0.54496997\n",
      "Iteration 4, loss = 0.51441664\n",
      "Iteration 5, loss = 0.48714039\n",
      "Iteration 6, loss = 0.48695035\n",
      "Iteration 7, loss = 0.48845380\n",
      "Iteration 8, loss = 0.48523633\n",
      "Iteration 9, loss = 0.51169545\n",
      "Iteration 10, loss = 0.51902434\n",
      "Iteration 11, loss = 0.51811128\n",
      "Iteration 12, loss = 0.50628345\n",
      "Iteration 13, loss = 0.49604118\n",
      "Iteration 14, loss = 0.48884545\n",
      "Iteration 15, loss = 0.49593365\n",
      "Iteration 16, loss = 0.50244503\n",
      "Iteration 17, loss = 0.50470063\n",
      "Iteration 18, loss = 0.49943218\n",
      "Iteration 19, loss = 0.49357744\n",
      "Iteration 20, loss = 0.48754649\n",
      "Iteration 21, loss = 0.49362670\n",
      "Iteration 22, loss = 0.50360303\n",
      "Iteration 23, loss = 0.50876476\n",
      "Iteration 24, loss = 0.51069811\n",
      "Iteration 25, loss = 0.51091724\n",
      "Iteration 26, loss = 0.50823270\n",
      "Iteration 27, loss = 0.50297862\n",
      "Iteration 28, loss = 0.49142133\n",
      "Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64512518\n",
      "Iteration 2, loss = 0.60727136\n",
      "Iteration 3, loss = 0.58953649\n",
      "Iteration 4, loss = 0.53727389\n",
      "Iteration 5, loss = 0.52963969\n",
      "Iteration 6, loss = 0.59683561\n",
      "Iteration 7, loss = 0.58706399\n",
      "Iteration 8, loss = 0.51987088\n",
      "Iteration 9, loss = 0.53023182\n",
      "Iteration 10, loss = 0.60865475\n",
      "Iteration 11, loss = 0.64046933\n",
      "Iteration 12, loss = 0.64013554\n",
      "Iteration 13, loss = 0.63304375\n",
      "Iteration 14, loss = 0.60576993\n",
      "Iteration 15, loss = 0.58301320\n",
      "Iteration 16, loss = 0.57437059\n",
      "Iteration 17, loss = 0.57690443\n",
      "Iteration 18, loss = 0.58449010\n",
      "Iteration 19, loss = 0.60998694\n",
      "Iteration 20, loss = 0.62900612\n",
      "Iteration 21, loss = 0.64005562\n",
      "Iteration 22, loss = 0.64652245\n",
      "Iteration 23, loss = 0.65298187\n",
      "Iteration 24, loss = 0.65200765\n",
      "Iteration 25, loss = 0.64827764\n",
      "Iteration 26, loss = 0.64858440\n",
      "Iteration 27, loss = 0.64519058\n",
      "Iteration 28, loss = 0.63685509\n",
      "Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67573574Iteration 1, loss = 0.62120534\n",
      "\n",
      "Iteration 2, loss = 0.59900533\n",
      "Iteration 2, loss = 0.66668798\n",
      "Iteration 3, loss = 0.55001403\n",
      "Iteration 3, loss = 0.59291090\n",
      "Iteration 4, loss = 0.63305019\n",
      "Iteration 4, loss = 0.63548552\n",
      "Iteration 5, loss = 0.65684830\n",
      "Iteration 5, loss = 0.52969494\n",
      "Iteration 1, loss = 0.63223438\n",
      "Iteration 6, loss = 0.52220656\n",
      "Iteration 2, loss = 0.75289349Iteration 7, loss = 0.51823889\n",
      "\n",
      "Iteration 8, loss = 0.65764551Iteration 3, loss = 0.56402681\n",
      "\n",
      "Iteration 4, loss = 0.62150641\n",
      "Iteration 9, loss = 0.68812342\n",
      "Iteration 10, loss = 0.64276088Iteration 5, loss = 0.55090506\n",
      "\n",
      "Iteration 6, loss = 0.52492003Iteration 11, loss = 0.56512177\n",
      "\n",
      "Iteration 6, loss = 0.55497886Iteration 12, loss = 0.51193131\n",
      "Iteration 7, loss = 0.64770727\n",
      "\n",
      "Iteration 13, loss = 0.54270733Iteration 8, loss = 0.66080608\n",
      "\n",
      "Iteration 9, loss = 0.62865690Iteration 14, loss = 0.59767532\n",
      "\n",
      "Iteration 15, loss = 0.64757780Iteration 10, loss = 0.57479137\n",
      "\n",
      "Iteration 7, loss = 0.64237354\n",
      "Iteration 11, loss = 0.55755197\n",
      "Iteration 16, loss = 0.67036088\n",
      "Iteration 8, loss = 0.66862921\n",
      "Iteration 17, loss = 0.67590106\n",
      "Iteration 12, loss = 0.59506600\n",
      "Iteration 9, loss = 0.65422347\n",
      "Iteration 18, loss = 0.66314126\n",
      "Iteration 13, loss = 0.64632863\n",
      "Iteration 19, loss = 0.63891282\n",
      "Iteration 14, loss = 0.67310734\n",
      "Iteration 20, loss = 0.59327878\n",
      "Iteration 15, loss = 0.69113659\n",
      "Iteration 21, loss = 0.56536254\n",
      "Iteration 10, loss = 0.60457300\n",
      "Iteration 16, loss = 0.70144481\n",
      "Iteration 22, loss = 0.54478089\n",
      "Iteration 17, loss = 0.70422945\n",
      "Iteration 11, loss = 0.55908367\n",
      "Iteration 23, loss = 0.55854847\n",
      "Iteration 18, loss = 0.70562197\n",
      "Iteration 12, loss = 0.53324567\n",
      "Iteration 24, loss = 0.58931414\n",
      "Iteration 19, loss = 0.70531184\n",
      "Iteration 25, loss = 0.62219153\n",
      "Iteration 13, loss = 0.59597842\n",
      "Iteration 20, loss = 0.70145777\n",
      "Iteration 26, loss = 0.65814841\n",
      "Iteration 21, loss = 0.69638782\n",
      "Iteration 27, loss = 0.68709810\n",
      "Iteration 22, loss = 0.68294369\n",
      "Iteration 28, loss = 0.70400578Iteration 14, loss = 0.63148055\n",
      "\n",
      "Iteration 23, loss = 0.67649868\n",
      "Iteration 29, loss = 0.71777584\n",
      "Iteration 15, loss = 0.65152502\n",
      "Iteration 24, loss = 0.67363748\n",
      "Iteration 30, loss = 0.72440703\n",
      "Iteration 16, loss = 0.66382169\n",
      "Iteration 25, loss = 0.67395246\n",
      "Iteration 31, loss = 0.72425498\n",
      "Iteration 17, loss = 0.67024247\n",
      "Iteration 26, loss = 0.69177990\n",
      "Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.71792256\n",
      "Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.67379825\n",
      "Iteration 1, loss = 0.65458866\n",
      "Iteration 1, loss = 0.68124704\n",
      "Iteration 2, loss = 0.71830840\n",
      "Iteration 19, loss = 0.67589077\n",
      "Iteration 2, loss = 0.68700609\n",
      "Iteration 3, loss = 0.66666584\n",
      "Iteration 3, loss = 0.59357126\n",
      "Iteration 20, loss = 0.67381816\n",
      "Iteration 4, loss = 0.83766167\n",
      "Iteration 4, loss = 0.71771061\n",
      "Iteration 5, loss = 0.72454229\n",
      "Iteration 21, loss = 0.66911430\n",
      "Iteration 5, loss = 0.61948176\n",
      "Iteration 6, loss = 0.56486010\n",
      "Iteration 22, loss = 0.65938329\n",
      "Iteration 6, loss = 0.52666184\n",
      "Iteration 7, loss = 0.55610825\n",
      "Iteration 7, loss = 0.64083598\n",
      "Iteration 8, loss = 0.73607150\n",
      "Iteration 8, loss = 0.71810197\n",
      "Iteration 9, loss = 0.84612937\n",
      "Iteration 9, loss = 0.73847198\n",
      "Iteration 10, loss = 0.90230004\n",
      "Iteration 10, loss = 0.72183826\n",
      "Iteration 11, loss = 0.90772334\n",
      "Iteration 23, loss = 0.65619789\n",
      "Iteration 11, loss = 0.67277613Iteration 12, loss = 0.88902853\n",
      "\n",
      "Iteration 24, loss = 0.65224101\n",
      "Iteration 13, loss = 0.83007875Iteration 12, loss = 0.62630615\n",
      "\n",
      "Iteration 13, loss = 0.59474477Iteration 14, loss = 0.78872257\n",
      "\n",
      "Iteration 25, loss = 0.66113541\n",
      "Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.71716994Iteration 14, loss = 0.63247967\n",
      "\n",
      "Iteration 15, loss = 0.69191004Iteration 16, loss = 0.67266680\n",
      "\n",
      "Iteration 16, loss = 0.73047336Iteration 17, loss = 0.71927147\n",
      "\n",
      "Iteration 18, loss = 0.77693929Iteration 17, loss = 0.75374288\n",
      "\n",
      "Iteration 19, loss = 0.83558881Iteration 18, loss = 0.77264848\n",
      "\n",
      "Iteration 20, loss = 0.88028177Iteration 19, loss = 0.79273743\n",
      "\n",
      "Iteration 20, loss = 0.81104473\n",
      "Iteration 21, loss = 0.91277963\n",
      "Iteration 1, loss = 0.61887905\n",
      "Iteration 22, loss = 0.94236172Iteration 21, loss = 0.82427379\n",
      "\n",
      "Iteration 2, loss = 0.58058925\n",
      "Iteration 22, loss = 0.83569342Iteration 23, loss = 0.96751839\n",
      "\n",
      "Iteration 3, loss = 0.55513608\n",
      "Iteration 23, loss = 0.84525071Iteration 24, loss = 0.98762608\n",
      "\n",
      "Iteration 4, loss = 0.50538779\n",
      "Iteration 25, loss = 1.00584473Iteration 24, loss = 0.84759194\n",
      "\n",
      "Iteration 25, loss = 0.85222830Iteration 26, loss = 1.02126309\n",
      "\n",
      "Iteration 5, loss = 0.70106763\n",
      "Iteration 27, loss = 1.03340278Iteration 26, loss = 0.85348465\n",
      "Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "\n",
      "Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.65424691\n",
      "Iteration 7, loss = 0.57823563\n",
      "Iteration 8, loss = 0.62116708\n",
      "Iteration 9, loss = 0.66915312\n",
      "Iteration 1, loss = 0.71871207Iteration 10, loss = 0.64647984\n",
      "\n",
      "Iteration 11, loss = 0.56406740Iteration 2, loss = 0.69578343\n",
      "\n",
      "Iteration 3, loss = 0.60525468\n",
      "Iteration 12, loss = 0.52391520\n",
      "Iteration 4, loss = 0.69210632\n",
      "Iteration 13, loss = 0.56557075\n",
      "Iteration 5, loss = 0.54886560\n",
      "Iteration 14, loss = 0.60625079Iteration 1, loss = 0.68540976\n",
      "\n",
      "Validation score: 0.731343\n",
      "Iteration 6, loss = 0.57578538\n",
      "Iteration 15, loss = 0.62202135\n",
      "Iteration 7, loss = 0.61152018\n",
      "Iteration 2, loss = 0.49639535Iteration 16, loss = 0.63459832\n",
      "Iteration 8, loss = 0.61737686\n",
      "Iteration 9, loss = 0.59271651\n",
      "Iteration 17, loss = 0.63655965\n",
      "Iteration 18, loss = 0.63421436\n",
      "\n",
      "Iteration 19, loss = 0.64092477\n",
      "Validation score: 0.761194\n",
      "Iteration 20, loss = 0.64311062Iteration 3, loss = 0.51349076\n",
      "\n",
      "Validation score: 0.776119\n",
      "Iteration 10, loss = 0.57797398\n",
      "Iteration 21, loss = 0.63829211\n",
      "Iteration 4, loss = 0.48803459\n",
      "Validation score: 0.761194\n",
      "Iteration 11, loss = 0.55405385\n",
      "Iteration 22, loss = 0.61270237\n",
      "Iteration 12, loss = 0.57632469\n",
      "Iteration 23, loss = 0.60433622\n",
      "Iteration 13, loss = 0.60008204\n",
      "Iteration 14, loss = 0.61805854Iteration 24, loss = 0.58420848\n",
      "Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 15, loss = 0.63254020\n",
      "Iteration 16, loss = 0.64414870\n",
      "Iteration 17, loss = 0.65515983\n",
      "Iteration 5, loss = 0.49034522\n",
      "Validation score: 0.761194\n",
      "Iteration 18, loss = 0.66495981\n",
      "Iteration 6, loss = 0.48473942\n",
      "Iteration 19, loss = 0.67139687Validation score: 0.761194\n",
      "\n",
      "Iteration 1, loss = 0.57324644\n",
      "Iteration 20, loss = 0.67768898\n",
      "Iteration 7, loss = 0.48198745\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 21, loss = 0.68832598\n",
      "Iteration 2, loss = 0.51111848\n",
      "Iteration 8, loss = 0.48169134Validation score: 0.746269\n",
      "\n",
      "Iteration 22, loss = 0.69534794\n",
      "Validation score: 0.746269\n",
      "Iteration 23, loss = 0.70727133\n",
      "Iteration 3, loss = 0.49132538Iteration 9, loss = 0.48028540\n",
      "\n",
      "Validation score: 0.761194Validation score: 0.746269\n",
      "Iteration 24, loss = 0.71742913\n",
      "Iteration 4, loss = 0.47903121\n",
      "Iteration 25, loss = 0.72728356\n",
      "Validation score: 0.746269Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 5, loss = 0.47555142\n",
      "Validation score: 0.746269\n",
      "Iteration 6, loss = 0.47155697\n",
      "Validation score: 0.746269\n",
      "\n",
      "Iteration 1, loss = 0.68366656\n",
      "Iteration 10, loss = 0.48004732\n",
      "Validation score: 0.761194\n",
      "Iteration 7, loss = 0.47093367\n",
      "Validation score: 0.731343\n",
      "Iteration 2, loss = 0.86246318\n",
      "Iteration 11, loss = 0.47930229\n",
      "Validation score: 0.761194\n",
      "Iteration 8, loss = 0.46939750Iteration 3, loss = 0.62394121\n",
      "\n",
      "Validation score: 0.731343\n",
      "Iteration 12, loss = 0.47882977Iteration 4, loss = 0.74214345\n",
      "Iteration 9, loss = 0.46890352\n",
      "Iteration 5, loss = 0.78249848\n",
      "Validation score: 0.731343\n",
      "\n",
      "Iteration 6, loss = 0.68350490\n",
      "Validation score: 0.761194\n",
      "Iteration 10, loss = 0.46815272\n",
      "Validation score: 0.731343Iteration 13, loss = 0.47836860\n",
      "\n",
      "Iteration 7, loss = 0.55044712\n",
      "Iteration 11, loss = 0.46786640\n",
      "Iteration 8, loss = 0.52919598Validation score: 0.731343\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 9, loss = 0.60285963\n",
      "Iteration 10, loss = 0.68106449\n",
      "Iteration 14, loss = 0.47795780\n",
      "Validation score: 0.761194\n",
      "Iteration 11, loss = 0.73457908\n",
      "Iteration 12, loss = 0.76303866\n",
      "Iteration 15, loss = 0.47761391\n",
      "Validation score: 0.761194\n",
      "Iteration 12, loss = 0.46752531\n",
      "Validation score: 0.731343\n",
      "Iteration 16, loss = 0.47728149\n",
      "Validation score: 0.761194\n",
      "Iteration 13, loss = 0.77281716\n",
      "Iteration 13, loss = 0.46733576\n",
      "Validation score: 0.731343\n",
      "Iteration 17, loss = 0.47698057\n",
      "Validation score: 0.761194\n",
      "Iteration 18, loss = 0.47669269\n",
      "Validation score: 0.761194\n",
      "Iteration 19, loss = 0.47641986\n",
      "Validation score: 0.761194\n",
      "Iteration 14, loss = 0.46710751\n",
      "Validation score: 0.731343\n",
      "Iteration 20, loss = 0.47615972\n",
      "Validation score: 0.761194\n",
      "Iteration 15, loss = 0.46692172\n",
      "Iteration 14, loss = 0.76750461\n",
      "Validation score: 0.731343\n",
      "Iteration 21, loss = 0.47590969\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910\n",
      "Iteration 22, loss = 0.46151967\n",
      "Iteration 16, loss = 0.46672009\n",
      "Validation score: 0.731343\n",
      "Iteration 15, loss = 0.74309283\n",
      "Validation score: 0.746269\n",
      "Iteration 16, loss = 0.71134968\n",
      "Iteration 17, loss = 0.46653633\n",
      "Iteration 23, loss = 0.46783616\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 17, loss = 0.68341440\n",
      "Iteration 18, loss = 0.46634971\n",
      "Iteration 24, loss = 0.46677250\n",
      "Iteration 18, loss = 0.66299952\n",
      "Validation score: 0.731343Validation score: 0.761194\n",
      "\n",
      "Iteration 25, loss = 0.46576123\n",
      "Validation score: 0.761194\n",
      "Iteration 19, loss = 0.46617264\n",
      "Validation score: 0.731343\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910\n",
      "Iteration 19, loss = 0.65333205\n",
      "Iteration 26, loss = 0.46509441\n",
      "Validation score: 0.761194Iteration 20, loss = 0.70067936\n",
      "\n",
      "Iteration 20, loss = 0.45830906\n",
      "Validation score: 0.746269\n",
      "Iteration 21, loss = 0.73428416\n",
      "Iteration 21, loss = 0.47089884\n",
      "Validation score: 0.731343Iteration 22, loss = 0.76200883\n",
      "\n",
      "Iteration 22, loss = 0.46214811\n",
      "Validation score: 0.731343\n",
      "Iteration 23, loss = 0.78870800\n",
      "Iteration 23, loss = 0.45917800\n",
      "Validation score: 0.731343Iteration 24, loss = 0.80682514\n",
      "\n",
      "Iteration 25, loss = 0.82281490\n",
      "Iteration 24, loss = 0.45817850\n",
      "Iteration 27, loss = 0.46470120Validation score: 0.731343\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 26, loss = 0.83521191\n",
      "Iteration 28, loss = 0.46457165Iteration 25, loss = 0.45831169\n",
      "Validation score: 0.731343\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 27, loss = 0.84539441\n",
      "Iteration 26, loss = 0.45815098\n",
      "Validation score: 0.731343\n",
      "Iteration 28, loss = 0.85413157\n",
      "Training loss did not improve more than tol=0.000876 for 19 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.45768814\n",
      "Iteration 29, loss = 0.46453083\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 28, loss = 0.45730885\n",
      "Iteration 30, loss = 0.46448223\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 29, loss = 0.45708439\n",
      "Validation score: 0.731343\n",
      "Iteration 31, loss = 0.46441905Iteration 30, loss = 0.45692968\n",
      "Validation score: 0.731343\n",
      "Iteration 1, loss = 0.61524687\n",
      "\n",
      "Validation score: 0.731343Validation score: 0.761194\n",
      "\n",
      "Iteration 31, loss = 0.45678624\n",
      "Validation score: 0.731343\n",
      "Iteration 32, loss = 0.46435768\n",
      "Iteration 32, loss = 0.45665237\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 33, loss = 0.46430781\n",
      "Iteration 33, loss = 0.45653822\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 34, loss = 0.45644341\n",
      "Validation score: 0.731343\n",
      "Iteration 2, loss = 0.55988203\n",
      "Validation score: 0.776119\n",
      "Iteration 35, loss = 0.45636210\n",
      "Validation score: 0.731343\n",
      "Iteration 36, loss = 0.45629029\n",
      "Iteration 34, loss = 0.46426976\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194Iteration 37, loss = 0.45622649Iteration 3, loss = 0.51368302\n",
      "\n",
      "Validation score: 0.731343Validation score: 0.761194\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "\n",
      "Iteration 4, loss = 0.51560479\n",
      "Iteration 38, loss = 0.45401865\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 35, loss = 0.46423997Iteration 39, loss = 0.45607329\n",
      "\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 5, loss = 0.50646898\n",
      "Iteration 40, loss = 0.45633061\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 36, loss = 0.46421527\n",
      "Validation score: 0.761194\n",
      "Iteration 6, loss = 0.50365662\n",
      "Validation score: 0.746269\n",
      "Iteration 41, loss = 0.45606630\n",
      "Validation score: 0.731343\n",
      "Iteration 7, loss = 0.49993906\n",
      "Validation score: 0.746269\n",
      "Iteration 42, loss = 0.45576912\n",
      "Validation score: 0.731343\n",
      "Iteration 43, loss = 0.45554424\n",
      "Validation score: 0.731343\n",
      "Iteration 8, loss = 0.49808739\n",
      "Validation score: 0.746269\n",
      "Iteration 37, loss = 0.46419406\n",
      "Iteration 44, loss = 0.45538855\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 9, loss = 0.49627591\n",
      "Validation score: 0.746269\n",
      "Iteration 45, loss = 0.45528133\n",
      "Validation score: 0.731343\n",
      "Iteration 10, loss = 0.49507382\n",
      "Validation score: 0.746269\n",
      "Iteration 46, loss = 0.45520533\n",
      "Validation score: 0.731343\n",
      "Iteration 38, loss = 0.46417568\n",
      "Validation score: 0.761194\n",
      "Iteration 47, loss = 0.45514901\n",
      "Iteration 11, loss = 0.49401074\n",
      "Validation score: 0.731343\n",
      "Iteration 39, loss = 0.46415977Validation score: 0.746269\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "Iteration 48, loss = 0.45510508\n",
      "Iteration 12, loss = 0.49322931\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269\n",
      "Iteration 40, loss = 0.46137384\n",
      "Validation score: 0.761194\n",
      "Iteration 49, loss = 0.45506907\n",
      "Iteration 13, loss = 0.49259059\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269\n",
      "Iteration 50, loss = 0.45503818\n",
      "Validation score: 0.731343\n",
      "Iteration 14, loss = 0.49211554\n",
      "Iteration 51, loss = 0.45501072\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 15, loss = 0.49174126\n",
      "Validation score: 0.746269\n",
      "Iteration 52, loss = 0.45498561\n",
      "Validation score: 0.731343\n",
      "Iteration 16, loss = 0.49146014\n",
      "Iteration 53, loss = 0.45496220\n",
      "Validation score: 0.731343\n",
      "Iteration 54, loss = 0.45494005\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269\n",
      "Iteration 41, loss = 0.46202289\n",
      "Iteration 55, loss = 0.45491892\n",
      "Validation score: 0.731343\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "Iteration 17, loss = 0.49124064\n",
      "Validation score: 0.746269\n",
      "Iteration 56, loss = 0.45445292\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 18, loss = 0.49107332\n",
      "Validation score: 0.746269\n",
      "Iteration 57, loss = 0.45477530\n",
      "Validation score: 0.731343\n",
      "Iteration 1, loss = 0.62656382\n",
      "Validation score: 0.731343\n",
      "Iteration 58, loss = 0.45485642\n",
      "Iteration 19, loss = 0.49094211\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269\n",
      "Iteration 2, loss = 0.57623979\n",
      "Iteration 42, loss = 0.46222570\n",
      "Iteration 59, loss = 0.45486728\n",
      "Validation score: 0.761194\n",
      "Iteration 20, loss = 0.49084004\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910\n",
      "Iteration 43, loss = 0.46227832Iteration 3, loss = 0.51572023\n",
      "Iteration 60, loss = 0.45485892Iteration 21, loss = 0.48727559\n",
      "Validation score: 0.746269\n",
      "\n",
      "Validation score: 0.731343Validation score: 0.746269\n",
      "\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 61, loss = 0.45484583\n",
      "Iteration 4, loss = 0.52429296\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269\n",
      "Iteration 22, loss = 0.48862360\n",
      "Validation score: 0.746269\n",
      "Iteration 62, loss = 0.45483202\n",
      "Iteration 5, loss = 0.51334407\n",
      "Validation score: 0.731343\n",
      "Iteration 23, loss = 0.48681437Validation score: 0.746269\n",
      "\n",
      "Iteration 44, loss = 0.46229142\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 6, loss = 0.51405726\n",
      "Iteration 24, loss = 0.48637879\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.746269\n",
      "Iteration 45, loss = 0.46229618\n",
      "Validation score: 0.761194\n",
      "Iteration 25, loss = 0.48637431\n",
      "Iteration 63, loss = 0.45481857\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 46, loss = 0.46229964\n",
      "Validation score: 0.761194\n",
      "Iteration 26, loss = 0.48642298\n",
      "Iteration 64, loss = 0.45480575\n",
      "Iteration 47, loss = 0.46230268Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 7, loss = 0.51186985\n",
      "Validation score: 0.746269\n",
      "Iteration 65, loss = 0.45479356\n",
      "Iteration 27, loss = 0.48632808\n",
      "Validation score: 0.746269Validation score: 0.731343\n",
      "\n",
      "Iteration 66, loss = 0.45478199\n",
      "Iteration 28, loss = 0.48620887\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269\n",
      "Iteration 8, loss = 0.51145723\n",
      "Validation score: 0.746269\n",
      "Iteration 67, loss = 0.45477099\n",
      "\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 9, loss = 0.51107385\n",
      "Iteration 29, loss = 0.48613491\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.746269\n",
      "Iteration 68, loss = 0.45476052\n",
      "Iteration 48, loss = 0.46230499\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194Iteration 30, loss = 0.48608806\n",
      "Iteration 10, loss = 0.51059149\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "\n",
      "Iteration 69, loss = 0.45475052\n",
      "Validation score: 0.731343\n",
      "Iteration 49, loss = 0.46230630\n",
      "Iteration 11, loss = 0.51038221\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "Iteration 31, loss = 0.48604382\n",
      "Validation score: 0.746269\n",
      "\n",
      "Iteration 32, loss = 0.48599918\n",
      "Iteration 70, loss = 0.45474098\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343Iteration 50, loss = 0.46230651\n",
      "\n",
      "Iteration 33, loss = 0.48595903\n",
      "Iteration 71, loss = 0.45473184\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 12, loss = 0.51006029Iteration 34, loss = 0.48592443\n",
      "Iteration 72, loss = 0.45472308\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 35, loss = 0.48589362\n",
      "Validation score: 0.746269\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 51, loss = 0.46230567\n",
      "Iteration 36, loss = 0.48586531\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 73, loss = 0.45471467\n",
      "Iteration 52, loss = 0.46230390\n",
      "Validation score: 0.761194\n",
      "Iteration 13, loss = 0.50988712\n",
      "Iteration 37, loss = 0.48583919\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 53, loss = 0.46230135\n",
      "Iteration 14, loss = 0.50969589Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.761194Validation score: 0.731343\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119\n",
      "\n",
      "Iteration 38, loss = 0.48581520\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "Iteration 15, loss = 0.50955081\n",
      "Validation score: 0.761194\n",
      "Iteration 54, loss = 0.46229819\n",
      "Iteration 1, loss = 0.64406327Validation score: 0.761194\n",
      "\n",
      "Iteration 74, loss = 0.45461668\n",
      "Iteration 16, loss = 0.50941474\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 39, loss = 0.48502104\n",
      "Validation score: 0.731343Iteration 55, loss = 0.46229456\n",
      "\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194Iteration 17, loss = 0.50929173\n",
      "\n",
      "Iteration 75, loss = 0.45467609\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 40, loss = 0.48548005\n",
      "Iteration 2, loss = 0.58707763Validation score: 0.746269\n",
      "Iteration 18, loss = 0.50918006\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.791045\n",
      "Iteration 41, loss = 0.48547204\n",
      "Validation score: 0.746269\n",
      "Iteration 76, loss = 0.45469088\n",
      "Iteration 56, loss = 0.46229058\n",
      "Iteration 3, loss = 0.52811361\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343Validation score: 0.805970\n",
      "Iteration 19, loss = 0.50907522\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 57, loss = 0.46228634\n",
      "Iteration 4, loss = 0.52670240\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "Validation score: 0.820896\n",
      "Iteration 42, loss = 0.48534977\n",
      "Iteration 20, loss = 0.50897819\n",
      "Validation score: 0.746269Iteration 77, loss = 0.45469347\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910\n",
      "Validation score: 0.731343\n",
      "Iteration 58, loss = 0.46172884\n",
      "Iteration 5, loss = 0.51415483\n",
      "Validation score: 0.820896\n",
      "Validation score: 0.761194\n",
      "Iteration 21, loss = 0.50759460\n",
      "Iteration 78, loss = 0.45469277\n",
      "Iteration 43, loss = 0.48523642Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269Iteration 6, loss = 0.51239055\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 22, loss = 0.51337057Iteration 79, loss = 0.45469118\n",
      "\n",
      "Validation score: 0.731343Iteration 44, loss = 0.48515477\n",
      "Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 7, loss = 0.50851743\n",
      "Iteration 59, loss = 0.46182173\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 23, loss = 0.50867788\n",
      "Iteration 8, loss = 0.50716040\n",
      "Iteration 60, loss = 0.46184929\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.791045\n",
      "Validation score: 0.761194\n",
      "Iteration 45, loss = 0.48510014\n",
      "Validation score: 0.746269\n",
      "Iteration 24, loss = 0.50587441\n",
      "Iteration 80, loss = 0.45468936\n",
      "Iteration 9, loss = 0.50588273\n",
      "Iteration 61, loss = 0.46185815\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.791045\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 46, loss = 0.48506401\n",
      "Validation score: 0.746269\n",
      "Iteration 10, loss = 0.50531260\n",
      "Iteration 25, loss = 0.50513787\n",
      "Iteration 81, loss = 0.45468749\n",
      "Iteration 62, loss = 0.46186175\n",
      "Validation score: 0.791045\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 47, loss = 0.48503963\n",
      "Validation score: 0.746269\n",
      "Iteration 11, loss = 0.50503240\n",
      "Iteration 26, loss = 0.50513074\n",
      "Iteration 63, loss = 0.46186385\n",
      "Iteration 82, loss = 0.45468562\n",
      "Validation score: 0.791045\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 48, loss = 0.48502244\n",
      "Validation score: 0.746269\n",
      "Iteration 27, loss = 0.50524857\n",
      "Validation score: 0.761194\n",
      "Iteration 49, loss = 0.48500957\n",
      "Iteration 28, loss = 0.50523291\n",
      "Iteration 83, loss = 0.45468376Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.731343\n",
      "Iteration 50, loss = 0.48499927\n",
      "Iteration 29, loss = 0.50511077\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 84, loss = 0.45468192\n",
      "Validation score: 0.731343\n",
      "Iteration 51, loss = 0.48499050\n",
      "Validation score: 0.746269\n",
      "Iteration 30, loss = 0.50497740\n",
      "Iteration 85, loss = 0.45468009\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 52, loss = 0.48498265\n",
      "Validation score: 0.746269\n",
      "Iteration 12, loss = 0.50493190\n",
      "Validation score: 0.791045Iteration 31, loss = 0.50487465\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 13, loss = 0.50496361Iteration 86, loss = 0.45467828Iteration 53, loss = 0.48497539\n",
      "\n",
      "Iteration 64, loss = 0.46186547\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 54, loss = 0.48496850\n",
      "Validation score: 0.746269\n",
      "Iteration 65, loss = 0.46186691\n",
      "Iteration 87, loss = 0.45467648\n",
      "Validation score: 0.761194Validation score: 0.731343\n",
      "\n",
      "\n",
      "Validation score: 0.791045\n",
      "Iteration 55, loss = 0.48496188\n",
      "Validation score: 0.746269\n",
      "Iteration 66, loss = 0.46186823Iteration 88, loss = 0.45467470\n",
      "\n",
      "Validation score: 0.761194Validation score: 0.731343\n",
      "\n",
      "Iteration 14, loss = 0.50501846\n",
      "Iteration 32, loss = 0.50479948\n",
      "Validation score: 0.791045\n",
      "Iteration 56, loss = 0.48495546\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "Iteration 67, loss = 0.46186948Iteration 89, loss = 0.45467293\n",
      "\n",
      "Validation score: 0.731343Validation score: 0.761194\n",
      "\n",
      "Iteration 33, loss = 0.50473799\n",
      "Iteration 57, loss = 0.48479150\n",
      "Validation score: 0.746269\n",
      "Iteration 68, loss = 0.46187064\n",
      "Validation score: 0.761194\n",
      "Iteration 58, loss = 0.48487662\n",
      "Iteration 90, loss = 0.45467118\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 15, loss = 0.50510293\n",
      "Validation score: 0.791045Iteration 69, loss = 0.46187173\n",
      "Validation score: 0.761194\n",
      "Iteration 91, loss = 0.45466944\n",
      "Iteration 59, loss = 0.48489486Validation score: 0.731343\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 70, loss = 0.46187275\n",
      "Validation score: 0.761194Iteration 92, loss = 0.45464971\n",
      "\n",
      "Iteration 60, loss = 0.48489396\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 71, loss = 0.46187370\n",
      "Validation score: 0.761194\n",
      "Iteration 61, loss = 0.48488801\n",
      "Validation score: 0.746269\n",
      "Iteration 34, loss = 0.50468204\n",
      "Iteration 93, loss = 0.45466142\n",
      "Validation score: 0.761194\n",
      "Iteration 72, loss = 0.46187458\n",
      "Iteration 62, loss = 0.48488099\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269\n",
      "Iteration 35, loss = 0.50462994\n",
      "Iteration 73, loss = 0.46187539\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 16, loss = 0.50517317\n",
      "Iteration 63, loss = 0.48487400\n",
      "Validation score: 0.791045\n",
      "Validation score: 0.746269Iteration 94, loss = 0.45466433\n",
      "\n",
      "Validation score: 0.731343\n",
      "Iteration 74, loss = 0.46187614Iteration 36, loss = 0.50458235\n",
      "\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "\n",
      "Iteration 17, loss = 0.50523645\n",
      "Iteration 64, loss = 0.48486734\n",
      "Validation score: 0.791045\n",
      "Iteration 95, loss = 0.45466485\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 75, loss = 0.46187683\n",
      "Iteration 37, loss = 0.50453961\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119\n",
      "Validation score: 0.761194\n",
      "Iteration 18, loss = 0.50528097\n",
      "Iteration 65, loss = 0.48486105\n",
      "Validation score: 0.791045\n",
      "Validation score: 0.746269\n",
      "Iteration 96, loss = 0.45466473\n",
      "Validation score: 0.731343\n",
      "Iteration 76, loss = 0.46176614\n",
      "Iteration 38, loss = 0.50450130\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194Iteration 66, loss = 0.48485513\n",
      "\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "Iteration 77, loss = 0.46178377\n",
      "Validation score: 0.761194\n",
      "Iteration 97, loss = 0.45466443\n",
      "Iteration 19, loss = 0.50531127Validation score: 0.731343\n",
      "\n",
      "Validation score: 0.791045Iteration 39, loss = 0.50396588\n",
      "\n",
      "Iteration 67, loss = 0.48484955\n",
      "Validation score: 0.761194Iteration 78, loss = 0.46178872\n",
      "\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 68, loss = 0.48484429\n",
      "Iteration 98, loss = 0.45466409\n",
      "Iteration 40, loss = 0.50502803\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 79, loss = 0.46179024\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 69, loss = 0.48483933\n",
      "Validation score: 0.746269\n",
      "Iteration 99, loss = 0.45466374\n",
      "Iteration 41, loss = 0.50512374\n",
      "Validation score: 0.731343\n",
      "Iteration 80, loss = 0.46179081\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194Iteration 70, loss = 0.48483464\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 81, loss = 0.46179113\n",
      "Iteration 71, loss = 0.48483020\n",
      "Validation score: 0.746269Validation score: 0.761194\n",
      "\n",
      "Iteration 100, loss = 0.45466339\n",
      "Validation score: 0.731343Iteration 42, loss = 0.50493636\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 82, loss = 0.46179139\n",
      "Validation score: 0.761194Iteration 72, loss = 0.48482600\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 101, loss = 0.45466304\n",
      "Iteration 43, loss = 0.50471233Validation score: 0.731343\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 20, loss = 0.50532655\n",
      "Iteration 83, loss = 0.46179161\n",
      "Iteration 73, loss = 0.48482201\n",
      "Validation score: 0.791045\n",
      "Validation score: 0.746269Validation score: 0.761194\n",
      "\n",
      "Iteration 102, loss = 0.45466268\n",
      "Validation score: 0.731343\n",
      "Iteration 44, loss = 0.50451802\n",
      "Validation score: 0.761194Iteration 21, loss = 0.50533004\n",
      "\n",
      "Iteration 84, loss = 0.46179184\n",
      "Iteration 74, loss = 0.48481822Validation score: 0.791045\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119\n",
      "Iteration 103, loss = 0.45466233\n",
      "Validation score: 0.731343\n",
      "Iteration 22, loss = 0.50532350\n",
      "Iteration 85, loss = 0.46179205\n",
      "Validation score: 0.791045\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910Iteration 75, loss = 0.48478348\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 45, loss = 0.50436318\n",
      "Iteration 23, loss = 0.50605174\n",
      "Iteration 104, loss = 0.45466198\n",
      "Iteration 86, loss = 0.46179227\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194Validation score: 0.731343\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 76, loss = 0.48479933Iteration 24, loss = 0.51140151\n",
      "\n",
      "Validation score: 0.791045\n",
      "Iteration 87, loss = 0.46179248\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 46, loss = 0.50424273\n",
      "Iteration 105, loss = 0.45466163\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 77, loss = 0.48480295Iteration 25, loss = 0.50229388\n",
      "\n",
      "Validation score: 0.791045Iteration 88, loss = 0.46179269\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 106, loss = 0.45466127\n",
      "Iteration 47, loss = 0.50414928\n",
      "Iteration 26, loss = 0.50036317\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 89, loss = 0.46179289\n",
      "Validation score: 0.761194\n",
      "Iteration 27, loss = 0.50078639\n",
      "Validation score: 0.776119\n",
      "Iteration 48, loss = 0.50407632Iteration 107, loss = 0.45466092\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 90, loss = 0.46179310\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 28, loss = 0.50144293\n",
      "Validation score: 0.776119\n",
      "Iteration 91, loss = 0.46179329\n",
      "Validation score: 0.761194\n",
      "Iteration 29, loss = 0.50136221\n",
      "Iteration 78, loss = 0.48480324\n",
      "Validation score: 0.776119\n",
      "Iteration 49, loss = 0.50401878\n",
      "Validation score: 0.746269Validation score: 0.761194\n",
      "Iteration 92, loss = 0.46179349\n",
      "Iteration 30, loss = 0.50112889\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.731343\n",
      "Iteration 31, loss = 0.50107156\n",
      "Iteration 93, loss = 0.46179369\n",
      "Iteration 50, loss = 0.50397285\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "\n",
      "Iteration 32, loss = 0.50109864\n",
      "Iteration 108, loss = 0.45466057\n",
      "\n",
      "Validation score: 0.731343Validation score: 0.776119\n",
      "\n",
      "Iteration 51, loss = 0.50393570Iteration 94, loss = 0.46177159\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 79, loss = 0.48480263\n",
      "Iteration 33, loss = 0.50110245\n",
      "Validation score: 0.746269\n",
      "Iteration 95, loss = 0.46177508\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 52, loss = 0.50390521\n",
      "Validation score: 0.761194\n",
      "Iteration 80, loss = 0.48480179\n",
      "Iteration 96, loss = 0.46177606\n",
      "Iteration 109, loss = 0.45466022Validation score: 0.761194\n",
      "\n",
      "Iteration 34, loss = 0.50108028\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.746269\n",
      "Iteration 53, loss = 0.50387981\n",
      "Validation score: 0.761194\n",
      "Iteration 97, loss = 0.46177635\n",
      "Validation score: 0.761194\n",
      "Iteration 35, loss = 0.50106042\n",
      "Validation score: 0.776119\n",
      "Iteration 54, loss = 0.50385832\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "Iteration 98, loss = 0.46177646\n",
      "Iteration 36, loss = 0.50104947\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 55, loss = 0.50383984\n",
      "Iteration 81, loss = 0.48480089\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 37, loss = 0.50104107Iteration 99, loss = 0.46177652\n",
      "\n",
      "Iteration 110, loss = 0.45465627\n",
      "Validation score: 0.791045\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 38, loss = 0.50103178\n",
      "Validation score: 0.791045\n",
      "Iteration 100, loss = 0.46177657\n",
      "Validation score: 0.761194\n",
      "Iteration 56, loss = 0.50382370\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "Iteration 39, loss = 0.50102237\n",
      "Iteration 111, loss = 0.45465861\n",
      "Validation score: 0.791045\n",
      "Validation score: 0.731343\n",
      "Iteration 101, loss = 0.46177661\n",
      "Validation score: 0.761194\n",
      "Iteration 57, loss = 0.50370252\n",
      "Iteration 40, loss = 0.50101381\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.791045\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "Iteration 112, loss = 0.45465919\n",
      "Validation score: 0.731343\n",
      "Iteration 102, loss = 0.46177665\n",
      "Iteration 41, loss = 0.50052946\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.791045\n",
      "Iteration 58, loss = 0.50389284\n",
      "Validation score: 0.761194\n",
      "Iteration 82, loss = 0.48479998\n",
      "Iteration 113, loss = 0.45465929\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 103, loss = 0.46177669\n",
      "Iteration 42, loss = 0.50198166\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.791045\n",
      "Iteration 59, loss = 0.50393673\n",
      "Validation score: 0.761194\n",
      "Iteration 83, loss = 0.48479908\n",
      "Validation score: 0.746269\n",
      "Iteration 114, loss = 0.45465927\n",
      "Iteration 104, loss = 0.46177674\n",
      "Iteration 43, loss = 0.50190582\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 44, loss = 0.50144269\n",
      "Iteration 105, loss = 0.46177678\n",
      "Validation score: 0.776119\n",
      "Iteration 60, loss = 0.50393909\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 115, loss = 0.45465921\n",
      "Validation score: 0.731343\n",
      "Iteration 45, loss = 0.50102789Iteration 84, loss = 0.48479818\n",
      "\n",
      "Iteration 106, loss = 0.46177682\n",
      "Validation score: 0.746269Validation score: 0.776119\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 61, loss = 0.50393022\n",
      "Validation score: 0.761194\n",
      "Iteration 116, loss = 0.45465914\n",
      "Iteration 85, loss = 0.48479730Iteration 46, loss = 0.50074019\n",
      "\n",
      "Validation score: 0.731343\n",
      "Iteration 107, loss = 0.46177686\n",
      "Validation score: 0.746269Validation score: 0.776119\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 62, loss = 0.50391859\n",
      "Validation score: 0.761194\n",
      "Iteration 117, loss = 0.45465907\n",
      "Iteration 47, loss = 0.50055829Iteration 86, loss = 0.48479642\n",
      "\n",
      "Validation score: 0.731343Iteration 108, loss = 0.46177690\n",
      "\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 63, loss = 0.50390656\n",
      "Validation score: 0.761194\n",
      "Iteration 48, loss = 0.50044805\n",
      "Iteration 109, loss = 0.46177694\n",
      "Iteration 87, loss = 0.48479555\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194Validation score: 0.746269\n",
      "\n",
      "Iteration 118, loss = 0.45465900\n",
      "Iteration 64, loss = 0.50389477\n",
      "Iteration 49, loss = 0.50038270\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 88, loss = 0.48479469\n",
      "Iteration 65, loss = 0.50388338\n",
      "Iteration 50, loss = 0.50034442\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 119, loss = 0.45465893\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.731343\n",
      "Iteration 89, loss = 0.48479384\n",
      "Validation score: 0.746269\n",
      "Iteration 66, loss = 0.50387241\n",
      "Validation score: 0.761194\n",
      "Iteration 51, loss = 0.50032202\n",
      "Iteration 120, loss = 0.45465886\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.731343\n",
      "Iteration 90, loss = 0.48479300\n",
      "Validation score: 0.746269\n",
      "Iteration 67, loss = 0.50386186\n",
      "Iteration 52, loss = 0.50030877\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 121, loss = 0.45465879\n",
      "Iteration 91, loss = 0.48479217\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.746269\n",
      "Iteration 53, loss = 0.50030066\n",
      "Iteration 68, loss = 0.50385171\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 122, loss = 0.45465872\n",
      "Iteration 92, loss = 0.48479134\n",
      "Iteration 69, loss = 0.50384194\n",
      "Iteration 54, loss = 0.50029543\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "Validation score: 0.731343\n",
      "Iteration 110, loss = 0.46177699\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 111, loss = 0.46177703Iteration 93, loss = 0.48478432\n",
      "\n",
      "Iteration 123, loss = 0.45465865\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "Iteration 55, loss = 0.50029176\n",
      "Validation score: 0.746269\n",
      "Iteration 70, loss = 0.50383254Validation score: 0.731343\n",
      "\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 112, loss = 0.46177261\n",
      "Validation score: 0.761194\n",
      "Iteration 94, loss = 0.48478745\n",
      "Iteration 56, loss = 0.50028894\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.776119\n",
      "Iteration 71, loss = 0.50382349\n",
      "Iteration 113, loss = 0.46177331\n",
      "Iteration 124, loss = 0.45465858\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 114, loss = 0.46177350\n",
      "Iteration 95, loss = 0.48478817\n",
      "Validation score: 0.761194\n",
      "Iteration 57, loss = 0.50028658\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.776119\n",
      "Iteration 72, loss = 0.50381478\n",
      "Iteration 115, loss = 0.46177356\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 58, loss = 0.50028448\n",
      "Iteration 125, loss = 0.45465851\n",
      "Validation score: 0.776119\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "Validation score: 0.731343\n",
      "Iteration 116, loss = 0.46177358\n",
      "Validation score: 0.761194\n",
      "Iteration 96, loss = 0.48478823\n",
      "Iteration 73, loss = 0.50380638\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 117, loss = 0.46177359\n",
      "Iteration 59, loss = 0.50016589\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 126, loss = 0.45465844\n",
      "Validation score: 0.731343\n",
      "Iteration 97, loss = 0.48478813\n",
      "Iteration 74, loss = 0.50379828\n",
      "Iteration 118, loss = 0.46177360\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119\n",
      "Iteration 60, loss = 0.50042248\n",
      "Validation score: 0.776119\n",
      "Iteration 127, loss = 0.45465837\n",
      "Iteration 119, loss = 0.46177361\n",
      "Validation score: 0.731343\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 98, loss = 0.48478797\n",
      "Validation score: 0.761194\n",
      "Iteration 75, loss = 0.50376968\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 61, loss = 0.50047844\n",
      "Iteration 120, loss = 0.46177362\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 128, loss = 0.45465758\n",
      "Iteration 99, loss = 0.48478781\n",
      "Validation score: 0.746269\n",
      "Iteration 121, loss = 0.46177363\n",
      "Validation score: 0.761194\n",
      "Iteration 76, loss = 0.50380538\n",
      "Validation score: 0.761194\n",
      "Iteration 62, loss = 0.50047549\n",
      "Iteration 122, loss = 0.46177364\n",
      "Iteration 100, loss = 0.48478764\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.746269\n",
      "Iteration 77, loss = 0.50381368\n",
      "Validation score: 0.761194\n",
      "Iteration 123, loss = 0.46177364\n",
      "Validation score: 0.761194\n",
      "Iteration 63, loss = 0.50045696\n",
      "Iteration 78, loss = 0.50381452Iteration 101, loss = 0.48478747\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 124, loss = 0.46177365\n",
      "Iteration 79, loss = 0.50381336\n",
      "Validation score: 0.761194\n",
      "Iteration 64, loss = 0.50043532\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 102, loss = 0.48478730\n",
      "Validation score: 0.746269\n",
      "Iteration 125, loss = 0.46177366\n",
      "Iteration 129, loss = 0.45465805\n",
      "Validation score: 0.761194\n",
      "Iteration 80, loss = 0.50381167\n",
      "Validation score: 0.731343\n",
      "Iteration 65, loss = 0.50041403\n",
      "Validation score: 0.776119\n",
      "Iteration 103, loss = 0.48478713\n",
      "Validation score: 0.761194\n",
      "Iteration 126, loss = 0.46177367\n",
      "Validation score: 0.761194\n",
      "Iteration 66, loss = 0.50039398\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.776119\n",
      "Iteration 127, loss = 0.46177368\n",
      "Iteration 81, loss = 0.50380984\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 1, loss = 0.65915305\n",
      "Iteration 130, loss = 0.45465817\n",
      "Validation score: 0.731343\n",
      "Iteration 104, loss = 0.48478696\n",
      "Iteration 67, loss = 0.50037534\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.776119\n",
      "Iteration 82, loss = 0.50380798\n",
      "Iteration 2, loss = 0.64961696\n",
      "Validation score: 0.761194\n",
      "Iteration 131, loss = 0.45465819\n",
      "Validation score: 0.731343\n",
      "Iteration 105, loss = 0.48478679\n",
      "Iteration 68, loss = 0.50035805\n",
      "Iteration 128, loss = 0.46177369\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 83, loss = 0.50380613\n",
      "Iteration 3, loss = 0.66293610\n",
      "Validation score: 0.761194\n",
      "Iteration 132, loss = 0.45465818\n",
      "Iteration 129, loss = 0.46177369\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 69, loss = 0.50034204\n",
      "Iteration 106, loss = 0.48478662\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.746269\n",
      "Iteration 4, loss = 0.52575351Iteration 84, loss = 0.50380428\n",
      "\n",
      "Iteration 130, loss = 0.46177281\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 70, loss = 0.50032720\n",
      "Iteration 107, loss = 0.48478645\n",
      "Iteration 131, loss = 0.46177295\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 133, loss = 0.45465817\n",
      "Iteration 5, loss = 0.56704384\n",
      "Validation score: 0.746269\n",
      "Iteration 85, loss = 0.50380245\n",
      "Iteration 132, loss = 0.46177299\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 71, loss = 0.50031345\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 133, loss = 0.46177300\n",
      "Iteration 6, loss = 0.49961823\n",
      "Iteration 108, loss = 0.48478628\n",
      "Validation score: 0.761194\n",
      "Iteration 134, loss = 0.45465816\n",
      "Validation score: 0.746269\n",
      "Iteration 86, loss = 0.50380063\n",
      "Validation score: 0.761194\n",
      "Iteration 72, loss = 0.50030069\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.776119\n",
      "Iteration 134, loss = 0.46177300\n",
      "Validation score: 0.761194\n",
      "Iteration 7, loss = 0.56390927\n",
      "Iteration 109, loss = 0.48478612\n",
      "Validation score: 0.746269\n",
      "Iteration 87, loss = 0.50379883\n",
      "Iteration 135, loss = 0.45465814\n",
      "Iteration 73, loss = 0.50028886\n",
      "Iteration 135, loss = 0.46177301Validation score: 0.731343\n",
      "\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 8, loss = 0.51494349\n",
      "Iteration 110, loss = 0.48478595\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "Iteration 136, loss = 0.46177301\n",
      "Validation score: 0.761194\n",
      "Iteration 74, loss = 0.50027788\n",
      "Iteration 136, loss = 0.45465813\n",
      "Iteration 88, loss = 0.50379704\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.761194\n",
      "Iteration 9, loss = 0.53480607\n",
      "Iteration 137, loss = 0.46177301\n",
      "Validation score: 0.761194\n",
      "Iteration 111, loss = 0.48478454\n",
      "Validation score: 0.746269\n",
      "Iteration 75, loss = 0.50026767\n",
      "Validation score: 0.776119\n",
      "Iteration 138, loss = 0.46177301\n",
      "Validation score: 0.761194\n",
      "Iteration 89, loss = 0.50379526\n",
      "Validation score: 0.761194\n",
      "Iteration 10, loss = 0.49807281\n",
      "Iteration 76, loss = 0.50025819\n",
      "Iteration 139, loss = 0.46177301\n",
      "Validation score: 0.776119\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119\n",
      "Validation score: 0.761194\n",
      "Iteration 137, loss = 0.45465812\n",
      "Iteration 112, loss = 0.48478516\n",
      "Validation score: 0.731343\n",
      "Iteration 90, loss = 0.50379350\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 11, loss = 0.58528353\n",
      "Iteration 140, loss = 0.46177302\n",
      "Validation score: 0.761194\n",
      "Iteration 77, loss = 0.50022663\n",
      "Validation score: 0.776119\n",
      "Iteration 138, loss = 0.45465810\n",
      "Iteration 113, loss = 0.48478531\n",
      "Validation score: 0.731343Iteration 91, loss = 0.50379175\n",
      "\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 141, loss = 0.46177302\n",
      "Validation score: 0.761194\n",
      "Iteration 12, loss = 0.51419148\n",
      "Iteration 78, loss = 0.50027342\n",
      "Validation score: 0.776119\n",
      "Iteration 114, loss = 0.48478532\n",
      "Iteration 142, loss = 0.46177302\n",
      "Iteration 92, loss = 0.50379001\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "Iteration 13, loss = 0.52513486\n",
      "Iteration 79, loss = 0.50028435\n",
      "Iteration 143, loss = 0.46177302\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 115, loss = 0.48478530\n",
      "Validation score: 0.746269\n",
      "Iteration 93, loss = 0.50378415\n",
      "Iteration 144, loss = 0.46177302\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 14, loss = 0.50175644\n",
      "Iteration 80, loss = 0.50028545\n",
      "Validation score: 0.776119\n",
      "Iteration 145, loss = 0.46177302\n",
      "Iteration 116, loss = 0.48478527\n",
      "Iteration 81, loss = 0.50028390\n",
      "Iteration 94, loss = 0.50379121\n",
      "Iteration 15, loss = 0.53823884\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 117, loss = 0.48478524\n",
      "Iteration 82, loss = 0.50028165\n",
      "Iteration 139, loss = 0.45465809\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.731343\n",
      "Iteration 1, loss = 0.66947772\n",
      "Iteration 83, loss = 0.50027924Iteration 118, loss = 0.48478520\n",
      "\n",
      "Iteration 95, loss = 0.50379285\n",
      "Validation score: 0.746269Validation score: 0.776119\n",
      "\n",
      "Iteration 140, loss = 0.45465807\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 2, loss = 0.56192670\n",
      "Iteration 16, loss = 0.50505280\n",
      "Iteration 84, loss = 0.50027680\n",
      "Iteration 119, loss = 0.48478517\n",
      "Iteration 96, loss = 0.50379303\n",
      "Iteration 3, loss = 0.56969557\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 1, loss = 0.64332644\n",
      "Iteration 4, loss = 0.55804510Iteration 141, loss = 0.45465806\n",
      "\n",
      "Iteration 85, loss = 0.50027439\n",
      "Validation score: 0.731343\n",
      "Iteration 120, loss = 0.48478514\n",
      "Validation score: 0.776119\n",
      "Iteration 97, loss = 0.50379282\n",
      "Validation score: 0.746269\n",
      "Iteration 2, loss = 0.54056667\n",
      "Validation score: 0.761194\n",
      "Iteration 5, loss = 0.54074575\n",
      "Validation score: 0.761194\n",
      "Iteration 142, loss = 0.45465805\n",
      "Iteration 121, loss = 0.48478510\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.731343\n",
      "Iteration 3, loss = 0.57627091\n",
      "Iteration 98, loss = 0.50379250\n",
      "Iteration 6, loss = 0.52277473\n",
      "Iteration 146, loss = 0.46177303\n",
      "Validation score: 0.761194\n",
      "Iteration 122, loss = 0.48478507\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 4, loss = 0.53994732\n",
      "Iteration 143, loss = 0.45465803\n",
      "Iteration 147, loss = 0.46177303\n",
      "Validation score: 0.731343\n",
      "Iteration 123, loss = 0.48478504\n",
      "Iteration 86, loss = 0.50027201\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 99, loss = 0.50379215\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.776119\n",
      "Iteration 17, loss = 0.51940683\n",
      "Validation score: 0.761194\n",
      "Iteration 5, loss = 0.51585748\n",
      "Iteration 144, loss = 0.45465802\n",
      "Validation score: 0.731343\n",
      "Iteration 87, loss = 0.50026966Iteration 124, loss = 0.48478500\n",
      "\n",
      "Validation score: 0.776119Validation score: 0.746269\n",
      "\n",
      "Iteration 18, loss = 0.51960306\n",
      "Iteration 7, loss = 0.52143091\n",
      "Iteration 6, loss = 0.61683628\n",
      "Iteration 100, loss = 0.50379180\n",
      "Iteration 8, loss = 0.52628215\n",
      "Iteration 125, loss = 0.48478497\n",
      "Validation score: 0.761194\n",
      "Iteration 7, loss = 0.55861442\n",
      "Validation score: 0.746269\n",
      "Iteration 19, loss = 0.50548216\n",
      "Iteration 101, loss = 0.50379144\n",
      "Iteration 9, loss = 0.55424847Validation score: 0.761194\n",
      "\n",
      "Iteration 126, loss = 0.48478494\n",
      "Iteration 8, loss = 0.52048628\n",
      "Validation score: 0.746269\n",
      "Iteration 20, loss = 0.52445562\n",
      "Iteration 102, loss = 0.50379108\n",
      "Validation score: 0.761194\n",
      "Iteration 10, loss = 0.54617938\n",
      "Iteration 1, loss = 0.60377225\n",
      "Iteration 9, loss = 0.52788169\n",
      "Iteration 127, loss = 0.48478490\n",
      "Iteration 21, loss = 0.51315421\n",
      "Validation score: 0.731343\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.\n",
      "Validation score: 0.746269\n",
      "Iteration 103, loss = 0.50379073\n",
      "Validation score: 0.761194\n",
      "Iteration 11, loss = 0.50624637\n",
      "Iteration 10, loss = 0.52384724\n",
      "Iteration 2, loss = 0.55138227\n",
      "Iteration 128, loss = 0.48478487\n",
      "Validation score: 0.776119\n",
      "Iteration 104, loss = 0.50379037\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "Validation score: 0.761194\n",
      "Iteration 88, loss = 0.50026735\n",
      "Validation score: 0.776119\n",
      "Iteration 12, loss = 0.53493073\n",
      "Iteration 11, loss = 0.51457021\n",
      "Iteration 3, loss = 0.52143713\n",
      "Iteration 145, loss = 0.45465800\n",
      "Validation score: 0.776119\n",
      "Iteration 89, loss = 0.50026506\n",
      "Validation score: 0.776119\n",
      "Iteration 105, loss = 0.50379002\n",
      "Validation score: 0.731343\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 13, loss = 0.52661303\n",
      "Iteration 1, loss = 0.82172131\n",
      "Validation score: 0.761194\n",
      "Iteration 129, loss = 0.48478459\n",
      "Iteration 12, loss = 0.53255895\n",
      "Validation score: 0.746269\n",
      "Iteration 4, loss = 0.51630198\n",
      "Validation score: 0.791045Iteration 90, loss = 0.50026281\n",
      "\n",
      "Validation score: 0.776119\n",
      "Iteration 106, loss = 0.50378966\n",
      "Iteration 14, loss = 0.55324498\n",
      "Iteration 2, loss = 0.66481391\n",
      "Validation score: 0.761194\n",
      "Iteration 130, loss = 0.48478471\n",
      "Iteration 13, loss = 0.52008474\n",
      "Validation score: 0.746269\n",
      "Iteration 5, loss = 0.50988603\n",
      "Iteration 91, loss = 0.50026059\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 15, loss = 0.52979566\n",
      "Iteration 107, loss = 0.50378931\n",
      "Iteration 3, loss = 0.54208318\n",
      "Validation score: 0.761194\n",
      "Iteration 14, loss = 0.52606776\n",
      "Iteration 131, loss = 0.48478474\n",
      "Validation score: 0.746269\n",
      "Iteration 6, loss = 0.50836415\n",
      "Iteration 92, loss = 0.50025841\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 16, loss = 0.53338098\n",
      "Iteration 4, loss = 0.57444408\n",
      "Iteration 108, loss = 0.50378896\n",
      "Validation score: 0.761194\n",
      "Iteration 15, loss = 0.52191289\n",
      "Iteration 132, loss = 0.48478474\n",
      "Iteration 1, loss = 0.54908914\n",
      "Iteration 7, loss = 0.50689893\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 17, loss = 0.54273357\n",
      "Iteration 16, loss = 0.53847542\n",
      "Iteration 5, loss = 0.59844808\n",
      "Validation score: 0.731343\n",
      "Iteration 109, loss = 0.50378860\n",
      "Iteration 8, loss = 0.50613388\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 133, loss = 0.48478474\n",
      "Iteration 18, loss = 0.51983843\n",
      "Validation score: 0.746269\n",
      "Iteration 17, loss = 0.50567294\n",
      "Iteration 6, loss = 0.56862307\n",
      "Iteration 2, loss = 0.54837811\n",
      "Iteration 110, loss = 0.50378825\n",
      "Validation score: 0.731343\n",
      "Iteration 9, loss = 0.50532309\n",
      "Iteration 134, loss = 0.48478473\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 19, loss = 0.52965025\n",
      "Iteration 18, loss = 0.52621245\n",
      "Iteration 7, loss = 0.52626953\n",
      "Iteration 135, loss = 0.48478473\n",
      "Iteration 111, loss = 0.50378707\n",
      "Validation score: 0.746269\n",
      "Iteration 10, loss = 0.50470988\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 3, loss = 0.49582968\n",
      "Iteration 20, loss = 0.54109872\n",
      "Validation score: 0.716418\n",
      "Iteration 19, loss = 0.53630573Iteration 8, loss = 0.52887629\n",
      "\n",
      "Iteration 112, loss = 0.50378848\n",
      "Iteration 11, loss = 0.50420448\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 4, loss = 0.51090118\n",
      "Iteration 136, loss = 0.48478472\n",
      "Iteration 21, loss = 0.52381350\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.746269\n",
      "Iteration 20, loss = 0.52944968\n",
      "Iteration 9, loss = 0.55811830\n",
      "Iteration 93, loss = 0.50025625\n",
      "Iteration 22, loss = 0.52436144\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.\n",
      "Validation score: 0.776119\n",
      "Iteration 21, loss = 0.53963911\n",
      "Iteration 5, loss = 0.50309601\n",
      "Iteration 137, loss = 0.48478471\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.746269\n",
      "Iteration 10, loss = 0.60461024\n",
      "Iteration 94, loss = 0.50025412\n",
      "Iteration 12, loss = 0.50382624\n",
      "Validation score: 0.776119\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "Validation score: 0.776119\n",
      "Iteration 22, loss = 0.52364007\n",
      "Iteration 138, loss = 0.48478471\n",
      "Validation score: 0.746269Iteration 95, loss = 0.50024753\n",
      "Iteration 113, loss = 0.50378881\n",
      "Iteration 11, loss = 0.52595478\n",
      "Iteration 13, loss = 0.50353633\n",
      "Validation score: 0.761194\n",
      "Iteration 23, loss = 0.52891005\n",
      "Validation score: 0.761194\n",
      "Iteration 1, loss = 0.64238368\n",
      "Iteration 139, loss = 0.48478470\n",
      "Validation score: 0.746269\n",
      "Iteration 14, loss = 0.50331245Iteration 24, loss = 0.52583196\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 2, loss = 0.58051053Iteration 12, loss = 0.54893376\n",
      "\n",
      "Iteration 140, loss = 0.48478469\n",
      "Validation score: 0.746269\n",
      "Iteration 6, loss = 0.50351380\n",
      "Iteration 114, loss = 0.50378885\n",
      "\n",
      "Iteration 25, loss = 0.49617889\n",
      "Validation score: 0.776119\n",
      "Iteration 15, loss = 0.50313854\n",
      "Validation score: 0.761194\n",
      "Iteration 3, loss = 0.55512014\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.716418\n",
      "Iteration 26, loss = 0.52143493\n",
      "Iteration 16, loss = 0.50300249Iteration 96, loss = 0.50025672\n",
      "\n",
      "Iteration 4, loss = 0.54486502\n",
      "Iteration 115, loss = 0.50378881\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 141, loss = 0.48478469\n",
      "Iteration 27, loss = 0.55844689\n",
      "Validation score: 0.746269\n",
      "Iteration 7, loss = 0.50166505\n",
      "Iteration 13, loss = 0.53169544\n",
      "Iteration 17, loss = 0.50289740\n",
      "Validation score: 0.716418\n",
      "Iteration 5, loss = 0.59941833\n",
      "Validation score: 0.761194\n",
      "Iteration 116, loss = 0.50378874\n",
      "Iteration 142, loss = 0.48478468Validation score: 0.761194\n",
      "Iteration 28, loss = 0.52287121\n",
      "Iteration 8, loss = 0.50110722\n",
      "Iteration 14, loss = 0.52955509\n",
      "Iteration 97, loss = 0.50025887\n",
      "Iteration 18, loss = 0.50281605\n",
      "Iteration 6, loss = 0.57254322Validation score: 0.716418\n",
      "\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 117, loss = 0.50378867\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 29, loss = 0.52151360\n",
      "Iteration 9, loss = 0.50101532\n",
      "Iteration 19, loss = 0.50275366\n",
      "Iteration 7, loss = 0.56518101\n",
      "Iteration 15, loss = 0.54009553\n",
      "Validation score: 0.761194\n",
      "Iteration 143, loss = 0.48478467\n",
      "Iteration 30, loss = 0.51425429\n",
      "Iteration 118, loss = 0.50378860\n",
      "Iteration 98, loss = 0.50025913\n",
      "Iteration 8, loss = 0.55312726\n",
      "Validation score: 0.761194\n",
      "Iteration 20, loss = 0.50270561Validation score: 0.776119\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.716418\n",
      "Iteration 31, loss = 0.52367013\n",
      "Iteration 119, loss = 0.50378853\n",
      "Iteration 99, loss = 0.50025887\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 9, loss = 0.52586344\n",
      "Iteration 21, loss = 0.50266876\n",
      "Validation score: 0.776119\n",
      "Iteration 10, loss = 0.50063856Validation score: 0.761194\n",
      "\n",
      "Iteration 144, loss = 0.48478467\n",
      "Validation score: 0.716418Iteration 32, loss = 0.55646614\n",
      "\n",
      "Iteration 100, loss = 0.50025847Iteration 120, loss = 0.50378846\n",
      "Iteration 16, loss = 0.52825419\n",
      "Iteration 10, loss = 0.52579160\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194Iteration 22, loss = 0.50264038\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910\n",
      "Iteration 145, loss = 0.48478466\n",
      "Iteration 11, loss = 0.50055843\n",
      "Validation score: 0.746269\n",
      "Iteration 17, loss = 0.51945064\n",
      "Iteration 33, loss = 0.51923048\n",
      "Validation score: 0.716418\n",
      "Iteration 11, loss = 0.51667326\n",
      "Iteration 121, loss = 0.50378839\n",
      "Iteration 23, loss = 0.49968948\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 146, loss = 0.48478465\n",
      "Validation score: 0.746269\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 18, loss = 0.53851421\n",
      "Validation score: 0.776119Iteration 12, loss = 0.55844545\n",
      "Iteration 34, loss = 0.51982299\n",
      "Iteration 24, loss = 0.50145124\n",
      "Iteration 122, loss = 0.50378832\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 19, loss = 0.54782226\n",
      "Iteration 13, loss = 0.53941051\n",
      "Iteration 35, loss = 0.51948954\n",
      "Iteration 123, loss = 0.50378825\n",
      "Iteration 25, loss = 0.50101640\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 20, loss = 0.52184960\n",
      "Iteration 14, loss = 0.51344437\n",
      "Iteration 36, loss = 0.51802448\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.Iteration 124, loss = 0.50378818\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 1, loss = 0.62351288\n",
      "Iteration 26, loss = 0.50055583\n",
      "Validation score: 0.731343Iteration 21, loss = 0.50861075\n",
      "\n",
      "Validation score: 0.761194\n",
      "\n",
      "Iteration 15, loss = 0.53287686\n",
      "Iteration 12, loss = 0.50027992\n",
      "Validation score: 0.716418\n",
      "Iteration 101, loss = 0.50025803\n",
      "Iteration 22, loss = 0.52036677\n",
      "Iteration 27, loss = 0.50029707\n",
      "Iteration 2, loss = 0.55666885\n",
      "Validation score: 0.776119\n",
      "Iteration 16, loss = 0.56943372\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 102, loss = 0.50025759\n",
      "Iteration 125, loss = 0.50378811\n",
      "Iteration 23, loss = 0.54860211\n",
      "Iteration 13, loss = 0.50016919\n",
      "Validation score: 0.776119\n",
      "Iteration 1, loss = 0.95939161\n",
      "Iteration 28, loss = 0.50023939Iteration 17, loss = 0.53103123\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.716418\n",
      "Iteration 3, loss = 0.54109531Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 103, loss = 0.50025715\n",
      "Iteration 24, loss = 0.58076695Iteration 126, loss = 0.50378804\n",
      "\n",
      "Validation score: 0.776119\n",
      "Iteration 2, loss = 0.60502250\n",
      "Iteration 18, loss = 0.56314080\n",
      "Iteration 29, loss = 0.50017228Iteration 14, loss = 0.50000059\n",
      "Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.716418Validation score: 0.761194\n",
      "\n",
      "Iteration 4, loss = 0.53287512\n",
      "Validation score: 0.776119\n",
      "Iteration 104, loss = 0.50025671\n",
      "Validation score: 0.776119\n",
      "Iteration 127, loss = 0.50378797\n",
      "Iteration 25, loss = 0.52270498Iteration 3, loss = 0.57546277\n",
      "Iteration 19, loss = 0.52262736\n",
      "\n",
      "Iteration 30, loss = 0.50007180Validation score: 0.761194\n",
      "\n",
      "Iteration 15, loss = 0.49989020\n",
      "Validation score: 0.761194Validation score: 0.716418\n",
      "\n",
      "Iteration 5, loss = 0.53010483\n",
      "Validation score: 0.761194\n",
      "Iteration 128, loss = 0.50378790Iteration 4, loss = 0.60752266\n",
      "Iteration 20, loss = 0.54074172\n",
      "\n",
      "Iteration 26, loss = 0.51202489\n",
      "Iteration 16, loss = 0.49976396\n",
      "Iteration 31, loss = 0.49998009Validation score: 0.761194\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "Validation score: 0.761194Validation score: 0.716418\n",
      "Iteration 6, loss = 0.52767394\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 21, loss = 0.55859763Iteration 5, loss = 0.53615071\n",
      "Iteration 27, loss = 0.53619306\n",
      "Iteration 129, loss = 0.50378766\n",
      "Iteration 32, loss = 0.49991361\n",
      "Iteration 7, loss = 0.52569578\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "\n",
      "Iteration 17, loss = 0.49965456\n",
      "Validation score: 0.716418\n",
      "Iteration 22, loss = 0.54284855\n",
      "Iteration 105, loss = 0.50025626Iteration 33, loss = 0.49986411\n",
      "\n",
      "Iteration 8, loss = 0.52409162\n",
      "Iteration 6, loss = 0.54434834\n",
      "Validation score: 0.761194Validation score: 0.776119\n",
      "Validation score: 0.776119\n",
      "\n",
      "Iteration 130, loss = 0.50378794\n",
      "Iteration 18, loss = 0.49954541\n",
      "Iteration 28, loss = 0.53504182\n",
      "Validation score: 0.761194\n",
      "Iteration 23, loss = 0.61111211\n",
      "Validation score: 0.716418\n",
      "Iteration 106, loss = 0.50025582Iteration 9, loss = 0.52279655\n",
      "Iteration 34, loss = 0.49982216\n",
      "Iteration 7, loss = 0.55023859\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 24, loss = 0.54629692\n",
      "Iteration 131, loss = 0.50378801\n",
      "Iteration 29, loss = 0.53810010Iteration 19, loss = 0.49944265\n",
      "\n",
      "Validation score: 0.716418Iteration 10, loss = 0.52186092\n",
      "\n",
      "Validation score: 0.791045\n",
      "\n",
      "Iteration 25, loss = 0.52395771\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.Validation score: 0.776119\n",
      "\n",
      "Iteration 8, loss = 0.52737806\n",
      "Iteration 20, loss = 0.49934299\n",
      "Iteration 30, loss = 0.50787499\n",
      "Validation score: 0.716418\n",
      "Iteration 11, loss = 0.52111475\n",
      "Iteration 35, loss = 0.49978536\n",
      "Validation score: 0.776119\n",
      "Iteration 107, loss = 0.50025538\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 9, loss = 0.52125580\n",
      "Iteration 12, loss = 0.52055858\n",
      "Iteration 36, loss = 0.49975415\n",
      "Iteration 31, loss = 0.52845497\n",
      "Iteration 108, loss = 0.50025494\n",
      "Validation score: 0.776119\n",
      "Iteration 21, loss = 0.49924616\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.716418\n",
      "Iteration 10, loss = 0.52039554\n",
      "Iteration 13, loss = 0.52011283\n",
      "Iteration 32, loss = 0.52926186\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.\n",
      "Validation score: 0.776119\n",
      "Iteration 22, loss = 0.49915176\n",
      "Iteration 37, loss = 0.49972827\n",
      "Validation score: 0.716418\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910\n",
      "Validation score: 0.761194\n",
      "Iteration 1, loss = 0.70284052\n",
      "Iteration 11, loss = 0.53827932\n",
      "Iteration 14, loss = 0.51977273\n",
      "Validation score: 0.761194\n",
      "Iteration 23, loss = 0.49306407\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Iteration 38, loss = 0.49970665\n",
      "Validation score: 0.761194\n",
      "Iteration 2, loss = 0.62023591\n",
      "Iteration 15, loss = 0.51950045\n",
      "Validation score: 0.761194\n",
      "Iteration 12, loss = 0.55217647\n",
      "Iteration 132, loss = 0.50378802\n",
      "Iteration 39, loss = 0.49968836\n",
      "Validation score: 0.761194\n",
      "Iteration 24, loss = 0.51437412\n",
      "Validation score: 0.761194\n",
      "Iteration 3, loss = 0.57947823\n",
      "Iteration 16, loss = 0.51928467\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.716418\n",
      "Iteration 13, loss = 0.58701124\n",
      "Iteration 40, loss = 0.49967283\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "Iteration 17, loss = 0.51910502\n",
      "Iteration 4, loss = 0.54846779Iteration 109, loss = 0.50025451\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 1, loss = 0.81800435\n",
      "Iteration 25, loss = 0.50074354\n",
      "Validation score: 0.701493\n",
      "Iteration 14, loss = 0.54074158Iteration 41, loss = 0.49891831\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 110, loss = 0.50025407Iteration 5, loss = 0.52991558\n",
      "\n",
      "Iteration 18, loss = 0.51895280\n",
      "Iteration 133, loss = 0.50378801\n",
      "Validation score: 0.776119Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.761194Iteration 2, loss = 0.71422182\n",
      "Iteration 26, loss = 0.49345610\n",
      "\n",
      "Validation score: 0.716418\n",
      "Iteration 15, loss = 0.53918940\n",
      "Iteration 6, loss = 0.53066504\n",
      "Iteration 111, loss = 0.50025364\n",
      "Iteration 42, loss = 0.49927011Iteration 19, loss = 0.51881820\n",
      "\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194Iteration 3, loss = 0.56263778\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 27, loss = 0.49070985\n",
      "Validation score: 0.701493\n",
      "Iteration 134, loss = 0.50378799Iteration 112, loss = 0.50025320\n",
      "\n",
      "Iteration 7, loss = 0.55005619\n",
      "Iteration 43, loss = 0.49934184\n",
      "Iteration 20, loss = 0.51869632\n",
      "Validation score: 0.776119\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 113, loss = 0.50025187\n",
      "Iteration 28, loss = 0.49060198\n",
      "Iteration 8, loss = 0.53950992\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.701493\n",
      "Iteration 44, loss = 0.49933486\n",
      "Iteration 135, loss = 0.50378798\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "\n",
      "Iteration 114, loss = 0.50025370\n",
      "Validation score: 0.776119\n",
      "Iteration 9, loss = 0.54756927\n",
      "Iteration 4, loss = 0.52852820\n",
      "Iteration 29, loss = 0.49122315\n",
      "Iteration 45, loss = 0.49931457\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 136, loss = 0.50378797\n",
      "Iteration 115, loss = 0.50025413\n",
      "Validation score: 0.761194Validation score: 0.776119\n",
      "\n",
      "Iteration 5, loss = 0.53051347\n",
      "Iteration 10, loss = 0.55036596\n",
      "Iteration 30, loss = 0.49136187\n",
      "Iteration 16, loss = 0.53068740\n",
      "Iteration 46, loss = 0.49929635\n",
      "Validation score: 0.701493\n",
      "Iteration 116, loss = 0.50025419\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 137, loss = 0.50378795\n",
      "Validation score: 0.761194Iteration 21, loss = 0.51858262\n",
      "\n",
      "Iteration 11, loss = 0.57729879\n",
      "Validation score: 0.761194\n",
      "Iteration 6, loss = 0.56489315\n",
      "Iteration 31, loss = 0.49115330\n",
      "Iteration 47, loss = 0.49928196\n",
      "Iteration 17, loss = 0.55076754\n",
      "Validation score: 0.701493\n",
      "Iteration 117, loss = 0.50025414Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.776119\n",
      "Iteration 138, loss = 0.50378794\n",
      "Iteration 22, loss = 0.51847463\n",
      "Iteration 12, loss = 0.54657327\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "\n",
      "Iteration 48, loss = 0.49927052\n",
      "Iteration 118, loss = 0.50025406Iteration 32, loss = 0.49094023\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 18, loss = 0.58497404\n",
      "Iteration 7, loss = 0.49614650Iteration 13, loss = 0.53355907\n",
      "Iteration 139, loss = 0.50378792\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 119, loss = 0.50025397Iteration 49, loss = 0.49926107\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119Validation score: 0.761194\n",
      "\n",
      "Iteration 14, loss = 0.53540816\n",
      "Iteration 33, loss = 0.49083427\n",
      "Validation score: 0.701493Iteration 140, loss = 0.50378791\n",
      "\n",
      "Iteration 120, loss = 0.50025389\n",
      "Validation score: 0.776119Iteration 23, loss = 0.51837036Validation score: 0.761194\n",
      "\n",
      "Iteration 50, loss = 0.49925290\n",
      "\n",
      "Iteration 8, loss = 0.52224421\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "\n",
      "Iteration 15, loss = 0.55836269\n",
      "Iteration 121, loss = 0.50025380\n",
      "Iteration 34, loss = 0.49079645\n",
      "Iteration 141, loss = 0.50378790\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 9, loss = 0.51040210\n",
      "Iteration 51, loss = 0.49924560\n",
      "Validation score: 0.761194\n",
      "Iteration 16, loss = 0.56355250\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.55905679\n",
      "Iteration 122, loss = 0.50025371\n",
      "Iteration 24, loss = 0.51826872\n",
      "Iteration 142, loss = 0.50378788\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.776119\n",
      "Iteration 35, loss = 0.49077309\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.701493\n",
      "Iteration 52, loss = 0.49923891\n",
      "Validation score: 0.761194\n",
      "Iteration 123, loss = 0.50025362\n",
      "Iteration 10, loss = 0.55418936Validation score: 0.776119\n",
      "\n",
      "Iteration 36, loss = 0.49074579\n",
      "Iteration 143, loss = 0.50378787\n",
      "Iteration 124, loss = 0.50025354Validation score: 0.701493Iteration 53, loss = 0.49923266\n",
      "\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194Iteration 20, loss = 0.53669826\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.\n",
      "Validation score: 0.776119\n",
      "\n",
      "Iteration 11, loss = 0.62999203\n",
      "Iteration 1, loss = 0.81370165\n",
      "Iteration 125, loss = 0.50025345\n",
      "Iteration 144, loss = 0.50378785\n",
      "Iteration 37, loss = 0.49071830\n",
      "Iteration 54, loss = 0.49922674\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 25, loss = 0.51816894\n",
      "Iteration 2, loss = 0.53641983\n",
      "Validation score: 0.776119\n",
      "Iteration 126, loss = 0.50025336Iteration 145, loss = 0.50378784Iteration 38, loss = 0.49069602\n",
      "\n",
      "\n",
      "Iteration 12, loss = 0.50421691\n",
      "Iteration 55, loss = 0.49922110\n",
      "Validation score: 0.761194Validation score: 0.701493\n",
      "\n",
      "Validation score: 0.776119Validation score: 0.761194\n",
      "\n",
      "Iteration 26, loss = 0.51807062\n",
      "Iteration 3, loss = 0.50676788\n",
      "Validation score: 0.776119\n",
      "Iteration 1, loss = 0.73279773\n",
      "Iteration 13, loss = 0.57224172\n",
      "Iteration 127, loss = 0.50025327\n",
      "Iteration 56, loss = 0.49921569\n",
      "Iteration 39, loss = 0.49067956Iteration 146, loss = 0.50378783\n",
      "\n",
      "Validation score: 0.776119Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Validation score: 0.701493\n",
      "Iteration 4, loss = 0.53393037\n",
      "Iteration 128, loss = 0.50025319\n",
      "Validation score: 0.776119\n",
      "Iteration 57, loss = 0.49921048\n",
      "Validation score: 0.761194\n",
      "Iteration 14, loss = 0.57362365\n",
      "Iteration 40, loss = 0.49066703\n",
      "Iteration 27, loss = 0.51797353\n",
      "Iteration 5, loss = 0.55457323\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.701493\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "Iteration 129, loss = 0.50025310\n",
      "Validation score: 0.776119\n",
      "Iteration 2, loss = 0.60087827\n",
      "Iteration 58, loss = 0.49920545\n",
      "Iteration 15, loss = 0.53517989\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "Iteration 41, loss = 0.48765560Iteration 6, loss = 0.55059923\n",
      "\n",
      "Validation score: 0.686567Iteration 130, loss = 0.50025301\n",
      "\n",
      "Iteration 28, loss = 0.51787759\n",
      "Iteration 3, loss = 0.56839691Validation score: 0.776119\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "\n",
      "Iteration 16, loss = 0.51378965\n",
      "Iteration 59, loss = 0.49904649\n",
      "Validation score: 0.761194\n",
      "Iteration 7, loss = 0.56572982Iteration 131, loss = 0.50025275\n",
      "\n",
      "Validation score: 0.776119Iteration 42, loss = 0.49136911Iteration 1, loss = 0.67758095\n",
      "\n",
      "\n",
      "Validation score: 0.686567Validation score: 0.731343\n",
      "Iteration 4, loss = 0.55432803\n",
      "\n",
      "Iteration 17, loss = 0.52177751\n",
      "Iteration 60, loss = 0.49910342\n",
      "Iteration 132, loss = 0.50025311\n",
      "Validation score: 0.761194\n",
      "Iteration 8, loss = 0.55754561Validation score: 0.776119\n",
      "\n",
      "Iteration 2, loss = 0.52955372\n",
      "Iteration 43, loss = 0.49190237\n",
      "Iteration 5, loss = 0.54216565Validation score: 0.776119\n",
      "Validation score: 0.686567\n",
      "\n",
      "Validation score: 0.776119\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910\n",
      "Iteration 133, loss = 0.50025320\n",
      "Iteration 61, loss = 0.49911853\n",
      "Iteration 18, loss = 0.51291756\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 9, loss = 0.54115093\n",
      "Iteration 44, loss = 0.49148523\n",
      "Iteration 29, loss = 0.51308781\n",
      "Iteration 3, loss = 0.53347710\n",
      "Iteration 134, loss = 0.50025321\n",
      "Iteration 62, loss = 0.49912184Validation score: 0.791045\n",
      "\n",
      "Validation score: 0.686567Validation score: 0.776119\n",
      "\n",
      "Iteration 10, loss = 0.55194760Validation score: 0.761194\n",
      "\n",
      "Iteration 6, loss = 0.53253638\n",
      "Iteration 135, loss = 0.50025320\n",
      "Iteration 4, loss = 0.51130748\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Iteration 45, loss = 0.49095164\n",
      "Validation score: 0.776119\n",
      "Iteration 63, loss = 0.49912189\n",
      "Validation score: 0.701493\n",
      "Iteration 11, loss = 0.51220176\n",
      "Validation score: 0.761194\n",
      "Iteration 1, loss = 0.62782414\n",
      "Iteration 30, loss = 0.51591124Iteration 7, loss = 0.52402761\n",
      "Iteration 5, loss = 0.51560262Iteration 136, loss = 0.50025318\n",
      "\n",
      "Validation score: 0.776119\n",
      "Iteration 46, loss = 0.49051309\n",
      "Validation score: 0.761194\n",
      "Iteration 64, loss = 0.49912106Validation score: 0.701493\n",
      "Iteration 12, loss = 0.51955651\n",
      "\n",
      "Iteration 2, loss = 0.59824562\n",
      "Validation score: 0.761194\n",
      "Iteration 8, loss = 0.51738716Iteration 137, loss = 0.50025317\n",
      "\n",
      "\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 47, loss = 0.49019049\n",
      "Iteration 13, loss = 0.51821742\n",
      "Iteration 65, loss = 0.49912001\n",
      "Validation score: 0.701493\n",
      "Iteration 3, loss = 0.51625155Iteration 6, loss = 0.51095025\n",
      "Validation score: 0.761194\n",
      "\n",
      "Iteration 138, loss = 0.50025315\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.776119\n",
      "Iteration 48, loss = 0.48996078Iteration 14, loss = 0.50110191\n",
      "\n",
      "Iteration 31, loss = 0.51289573\n",
      "Iteration 66, loss = 0.49911893\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.776119Iteration 139, loss = 0.50025313Validation score: 0.761194\n",
      "\n",
      "Iteration 4, loss = 0.56566637\n",
      "Validation score: 0.776119\n",
      "Iteration 7, loss = 0.50972580\n",
      "Iteration 9, loss = 0.51332082\n",
      "Validation score: 0.761194\n",
      "Iteration 15, loss = 0.52298199\n",
      "Iteration 67, loss = 0.49911785\n",
      "Iteration 140, loss = 0.50025312\n",
      "\n",
      "Iteration 5, loss = 0.53615809Validation score: 0.776119\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 49, loss = 0.48979789\n",
      "Iteration 10, loss = 0.51052339\n",
      "Iteration 8, loss = 0.50825319\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 16, loss = 0.50040394\n",
      "Iteration 32, loss = 0.51152629Iteration 141, loss = 0.50025310\n",
      "\n",
      "Iteration 68, loss = 0.49911679\n",
      "Iteration 6, loss = 0.53786963\n",
      "Validation score: 0.776119Validation score: 0.776119\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 50, loss = 0.48968153\n",
      "Iteration 11, loss = 0.50603730\n",
      "Validation score: 0.701493\n",
      "Iteration 9, loss = 0.50686992\n",
      "Iteration 17, loss = 0.52452448\n",
      "Iteration 33, loss = 0.51144261\n",
      "Iteration 142, loss = 0.50025308\n",
      "Iteration 69, loss = 0.49911575\n",
      "Iteration 7, loss = 0.51837516\n",
      "Validation score: 0.776119Validation score: 0.761194\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 12, loss = 0.50331564Iteration 51, loss = 0.48959736\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 18, loss = 0.51185710Iteration 143, loss = 0.50025306\n",
      "\n",
      "Validation score: 0.776119\n",
      "Iteration 70, loss = 0.49911473\n",
      "Iteration 8, loss = 0.50484815\n",
      "Validation score: 0.761194\n",
      "Iteration 52, loss = 0.48953560\n",
      "Iteration 144, loss = 0.50025305\n",
      "Iteration 19, loss = 0.60513376\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.776119\n",
      "\n",
      "Iteration 71, loss = 0.49911372\n",
      "Iteration 9, loss = 0.52495414\n",
      "Validation score: 0.761194\n",
      "Iteration 53, loss = 0.48948954\n",
      "Iteration 13, loss = 0.50031012\n",
      "Iteration 145, loss = 0.50025303\n",
      "Validation score: 0.701493Iteration 34, loss = 0.51164177\n",
      "\n",
      "Iteration 20, loss = 0.50808515\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.776119\n",
      "Iteration 72, loss = 0.49911272\n",
      "Iteration 10, loss = 0.48572206\n",
      "Iteration 10, loss = 0.50630505\n",
      "Validation score: 0.761194\n",
      "Iteration 14, loss = 0.49734352\n",
      "Iteration 146, loss = 0.50025301\n",
      "Validation score: 0.761194\n",
      "Iteration 21, loss = 0.56038586Validation score: 0.776119\n",
      "\n",
      "Iteration 54, loss = 0.48945461\n",
      "Validation score: 0.701493\n",
      "Iteration 73, loss = 0.49911174Iteration 11, loss = 0.49921608\n",
      "\n",
      "Iteration 147, loss = 0.50025299\n",
      "Iteration 11, loss = 0.50564351\n",
      "Validation score: 0.761194\n",
      "Iteration 15, loss = 0.49503656\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 22, loss = 0.50461373\n",
      "Iteration 55, loss = 0.48942764\n",
      "Validation score: 0.701493\n",
      "Iteration 148, loss = 0.50025298Iteration 74, loss = 0.49911076\n",
      "\n",
      "Iteration 12, loss = 0.52802059\n",
      "Validation score: 0.776119Validation score: 0.761194\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 35, loss = 0.51170406\n",
      "Validation score: 0.761194\n",
      "Iteration 12, loss = 0.50537455Iteration 23, loss = 0.53160378\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 75, loss = 0.49910979\n",
      "Iteration 56, loss = 0.48940642\n",
      "Validation score: 0.761194\n",
      "Iteration 13, loss = 0.48570864\n",
      "Validation score: 0.701493Iteration 24, loss = 0.51518444\n",
      "\n",
      "Iteration 13, loss = 0.50507361\n",
      "Validation score: 0.761194\n",
      "Iteration 76, loss = 0.49910883Iteration 36, loss = 0.51161683\n",
      "\n",
      "Iteration 57, loss = 0.48938938\n",
      "Iteration 25, loss = 0.53548159\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 14, loss = 0.50489713\n",
      "Iteration 1, loss = 0.61309480\n",
      "Iteration 14, loss = 0.57537620\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.731343\n",
      "Validation score: 0.776119\n",
      "Iteration 58, loss = 0.48937542\n",
      "Iteration 77, loss = 0.49907691\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.701493\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "Iteration 15, loss = 0.50472784Iteration 15, loss = 0.51516271\n",
      "\n",
      "Iteration 2, loss = 0.55269805\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.776119Iteration 16, loss = 0.49416385\n",
      "\n",
      "Iteration 78, loss = 0.49908745\n",
      "Iteration 59, loss = 0.48869029\n",
      "Validation score: 0.761194\n",
      "Iteration 3, loss = 0.49752048\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 37, loss = 0.51151617\n",
      "Iteration 16, loss = 0.55859104\n",
      "Validation score: 0.776119Iteration 16, loss = 0.50459759\n",
      "Validation score: 0.746269\n",
      "Iteration 79, loss = 0.49909017\n",
      "Iteration 4, loss = 0.50309035\n",
      "Iteration 1, loss = 0.66238886Iteration 60, loss = 0.48928112\n",
      "\n",
      "Validation score: 0.805970Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 17, loss = 0.50795669\n",
      "Iteration 17, loss = 0.50448136\n",
      "Iteration 17, loss = 0.49156008\n",
      "Iteration 5, loss = 0.49571070Validation score: 0.746269\n",
      "\n",
      "Iteration 80, loss = 0.49909077Validation score: 0.805970\n",
      "\n",
      "Iteration 2, loss = 0.58801356\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 18, loss = 0.52456927\n",
      "Iteration 61, loss = 0.48943436\n",
      "Iteration 18, loss = 0.49221162\n",
      "Iteration 18, loss = 0.50437836\n",
      "Validation score: 0.701493Iteration 6, loss = 0.49563976\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 38, loss = 0.51145702\n",
      "Validation score: 0.791045\n",
      "Iteration 81, loss = 0.49909080Iteration 3, loss = 0.57353058\n",
      "\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Iteration 19, loss = 0.51219840\n",
      "Iteration 19, loss = 0.49188130\n",
      "Iteration 62, loss = 0.48946078\n",
      "Iteration 19, loss = 0.50428384\n",
      "Iteration 7, loss = 0.49500553\n",
      "Validation score: 0.701493\n",
      "Iteration 39, loss = 0.51142456\n",
      "Validation score: 0.746269Validation score: 0.791045\n",
      "Iteration 4, loss = 0.56308030\n",
      "\n",
      "Iteration 82, loss = 0.49909068\n",
      "Validation score: 0.776119\n",
      "Iteration 20, loss = 0.50507562Validation score: 0.761194\n",
      "\n",
      "Iteration 8, loss = 0.49408618\n",
      "Iteration 63, loss = 0.48945227\n",
      "Validation score: 0.791045Iteration 5, loss = 0.55385067\n",
      "Validation score: 0.701493\n",
      "Iteration 83, loss = 0.49909051Iteration 40, loss = 0.51139711\n",
      "\n",
      "Iteration 20, loss = 0.50419389\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 21, loss = 0.52863681\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.48975746\n",
      "Iteration 6, loss = 0.54579322\n",
      "Iteration 64, loss = 0.48943488\n",
      "Iteration 41, loss = 0.51136832\n",
      "Validation score: 0.701493Validation score: 0.776119\n",
      "\n",
      "Iteration 21, loss = 0.50410776\n",
      "Iteration 84, loss = 0.49909033\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910\n",
      "Iteration 42, loss = 0.51134068\n",
      "Iteration 7, loss = 0.53878267Iteration 21, loss = 0.49107042\n",
      "\n",
      "Validation score: 0.776119\n",
      "Iteration 65, loss = 0.48941587\n",
      "Validation score: 0.701493\n",
      "Iteration 22, loss = 0.50501333\n",
      "Iteration 85, loss = 0.49909015\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 43, loss = 0.51131653\n",
      "Validation score: 0.776119\n",
      "Iteration 8, loss = 0.53439576Iteration 22, loss = 0.48926465\n",
      "\n",
      "\n",
      "Iteration 86, loss = 0.49908997\n",
      "Iteration 23, loss = 0.51719499Iteration 66, loss = 0.48939723\n",
      "\n",
      "Iteration 44, loss = 0.51129583Validation score: 0.761194\n",
      "Validation score: 0.701493\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 23, loss = 0.48840073Iteration 1, loss = 0.66893717\n",
      "\n",
      "Validation score: 0.776119\n",
      "Iteration 9, loss = 0.53053186\n",
      "Iteration 9, loss = 0.49401458\n",
      "Validation score: 0.805970\n",
      "Iteration 87, loss = 0.49908979\n",
      "Iteration 67, loss = 0.48937945Validation score: 0.761194\n",
      "Iteration 45, loss = 0.51127765\n",
      "\n",
      "Iteration 24, loss = 0.50526934\n",
      "Iteration 10, loss = 0.52637900\n",
      "Validation score: 0.776119Iteration 10, loss = 0.49332072Validation score: 0.746269\n",
      "\n",
      "Iteration 2, loss = 0.62159956\n",
      "\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.805970\n",
      "Iteration 88, loss = 0.49908960\n",
      "Validation score: 0.761194\n",
      "Iteration 24, loss = 0.48716336\n",
      "Iteration 25, loss = 0.49998220\n",
      "Iteration 46, loss = 0.51126132\n",
      "Iteration 11, loss = 0.52266011Iteration 3, loss = 0.59476648\n",
      "\n",
      "Iteration 68, loss = 0.48936262\n",
      "Iteration 11, loss = 0.49316860\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.776119\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.805970\n",
      "Iteration 89, loss = 0.49908942\n",
      "Validation score: 0.761194\n",
      "Iteration 25, loss = 0.48756745\n",
      "Iteration 47, loss = 0.51038877\n",
      "Iteration 4, loss = 0.57780365\n",
      "Iteration 12, loss = 0.51932605\n",
      "Iteration 26, loss = 0.49879614\n",
      "Iteration 12, loss = 0.49278229\n",
      "Iteration 69, loss = 0.48934672\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.701493\n",
      "Iteration 90, loss = 0.49908924\n",
      "Iteration 26, loss = 0.48693364\n",
      "Iteration 48, loss = 0.51107092\n",
      "Validation score: 0.761194\n",
      "Iteration 5, loss = 0.56562770\n",
      "Validation score: 0.761194\n",
      "Iteration 13, loss = 0.51712718\n",
      "Iteration 70, loss = 0.48933170\n",
      "Validation score: 0.701493\n",
      "Iteration 49, loss = 0.51108113\n",
      "Iteration 91, loss = 0.49908906\n",
      "Iteration 27, loss = 0.48599944\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 14, loss = 0.51503175\n",
      "Iteration 71, loss = 0.48931751\n",
      "Iteration 6, loss = 0.55669647\n",
      "Iteration 50, loss = 0.51090322\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 15, loss = 0.51418677\n",
      "Iteration 92, loss = 0.49908888\n",
      "Validation score: 0.761194Iteration 7, loss = 0.54756297\n",
      "\n",
      "Iteration 51, loss = 0.51071708\n",
      "Iteration 28, loss = 0.48622690\n",
      "Iteration 72, loss = 0.48930408\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.701493\n",
      "Iteration 16, loss = 0.51242827\n",
      "Iteration 93, loss = 0.49908870\n",
      "Iteration 8, loss = 0.54016617\n",
      "Validation score: 0.761194\n",
      "Iteration 52, loss = 0.51056752\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.805970\n",
      "Iteration 29, loss = 0.48828171\n",
      "Iteration 17, loss = 0.50961492\n",
      "Iteration 73, loss = 0.48929138\n",
      "Iteration 94, loss = 0.49908852Iteration 9, loss = 0.53470007\n",
      "\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "Iteration 53, loss = 0.51045677\n",
      "Iteration 13, loss = 0.49260180\n",
      "Iteration 30, loss = 0.48539845Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 18, loss = 0.50869385\n",
      "Iteration 10, loss = 0.52831990\n",
      "Iteration 74, loss = 0.48927935\n",
      "Iteration 95, loss = 0.49908214\n",
      "Validation score: 0.701493Validation score: 0.761194\n",
      "\n",
      "Iteration 54, loss = 0.51037697Iteration 14, loss = 0.49237553\n",
      "\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.761194\n",
      "Iteration 31, loss = 0.48844489Iteration 19, loss = 0.51087785\n",
      "\n",
      "Iteration 96, loss = 0.49908422\n",
      "Iteration 11, loss = 0.52504824\n",
      "Validation score: 0.761194\n",
      "Iteration 75, loss = 0.48926795\n",
      "Iteration 15, loss = 0.49220584\n",
      "Validation score: 0.805970Iteration 55, loss = 0.51031981\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194Iteration 20, loss = 0.50740219\n",
      "\n",
      "Iteration 97, loss = 0.49908475Iteration 12, loss = 0.52175166Iteration 32, loss = 0.48920002\n",
      "\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 56, loss = 0.51027866\n",
      "Iteration 76, loss = 0.48925715\n",
      "Validation score: 0.761194\n",
      "Iteration 27, loss = 0.49908474\n",
      "Iteration 21, loss = 0.50409852\n",
      "Validation score: 0.701493\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119\n",
      "Validation score: 0.746269\n",
      "Iteration 33, loss = 0.48499254Iteration 13, loss = 0.51781816\n",
      "\n",
      "Iteration 98, loss = 0.49908487\n",
      "Iteration 57, loss = 0.51024872\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "\n",
      "Iteration 77, loss = 0.48911087Iteration 22, loss = 0.50500269\n",
      "\n",
      "Iteration 28, loss = 0.49941040\n",
      "Validation score: 0.746269\n",
      "Iteration 58, loss = 0.51022658\n",
      "Iteration 99, loss = 0.49908487\n",
      "Validation score: 0.761194\n",
      "Iteration 34, loss = 0.48491369\n",
      "Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 23, loss = 0.50405404\n",
      "Iteration 29, loss = 0.49928239\n",
      "Validation score: 0.746269Iteration 59, loss = 0.51020988\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 16, loss = 0.49204374\n",
      "Iteration 100, loss = 0.49908485\n",
      "Validation score: 0.805970Iteration 35, loss = 0.48389264\n",
      "Validation score: 0.761194\n",
      "\n",
      "Iteration 24, loss = 0.50288387\n",
      "Iteration 78, loss = 0.48921839\n",
      "Iteration 30, loss = 0.49897212Validation score: 0.701493\n",
      "Iteration 60, loss = 0.51019699\n",
      "Validation score: 0.761194\n",
      "Iteration 101, loss = 0.49908482\n",
      "Iteration 17, loss = 0.49189769\n",
      "Iteration 14, loss = 0.51302933\n",
      "Validation score: 0.761194Validation score: 0.820896\n",
      "\n",
      "Iteration 25, loss = 0.50248697\n",
      "Iteration 36, loss = 0.48494372\n",
      "Iteration 61, loss = 0.51018678\n",
      "Iteration 18, loss = 0.49176544\n",
      "Iteration 102, loss = 0.49908478\n",
      "Validation score: 0.820896Validation score: 0.761194\n",
      "Iteration 26, loss = 0.50381555\n",
      "Validation score: 0.761194\n",
      "Iteration 37, loss = 0.48393041\n",
      "\n",
      "Iteration 15, loss = 0.51289610\n",
      "Iteration 62, loss = 0.51017846\n",
      "Iteration 27, loss = 0.50356385\n",
      "Validation score: 0.761194\n",
      "Iteration 103, loss = 0.49908475\n",
      "Validation score: 0.761194\n",
      "Iteration 38, loss = 0.48320322\n",
      "Iteration 63, loss = 0.51017148\n",
      "Iteration 28, loss = 0.50151319\n",
      "Iteration 16, loss = 0.50795371\n",
      "Validation score: 0.761194\n",
      "\n",
      "Iteration 104, loss = 0.49908471\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 39, loss = 0.48270292\n",
      "Iteration 29, loss = 0.50016473Iteration 79, loss = 0.48924579\n",
      "\n",
      "Iteration 64, loss = 0.51016547Iteration 17, loss = 0.50648175\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 31, loss = 0.49872309Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "\n",
      "Iteration 105, loss = 0.49908468\n",
      "Validation score: 0.746269Validation score: 0.701493\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 40, loss = 0.48216653\n",
      "Iteration 30, loss = 0.50024956\n",
      "Iteration 65, loss = 0.50998966\n",
      "Iteration 18, loss = 0.50403901\n",
      "Validation score: 0.761194\n",
      "Iteration 32, loss = 0.49857186Iteration 80, loss = 0.48925131\n",
      "\n",
      "Iteration 106, loss = 0.49908464\n",
      "Validation score: 0.746269Validation score: 0.701493\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 41, loss = 0.48266024\n",
      "Iteration 66, loss = 0.51012152Iteration 31, loss = 0.50103273\n",
      "\n",
      "Iteration 19, loss = 0.50236995\n",
      "Validation score: 0.761194\n",
      "Iteration 19, loss = 0.49164121\n",
      "Iteration 81, loss = 0.48925091Iteration 107, loss = 0.49908460\n",
      "Iteration 33, loss = 0.49847035\n",
      "\n",
      "Validation score: 0.820896\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 32, loss = 0.50022430\n",
      "Iteration 42, loss = 0.48333968\n",
      "Iteration 67, loss = 0.51015042\n",
      "Validation score: 0.701493\n",
      "Iteration 20, loss = 0.50252996\n",
      "Validation score: 0.761194\n",
      "Iteration 20, loss = 0.49152670\n",
      "Iteration 108, loss = 0.49908457Iteration 34, loss = 0.49838128\n",
      "\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.746269Validation score: 0.761194\n",
      "Iteration 33, loss = 0.49877804\n",
      "Iteration 68, loss = 0.51014989Iteration 43, loss = 0.48345832\n",
      "\n",
      "Iteration 21, loss = 0.49924759Validation score: 0.761194\n",
      "\n",
      "Iteration 21, loss = 0.49141835\n",
      "Validation score: 0.805970\n",
      "Iteration 109, loss = 0.49908453\n",
      "Iteration 82, loss = 0.48924893\n",
      "Iteration 34, loss = 0.49771291\n",
      "Validation score: 0.761194\n",
      "Iteration 69, loss = 0.51014148\n",
      "Iteration 44, loss = 0.48381853\n",
      "Validation score: 0.701493Iteration 22, loss = 0.49959055\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 22, loss = 0.49131631\n",
      "Validation score: 0.805970\n",
      "Iteration 110, loss = 0.49908450\n",
      "Iteration 35, loss = 0.49770240\n",
      "Validation score: 0.761194\n",
      "Iteration 83, loss = 0.48924653\n",
      "Iteration 45, loss = 0.48352700Iteration 70, loss = 0.51013129\n",
      "\n",
      "Iteration 23, loss = 0.49789908\n",
      "\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 23, loss = 0.49121929Iteration 111, loss = 0.49908446Iteration 36, loss = 0.49739453\n",
      "\n",
      "Validation score: 0.761194Iteration 71, loss = 0.51012100\n",
      "\n",
      "Iteration 35, loss = 0.49829658\n",
      "Validation score: 0.761194Validation score: 0.746269\n",
      "\n",
      "Iteration 24, loss = 0.49701880\n",
      "Validation score: 0.805970\n",
      "Iteration 37, loss = 0.49939938\n",
      "\n",
      "Iteration 112, loss = 0.49908443Iteration 84, loss = 0.48924405\n",
      "Iteration 46, loss = 0.48255321\n",
      "\n",
      "Iteration 36, loss = 0.49822044Validation score: 0.701493Validation score: 0.761194\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 25, loss = 0.49646622\n",
      "Iteration 72, loss = 0.51011107\n",
      "Validation score: 0.761194\n",
      "Iteration 38, loss = 0.49785752Iteration 47, loss = 0.48134242\n",
      "\n",
      "Iteration 113, loss = 0.49908315\n",
      "Iteration 37, loss = 0.49815560Iteration 24, loss = 0.49112679Validation score: 0.761194\n",
      "\n",
      "\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.805970\n",
      "Iteration 26, loss = 0.49646096Iteration 39, loss = 0.49700879\n",
      "\n",
      "Iteration 114, loss = 0.49908357\n",
      "Iteration 73, loss = 0.51010161\n",
      "Iteration 38, loss = 0.49810103\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 85, loss = 0.48924156\n",
      "Iteration 48, loss = 0.48194072\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.701493\n",
      "Iteration 27, loss = 0.49677910\n",
      "Iteration 40, loss = 0.49664507\n",
      "Iteration 74, loss = 0.51009261\n",
      "Iteration 115, loss = 0.49908367Validation score: 0.761194\n",
      "\n",
      "Iteration 39, loss = 0.49805442Iteration 49, loss = 0.48165469\n",
      "Validation score: 0.761194\n",
      "Iteration 86, loss = 0.48923908\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 41, loss = 0.50119487\n",
      "Iteration 28, loss = 0.49767256\n",
      "Validation score: 0.746269\n",
      "Iteration 75, loss = 0.51008408\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "Validation score: 0.761194\n",
      "Iteration 116, loss = 0.49908370\n",
      "Iteration 25, loss = 0.49103830Iteration 50, loss = 0.48044756\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.805970\n",
      "Iteration 87, loss = 0.48923663\n",
      "Iteration 40, loss = 0.49728434Validation score: 0.701493\n",
      "Iteration 42, loss = 0.49880168\n",
      "\n",
      "Iteration 29, loss = 0.49632288\n",
      "Iteration 76, loss = 0.51007597\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 51, loss = 0.47997333\n",
      "Iteration 26, loss = 0.49095336Iteration 117, loss = 0.49908370\n",
      "Validation score: 0.761194\n",
      "Iteration 43, loss = 0.49561169\n",
      "Iteration 30, loss = 0.49488375\n",
      "Iteration 77, loss = 0.51006827Iteration 41, loss = 0.49979081\n",
      "\n",
      "Validation score: 0.761194Validation score: 0.746269\n",
      "\n",
      "Iteration 118, loss = 0.49908369\n",
      "Validation score: 0.761194\n",
      "Iteration 78, loss = 0.51006096Iteration 44, loss = 0.49528216\n",
      "\n",
      "Iteration 52, loss = 0.48146137\n",
      "Validation score: 0.761194\n",
      "Iteration 88, loss = 0.48923421\n",
      "Iteration 42, loss = 0.49999399\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.746269\n",
      "Iteration 119, loss = 0.49908368\n",
      "Validation score: 0.761194\n",
      "Iteration 79, loss = 0.51005401\n",
      "Iteration 45, loss = 0.49470701\n",
      "Validation score: 0.761194\n",
      "Iteration 53, loss = 0.48014199\n",
      "Iteration 43, loss = 0.49952567Iteration 89, loss = 0.48923181\n",
      "\n",
      "Iteration 120, loss = 0.49908368Validation score: 0.701493\n",
      "Validation score: 0.746269\n",
      "\n",
      "Iteration 80, loss = 0.51004740\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "\n",
      "Iteration 31, loss = 0.49528508\n",
      "Iteration 46, loss = 0.49729256\n",
      "Iteration 54, loss = 0.48036304\n",
      "Iteration 90, loss = 0.48922944Iteration 44, loss = 0.49899852\n",
      "Iteration 81, loss = 0.51004111\n",
      "\n",
      "Validation score: 0.761194Iteration 121, loss = 0.49908367\n",
      "Validation score: 0.746269\n",
      "\n",
      "\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 32, loss = 0.49469378Validation score: 0.805970\n",
      "\n",
      "Iteration 47, loss = 0.49545164\n",
      "Iteration 55, loss = 0.48313544\n",
      "Iteration 45, loss = 0.49856670\n",
      "Iteration 82, loss = 0.51003513\n",
      "Iteration 122, loss = 0.49908366\n",
      "Iteration 27, loss = 0.49087167\n",
      "Iteration 91, loss = 0.48922709\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194Validation score: 0.805970\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119\n",
      "Validation score: 0.761194\n",
      "Iteration 46, loss = 0.49824221\n",
      "Validation score: 0.701493Iteration 28, loss = 0.49079294\n",
      "\n",
      "Iteration 123, loss = 0.49908366Iteration 83, loss = 0.50999618\n",
      "\n",
      "Iteration 56, loss = 0.48027949\n",
      "Validation score: 0.805970Validation score: 0.746269\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194\n",
      "Iteration 48, loss = 0.49455210\n",
      "Iteration 92, loss = 0.48922477\n",
      "Iteration 47, loss = 0.49800406Iteration 29, loss = 0.49071693\n",
      "\n",
      "Iteration 57, loss = 0.47917004\n",
      "Iteration 124, loss = 0.49908365\n",
      "Iteration 84, loss = 0.51002093\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "\n",
      "Iteration 49, loss = 0.49550259\n",
      "Iteration 33, loss = 0.49451556\n",
      "Iteration 30, loss = 0.49064345Iteration 58, loss = 0.48088244\n",
      "Iteration 48, loss = 0.49782929\n",
      "Iteration 85, loss = 0.51002657Iteration 93, loss = 0.48922248\n",
      "\n",
      "Validation score: 0.746269Iteration 125, loss = 0.49908364\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.701493\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 50, loss = 0.49552950\n",
      "Iteration 34, loss = 0.49437421Iteration 86, loss = 0.51002700\n",
      "\n",
      "Validation score: 0.761194Iteration 49, loss = 0.49769971Iteration 94, loss = 0.48922020\n",
      "\n",
      "\n",
      "\n",
      "Iteration 59, loss = 0.48172015\n",
      "Iteration 126, loss = 0.49908364\n",
      "Validation score: 0.746269Validation score: 0.701493\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.761194\n",
      "Iteration 51, loss = 0.49311823\n",
      "Iteration 50, loss = 0.49760219\n",
      "Iteration 87, loss = 0.51002604\n",
      "Iteration 35, loss = 0.49387087\n",
      "Iteration 31, loss = 0.49057233\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 127, loss = 0.49908363\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.761194Iteration 52, loss = 0.49495897\n",
      "\n",
      "Iteration 60, loss = 0.47942527Iteration 36, loss = 0.49394884\n",
      "\n",
      "Iteration 88, loss = 0.51002470Iteration 51, loss = 0.49752749\n",
      "Iteration 32, loss = 0.49050342\n",
      "Iteration 95, loss = 0.48919072\n",
      "Validation score: 0.746269\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 128, loss = 0.49908362\n",
      "Iteration 53, loss = 0.49485723Validation score: 0.701493\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 33, loss = 0.49043661\n",
      "Validation score: 0.805970\n",
      "Iteration 96, loss = 0.48921188\n",
      "Validation score: 0.761194\n",
      "Iteration 54, loss = 0.49324480\n",
      "Iteration 52, loss = 0.49746913\n",
      "Iteration 37, loss = 0.49301470Validation score: 0.701493\n",
      "\n",
      "Iteration 129, loss = 0.49908361\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 34, loss = 0.49037176\n",
      "Validation score: 0.805970\n",
      "Iteration 97, loss = 0.48921724Iteration 55, loss = 0.49419971\n",
      "\n",
      "Iteration 53, loss = 0.49742257\n",
      "Iteration 38, loss = 0.49332008\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.746269\n",
      "Iteration 130, loss = 0.49908361\n",
      "Iteration 35, loss = 0.49030880\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "Validation score: 0.805970\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.014910\n",
      "Iteration 56, loss = 0.49429966\n",
      "Iteration 54, loss = 0.49738460\n",
      "Iteration 89, loss = 0.51002327Iteration 39, loss = 0.49287109\n",
      "\n",
      "Validation score: 0.746269Iteration 36, loss = 0.49109548\n",
      "\n",
      "Validation score: 0.761194Validation score: 0.761194\n",
      "Iteration 98, loss = 0.48921835\n",
      "Validation score: 0.701493\n",
      "Iteration 57, loss = 0.49553909\n",
      "Iteration 55, loss = 0.49735293Iteration 37, loss = 0.50323424\n",
      "\n",
      "Iteration 40, loss = 0.49242637\n",
      "Iteration 61, loss = 0.47885801\n",
      "Validation score: 0.776119\n",
      "Validation score: 0.746269\n",
      "Iteration 131, loss = 0.49908335\n",
      "Iteration 99, loss = 0.48921830Validation score: 0.761194\n",
      "\n",
      "Iteration 58, loss = 0.49418981\n",
      "Iteration 38, loss = 0.49452487Validation score: 0.701493\n",
      "\n",
      "Iteration 41, loss = 0.49297694Iteration 62, loss = 0.47908797\n",
      "\n",
      "Validation score: 0.820896\n",
      "Iteration 56, loss = 0.49732593\n",
      "Validation score: 0.746269\n",
      "Iteration 132, loss = 0.49908343\n",
      "Validation score: 0.761194\n",
      "Iteration 100, loss = 0.48921794Iteration 39, loss = 0.48826152\n",
      "\n",
      "Iteration 59, loss = 0.49561516\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.701493Iteration 42, loss = 0.49234292\n",
      "\n",
      "Iteration 63, loss = 0.47869666\n",
      "Iteration 57, loss = 0.49730243\n",
      "Validation score: 0.746269Iteration 133, loss = 0.49908346\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "Iteration 40, loss = 0.48608563Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.805970Iteration 60, loss = 0.49608681\n",
      "\n",
      "Iteration 101, loss = 0.48921751\n",
      "Iteration 43, loss = 0.49265129\n",
      "Iteration 64, loss = 0.47772042\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 58, loss = 0.49710481\n",
      "Iteration 134, loss = 0.49908346\n",
      "Validation score: 0.746269\n",
      "Iteration 41, loss = 0.48574424\n",
      "Validation score: 0.761194\n",
      "Iteration 61, loss = 0.49185373\n",
      "Validation score: 0.805970\n",
      "Iteration 90, loss = 0.51002183\n",
      "Validation score: 0.761194\n",
      "Iteration 44, loss = 0.49161881\n",
      "Iteration 59, loss = 0.49754002\n",
      "Iteration 135, loss = 0.49908346\n",
      "Validation score: 0.746269\n",
      "Iteration 42, loss = 0.48603214\n",
      "Iteration 102, loss = 0.48921705\n",
      "Validation score: 0.761194Validation score: 0.805970\n",
      "\n",
      "Iteration 91, loss = 0.51002040\n",
      "Iteration 45, loss = 0.49217102\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.701493Iteration 62, loss = 0.49372656\n",
      "\n",
      "Iteration 92, loss = 0.51001898\n",
      "Iteration 46, loss = 0.49420840Validation score: 0.761194\n",
      "\n",
      "Iteration 103, loss = 0.48921658\n",
      "Iteration 136, loss = 0.49908346\n",
      "Validation score: 0.761194Validation score: 0.701493\n",
      "Iteration 93, loss = 0.51001757\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 47, loss = 0.49515195\n",
      "Iteration 104, loss = 0.48921612\n",
      "Iteration 137, loss = 0.49908346\n",
      "Iteration 94, loss = 0.51001617\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194Iteration 48, loss = 0.49270845\n",
      "Iteration 138, loss = 0.49908346\n",
      "Iteration 105, loss = 0.48921565\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.701493\n",
      "Iteration 49, loss = 0.49040842\n",
      "Iteration 139, loss = 0.49908346\n",
      "Validation score: 0.761194\n",
      "Iteration 50, loss = 0.49039852\n",
      "\n",
      "Iteration 106, loss = 0.48921519\n",
      "Validation score: 0.701493\n",
      "Iteration 65, loss = 0.48121890\n",
      "Iteration 140, loss = 0.49908345\n",
      "Validation score: 0.761194\n",
      "Iteration 51, loss = 0.49068199\n",
      "Iteration 95, loss = 0.51001479\n",
      "Validation score: 0.761194Iteration 107, loss = 0.48921472\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 141, loss = 0.49908345\n",
      "Iteration 66, loss = 0.47745259Validation score: 0.761194\n",
      "\n",
      "Iteration 52, loss = 0.48996916Iteration 96, loss = 0.51001342\n",
      "\n",
      "Iteration 108, loss = 0.48921426\n",
      "Validation score: 0.761194Validation score: 0.701493\n",
      "\n",
      "Iteration 43, loss = 0.48627389\n",
      "Iteration 142, loss = 0.49908345\n",
      "Validation score: 0.805970\n",
      "Iteration 60, loss = 0.49764300Validation score: 0.761194\n",
      "Iteration 53, loss = 0.48985039\n",
      "Iteration 109, loss = 0.48921380\n",
      "Iteration 143, loss = 0.49908345\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 63, loss = 0.49685033\n",
      "Iteration 54, loss = 0.48977741\n",
      "Iteration 110, loss = 0.48921334\n",
      "Iteration 144, loss = 0.49908345Validation score: 0.701493\n",
      "\n",
      "Iteration 55, loss = 0.49084514Validation score: 0.746269\n",
      "\n",
      "Iteration 64, loss = 0.49716894Iteration 44, loss = 0.48629871\n",
      "Iteration 61, loss = 0.49765043Iteration 56, loss = 0.49090082Iteration 111, loss = 0.48921288\n",
      "\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 67, loss = 0.47750328\n",
      "Validation score: 0.761194\n",
      "\n",
      "\n",
      "Iteration 112, loss = 0.48921242\n",
      "Validation score: 0.701493\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "Iteration 68, loss = 0.47801063\n",
      "Iteration 97, loss = 0.51001207\n",
      "Iteration 145, loss = 0.49908345\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.761194Iteration 65, loss = 0.49289133Iteration 98, loss = 0.51001072\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "\n",
      "Iteration 57, loss = 0.48905236\n",
      "\n",
      "Iteration 113, loss = 0.48920652\n",
      "Iteration 62, loss = 0.49763192Iteration 45, loss = 0.48620572\n",
      "Iteration 99, loss = 0.51000939\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.805970Iteration 146, loss = 0.49908345Validation score: 0.761194\n",
      "Iteration 58, loss = 0.48919137Iteration 66, loss = 0.49249768\n",
      "\n",
      "Iteration 114, loss = 0.48921074\n",
      "\n",
      "Iteration 100, loss = 0.51000808\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.746269Iteration 69, loss = 0.47733974\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "Iteration 46, loss = 0.48610212\n",
      "Validation score: 0.805970\n",
      "\n",
      "Iteration 101, loss = 0.51000016Iteration 115, loss = 0.48921181\n",
      "Iteration 70, loss = 0.47861022\n",
      "Validation score: 0.701493\n",
      "\n",
      "Validation score: 0.761194Iteration 59, loss = 0.48860193\n",
      "\n",
      "Iteration 47, loss = 0.48602751\n",
      "Iteration 63, loss = 0.49760713\n",
      "Iteration 67, loss = 0.49190707\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.746269\n",
      "\n",
      "Iteration 60, loss = 0.48945151\n",
      "Iteration 48, loss = 0.48597825\n",
      "Validation score: 0.761194\n",
      "Iteration 71, loss = 0.47750315\n",
      "Validation score: 0.805970\n",
      "Iteration 68, loss = 0.49272659\n",
      "Iteration 147, loss = 0.49908344\n",
      "Iteration 64, loss = 0.49758154\n",
      "Iteration 102, loss = 0.51000506Validation score: 0.746269\n",
      "Iteration 49, loss = 0.48594087\n",
      "Iteration 61, loss = 0.48940557\n",
      "Iteration 72, loss = 0.47745067\n",
      "Validation score: 0.805970\n",
      "Iteration 116, loss = 0.48921203\n",
      "Iteration 69, loss = 0.49125116\n",
      "Validation score: 0.701493\n",
      "Iteration 50, loss = 0.48590713\n",
      "Iteration 65, loss = 0.49755662\n",
      "Iteration 62, loss = 0.48994448Validation score: 0.805970\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 70, loss = 0.49205627Validation score: 0.761194\n",
      "\n",
      "Iteration 117, loss = 0.48921202\n",
      "Validation score: 0.701493\n",
      "Iteration 51, loss = 0.48587497\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 66, loss = 0.49753275\n",
      "Iteration 63, loss = 0.48933089\n",
      "Validation score: 0.761194Validation score: 0.746269\n",
      "\n",
      "Iteration 148, loss = 0.49908344\n",
      "Iteration 71, loss = 0.49366649\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 52, loss = 0.48584501\n",
      "Iteration 67, loss = 0.49750996\n",
      "Validation score: 0.805970\n",
      "Iteration 73, loss = 0.48120311\n",
      "Iteration 103, loss = 0.51000618\n",
      "Validation score: 0.761194\n",
      "Iteration 53, loss = 0.48581791\n",
      "Validation score: 0.805970\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.002982\n",
      "Iteration 64, loss = 0.48886093\n",
      "Iteration 54, loss = 0.48518854Iteration 72, loss = 0.49368290\n",
      "\n",
      "Iteration 74, loss = 0.47742325\n",
      "Iteration 104, loss = 0.51000628\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.761194Iteration 118, loss = 0.48921195Iteration 65, loss = 0.48805372\n",
      "\n",
      "\n",
      "Iteration 55, loss = 0.48743500\n",
      "Iteration 73, loss = 0.49033129\n",
      "Iteration 75, loss = 0.47841053\n",
      "Validation score: 0.701493\n",
      "Iteration 76, loss = 0.47727818\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Iteration 77, loss = 0.47578375\n",
      "Iteration 119, loss = 0.48921186\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.746269Validation score: 0.820896\n",
      "\n",
      "Iteration 78, loss = 0.47868343\n",
      "Iteration 120, loss = 0.48921177\n",
      "Validation score: 0.701493\n",
      "Iteration 66, loss = 0.48872522Iteration 56, loss = 0.48769620\n",
      "\n",
      "Iteration 105, loss = 0.51000610\n",
      "Validation score: 0.805970\n",
      "Iteration 68, loss = 0.49748823\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269Iteration 67, loss = 0.48831674Iteration 74, loss = 0.49201394\n",
      "Iteration 1, loss = 0.69341339Iteration 121, loss = 0.48921168\n",
      "Iteration 106, loss = 0.51000586\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Iteration 79, loss = 0.47840917\n",
      "\n",
      "\n",
      "Iteration 122, loss = 0.48921159\n",
      "Iteration 107, loss = 0.51000559\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194Iteration 68, loss = 0.49057859Iteration 69, loss = 0.49746750\n",
      "Iteration 123, loss = 0.48921150\n",
      "Iteration 80, loss = 0.47813693Validation score: 0.701493\n",
      "\n",
      "Iteration 124, loss = 0.48921140\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 81, loss = 0.47746313\n",
      "Iteration 69, loss = 0.48859321\n",
      "Iteration 57, loss = 0.48736084\n",
      "Iteration 125, loss = 0.48921131\n",
      "\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 70, loss = 0.48776236Validation score: 0.746269Iteration 108, loss = 0.51000532\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 126, loss = 0.48921122\n",
      "\n",
      "Validation score: 0.701493Validation score: 0.805970\n",
      "\n",
      "Iteration 82, loss = 0.47685720Iteration 75, loss = 0.49107700\n",
      "\n",
      "Iteration 2, loss = 0.62043521\n",
      "Iteration 70, loss = 0.49744773Iteration 109, loss = 0.51000505\n",
      "Validation score: 0.761194\n",
      "Iteration 127, loss = 0.48921113\n",
      "Validation score: 0.701493\n",
      "Iteration 110, loss = 0.51000478\n",
      "Iteration 71, loss = 0.48898571Iteration 83, loss = 0.47712119Validation score: 0.761194\n",
      "\n",
      "Iteration 128, loss = 0.48921104\n",
      "Validation score: 0.701493\n",
      "\n",
      "Iteration 111, loss = 0.51000451Iteration 84, loss = 0.47658660\n",
      "\n",
      "Iteration 129, loss = 0.48921095\n",
      "\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.746269\n",
      "Iteration 130, loss = 0.48921085\n",
      "Iteration 3, loss = 0.60618987Iteration 72, loss = 0.48885269\n",
      "\n",
      "Validation score: 0.701493\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 112, loss = 0.51000424Iteration 71, loss = 0.49742885Iteration 58, loss = 0.48693089\n",
      "Iteration 131, loss = 0.48920967\n",
      "Validation score: 0.701493\n",
      "\n",
      "Iteration 73, loss = 0.48888525\n",
      "Validation score: 0.805970\n",
      "Iteration 76, loss = 0.49322539Iteration 85, loss = 0.47599921\n",
      "\n",
      "Validation score: 0.746269Iteration 59, loss = 0.48654673Iteration 132, loss = 0.48921052\n",
      "Validation score: 0.701493\n",
      "Iteration 4, loss = 0.58854177\n",
      "\n",
      "\n",
      "\n",
      "Iteration 86, loss = 0.47600871\n",
      "Validation score: 0.805970\n",
      "Iteration 133, loss = 0.48921073\n",
      "Iteration 74, loss = 0.48831851\n",
      "Validation score: 0.701493Iteration 5, loss = 0.57755038\n",
      "Iteration 72, loss = 0.49741082\n",
      "Iteration 60, loss = 0.48623383\n",
      "Iteration 87, loss = 0.47617022\n",
      "Iteration 77, loss = 0.49317923\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.761194\n",
      "Iteration 134, loss = 0.48921077\n",
      "Iteration 75, loss = 0.48814600Iteration 6, loss = 0.56670275\n",
      "\n",
      "\n",
      "Iteration 61, loss = 0.48598614\n",
      "Validation score: 0.701493\n",
      "Iteration 78, loss = 0.49028984\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.805970\n",
      "Iteration 113, loss = 0.51000397\n",
      "Validation score: 0.761194\n",
      "Iteration 62, loss = 0.48579142\n",
      "Iteration 76, loss = 0.49001020Iteration 73, loss = 0.49739360\n",
      "\n",
      "Iteration 135, loss = 0.48921077\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.701493\n",
      "Iteration 79, loss = 0.49162362\n",
      "Iteration 77, loss = 0.48922795Iteration 114, loss = 0.51000370\n",
      "\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Iteration 63, loss = 0.48563815\n",
      "Iteration 136, loss = 0.48921076\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.805970\n",
      "Iteration 80, loss = 0.49120423\n",
      "Validation score: 0.701493\n",
      "Iteration 88, loss = 0.47630421\n",
      "Iteration 74, loss = 0.49737712\n",
      "Iteration 115, loss = 0.51000343Validation score: 0.746269\n",
      "Iteration 7, loss = 0.55764921Iteration 78, loss = 0.48752079\n",
      "Iteration 64, loss = 0.48551701\n",
      "Iteration 81, loss = 0.49190993Validation score: 0.805970\n",
      "Iteration 137, loss = 0.48921074\n",
      "\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 75, loss = 0.49736136\n",
      "Iteration 65, loss = 0.48542075\n",
      "\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.761194Iteration 138, loss = 0.48921072\n",
      "Validation score: 0.701493\n",
      "\n",
      "Iteration 82, loss = 0.49091908\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119Iteration 66, loss = 0.48534380\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 8, loss = 0.55165237\n",
      "Iteration 139, loss = 0.48921070\n",
      "Validation score: 0.701493\n",
      "Iteration 140, loss = 0.48921069\n",
      "Iteration 116, loss = 0.51000316Validation score: 0.701493\n",
      "Iteration 67, loss = 0.48528193\n",
      "Validation score: 0.805970\n",
      "Iteration 9, loss = 0.54895067\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 79, loss = 0.48871421\n",
      "Iteration 68, loss = 0.48523185\n",
      "Iteration 141, loss = 0.48921067\n",
      "Iteration 83, loss = 0.49025210\n",
      "Iteration 76, loss = 0.49731147\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.746269\n",
      "Iteration 10, loss = 0.54543820\n",
      "Iteration 80, loss = 0.48836634Iteration 117, loss = 0.51000289\n",
      "Iteration 84, loss = 0.49012215Iteration 69, loss = 0.48519106\n",
      "Iteration 89, loss = 0.47637535\n",
      "Iteration 142, loss = 0.48921065\n",
      "\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890Validation score: 0.805970\n",
      "Iteration 77, loss = 0.49739192\n",
      "Validation score: 0.701493\n",
      "\n",
      "Validation score: 0.746269Iteration 11, loss = 0.54171732\n",
      "\n",
      "Iteration 81, loss = 0.48802779\n",
      "Iteration 70, loss = 0.48515759\n",
      "\n",
      "Iteration 85, loss = 0.49029985\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.761194Iteration 143, loss = 0.48921063\n",
      "\n",
      "Validation score: 0.701493\n",
      "Iteration 12, loss = 0.53568312\n",
      "Iteration 71, loss = 0.48512991Iteration 82, loss = 0.48767233\n",
      "\n",
      "Iteration 78, loss = 0.49741104\n",
      "Iteration 86, loss = 0.49051284\n",
      "Validation score: 0.805970\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000596\n",
      "Validation score: 0.746269\n",
      "Iteration 13, loss = 0.53358824\n",
      "Iteration 72, loss = 0.48495858Iteration 83, loss = 0.48762567\n",
      "\n",
      "Iteration 144, loss = 0.48921061\n",
      "Iteration 79, loss = 0.49741344Validation score: 0.805970\n",
      "\n",
      "Iteration 87, loss = 0.49006551\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.746269\n",
      "Iteration 90, loss = 0.47618319\n",
      "Iteration 84, loss = 0.48711701Iteration 73, loss = 0.48535009\n",
      "\n",
      "Iteration 14, loss = 0.53125718\n",
      "Validation score: 0.805970Iteration 118, loss = 0.51000262\n",
      "Iteration 145, loss = 0.48921059Iteration 88, loss = 0.48993156\n",
      "\n",
      "\n",
      "Iteration 80, loss = 0.49741132\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "Validation score: 0.701493\n",
      "Iteration 91, loss = 0.47622811\n",
      "Validation score: 0.746269\n",
      "Iteration 85, loss = 0.48687565\n",
      "Iteration 74, loss = 0.48544360\n",
      "Iteration 15, loss = 0.53049076\n",
      "Iteration 89, loss = 0.48948762\n",
      "Validation score: 0.805970\n",
      "Iteration 119, loss = 0.51000103\n",
      "Iteration 81, loss = 0.49740800Iteration 146, loss = 0.48921058\n",
      "\n",
      "Validation score: 0.761194Iteration 92, loss = 0.47621946\n",
      "Validation score: 0.701493\n",
      "Validation score: 0.746269\n",
      "\n",
      "Iteration 75, loss = 0.48545236\n",
      "Iteration 16, loss = 0.52623129Iteration 86, loss = 0.48672218\n",
      "\n",
      "Iteration 90, loss = 0.48937777\n",
      "Validation score: 0.805970\n",
      "Iteration 147, loss = 0.48921056\n",
      "Validation score: 0.701493\n",
      "Iteration 82, loss = 0.49740438\n",
      "Iteration 76, loss = 0.48543804\n",
      "Validation score: 0.746269Iteration 87, loss = 0.48699114\n",
      "\n",
      "Iteration 91, loss = 0.48910188\n",
      "Iteration 17, loss = 0.52456158\n",
      "Validation score: 0.805970\n",
      "Iteration 120, loss = 0.51000201Iteration 148, loss = 0.48921054\n",
      "Iteration 93, loss = 0.47620412\n",
      "Validation score: 0.701493Iteration 77, loss = 0.48541795\n",
      "Iteration 83, loss = 0.49740070\n",
      "Iteration 88, loss = 0.48681807\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Validation score: 0.805970\n",
      "Validation score: 0.746269Iteration 92, loss = 0.48939215\n",
      "Iteration 18, loss = 0.52525029\n",
      "\n",
      "Iteration 94, loss = 0.47608054\n",
      "Iteration 78, loss = 0.48539689\n",
      "Validation score: 0.805970\n",
      "Iteration 89, loss = 0.48683021\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890\n",
      "Iteration 84, loss = 0.49739703\n",
      "Validation score: 0.746269\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 93, loss = 0.48872567\n",
      "Iteration 19, loss = 0.52377873Iteration 79, loss = 0.48537618\n",
      "\n",
      "Iteration 95, loss = 0.47607314\n",
      "Validation score: 0.805970\n",
      "Iteration 90, loss = 0.48661384\n",
      "Iteration 85, loss = 0.49739338\n",
      "Iteration 121, loss = 0.51000223\n",
      "Iteration 94, loss = 0.48879214Validation score: 0.761194\n",
      "\n",
      "Iteration 20, loss = 0.52228616\n",
      "Iteration 80, loss = 0.48535614\n",
      "Iteration 96, loss = 0.47607994\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.805970\n",
      "Iteration 1, loss = 0.59190218\n",
      "Iteration 91, loss = 0.48650215\n",
      "Iteration 95, loss = 0.48894578\n",
      "Iteration 81, loss = 0.48533686\n",
      "Iteration 21, loss = 0.52092554\n",
      "Validation score: 0.805970\n",
      "Iteration 122, loss = 0.51000225Iteration 92, loss = 0.48642502\n",
      "Iteration 86, loss = 0.49738977\n",
      "Iteration 2, loss = 0.56315658\n",
      "Validation score: 0.746269\n",
      "Iteration 82, loss = 0.48531832\n",
      "Iteration 96, loss = 0.48908697Iteration 22, loss = 0.52080472\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 93, loss = 0.48645107\n",
      "Iteration 97, loss = 0.47611061\n",
      "Iteration 83, loss = 0.48530050\n",
      "Iteration 3, loss = 0.54885054\n",
      "Validation score: 0.805970\n",
      "Iteration 23, loss = 0.51859256Iteration 97, loss = 0.48890717\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890\n",
      "\n",
      "Iteration 87, loss = 0.49738619\n",
      "Validation score: 0.746269\n",
      "Iteration 94, loss = 0.48638442\n",
      "Iteration 98, loss = 0.47609576\n",
      "Iteration 84, loss = 0.48528336\n",
      "Iteration 4, loss = 0.54059531Validation score: 0.761194\n",
      "\n",
      "Iteration 24, loss = 0.51905589Iteration 98, loss = 0.48867880\n",
      "\n",
      "Iteration 88, loss = 0.49738264\n",
      "Iteration 95, loss = 0.48637481\n",
      "Iteration 99, loss = 0.47606014\n",
      "Validation score: 0.746269\n",
      "Iteration 123, loss = 0.51000222\n",
      "Validation score: 0.805970\n",
      "Iteration 5, loss = 0.53279663\n",
      "Iteration 25, loss = 0.51785303Iteration 99, loss = 0.48865859\n",
      "\n",
      "Iteration 85, loss = 0.48526689\n",
      "Iteration 89, loss = 0.49737912Validation score: 0.805970\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 100, loss = 0.48868439\n",
      "Iteration 6, loss = 0.52360837\n",
      "Iteration 26, loss = 0.51795697\n",
      "Iteration 86, loss = 0.48525104\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 90, loss = 0.49737563\n",
      "Iteration 100, loss = 0.47614185\n",
      "Validation score: 0.746269Iteration 101, loss = 0.48864606\n",
      "\n",
      "Iteration 27, loss = 0.51845090\n",
      "Iteration 124, loss = 0.51000217\n",
      "Iteration 7, loss = 0.51983021Validation score: 0.761194\n",
      "\n",
      "Iteration 101, loss = 0.47611140\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178\n",
      "Iteration 102, loss = 0.48859459\n",
      "Iteration 91, loss = 0.49737217\n",
      "Iteration 125, loss = 0.51000212\n",
      "Validation score: 0.761194Validation score: 0.746269\n",
      "\n",
      "Iteration 28, loss = 0.51896667\n",
      "Iteration 96, loss = 0.48634190Iteration 8, loss = 0.51331499\n",
      "\n",
      "Iteration 102, loss = 0.47600639\n",
      "Iteration 103, loss = 0.48865047\n",
      "Iteration 126, loss = 0.51000207\n",
      "Validation score: 0.761194\n",
      "Iteration 29, loss = 0.51657698\n",
      "Iteration 9, loss = 0.50950481\n",
      "Iteration 97, loss = 0.48637732\n",
      "Iteration 103, loss = 0.47598757\n",
      "Iteration 127, loss = 0.51000201\n",
      "Iteration 104, loss = 0.48866750\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.805970\n",
      "Iteration 30, loss = 0.51628318\n",
      "Iteration 10, loss = 0.51016740\n",
      "Iteration 98, loss = 0.48636311\n",
      "Iteration 128, loss = 0.51000196Iteration 105, loss = 0.48864615\n",
      "\n",
      "Iteration 87, loss = 0.48523580Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 31, loss = 0.51701775\n",
      "Iteration 11, loss = 0.50672096\n",
      "Iteration 99, loss = 0.48633635\n",
      "Iteration 104, loss = 0.47598092\n",
      "Iteration 106, loss = 0.48861878\n",
      "Iteration 129, loss = 0.51000191\n",
      "Iteration 88, loss = 0.48522114\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.805970\n",
      "Iteration 32, loss = 0.51641996\n",
      "Iteration 12, loss = 0.50073267Iteration 100, loss = 0.48634929\n",
      "\n",
      "Iteration 107, loss = 0.48861002\n",
      "Iteration 130, loss = 0.51000185\n",
      "Validation score: 0.761194\n",
      "Iteration 101, loss = 0.48636868\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178Iteration 33, loss = 0.51792915\n",
      "\n",
      "Iteration 13, loss = 0.50048149\n",
      "Iteration 108, loss = 0.48861903\n",
      "Iteration 131, loss = 0.51000180\n",
      "Iteration 89, loss = 0.48520703\n",
      "Validation score: 0.761194\n",
      "Iteration 92, loss = 0.49736874\n",
      "Validation score: 0.746269\n",
      "Iteration 14, loss = 0.49627328Iteration 34, loss = 0.51543554\n",
      "\n",
      "Iteration 102, loss = 0.48626905\n",
      "Iteration 109, loss = 0.48861860\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178\n",
      "Validation score: 0.805970\n",
      "Iteration 132, loss = 0.51000174\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000119\n",
      "Validation score: 0.761194\n",
      "Iteration 103, loss = 0.48626621\n",
      "Iteration 35, loss = 0.51544068\n",
      "Iteration 15, loss = 0.49614006\n",
      "Iteration 110, loss = 0.48856877\n",
      "Iteration 133, loss = 0.51000169\n",
      "Validation score: 0.761194\n",
      "Iteration 104, loss = 0.48626427Iteration 16, loss = 0.49459814\n",
      "\n",
      "Iteration 36, loss = 0.51491207\n",
      "Iteration 134, loss = 0.51000164\n",
      "Iteration 111, loss = 0.48856798\n",
      "Iteration 105, loss = 0.47596996Validation score: 0.761194\n",
      "\n",
      "Iteration 90, loss = 0.48516448\n",
      "Iteration 105, loss = 0.48627045\n",
      "Iteration 17, loss = 0.49399790\n",
      "Validation score: 0.805970\n",
      "Iteration 135, loss = 0.51000158\n",
      "Iteration 37, loss = 0.51546216\n",
      "Iteration 112, loss = 0.48856805\n",
      "Validation score: 0.761194\n",
      "Iteration 106, loss = 0.47597792\n",
      "Iteration 18, loss = 0.49289633\n",
      "Iteration 136, loss = 0.51000153\n",
      "Iteration 38, loss = 0.51483159Iteration 113, loss = 0.48856635\n",
      "\n",
      "Iteration 93, loss = 0.49736534\n",
      "Validation score: 0.761194\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 106, loss = 0.48626630\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "Iteration 91, loss = 0.48523719Iteration 19, loss = 0.49362629\n",
      "Iteration 107, loss = 0.47596273\n",
      "Iteration 137, loss = 0.51000121\n",
      "Iteration 114, loss = 0.48858092Iteration 39, loss = 0.51514958\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 94, loss = 0.49735505\n",
      "Iteration 107, loss = 0.48626354\n",
      "Validation score: 0.746269\n",
      "Iteration 138, loss = 0.51000141\n",
      "Iteration 20, loss = 0.49060271\n",
      "Validation score: 0.805970\n",
      "Iteration 115, loss = 0.48856100\n",
      "Iteration 40, loss = 0.51590110\n",
      "Iteration 108, loss = 0.48626034\n",
      "Iteration 95, loss = 0.49737092\n",
      "Validation score: 0.746269\n",
      "Iteration 108, loss = 0.47596722\n",
      "Iteration 92, loss = 0.48525451\n",
      "Validation score: 0.805970\n",
      "Iteration 116, loss = 0.48855984\n",
      "Iteration 21, loss = 0.48864542Validation score: 0.761194\n",
      "\n",
      "Iteration 41, loss = 0.51607055\n",
      "Iteration 109, loss = 0.48627212\n",
      "Iteration 96, loss = 0.49737469Iteration 109, loss = 0.47594388\n",
      "Iteration 139, loss = 0.51000145\n",
      "Iteration 22, loss = 0.48888726Iteration 117, loss = 0.48856441\n",
      "\n",
      "\n",
      "Iteration 42, loss = 0.51762419\n",
      "Validation score: 0.746269\n",
      "Iteration 110, loss = 0.48626668Iteration 93, loss = 0.48525676\n",
      "\n",
      "Validation score: 0.805970Iteration 110, loss = 0.47594144\n",
      "\n",
      "Iteration 118, loss = 0.48855404\n",
      "Iteration 23, loss = 0.48843083\n",
      "Validation score: 0.761194\n",
      "Iteration 97, loss = 0.49737519Iteration 43, loss = 0.51535609\n",
      "Iteration 140, loss = 0.51000145\n",
      "Iteration 111, loss = 0.48626217\n",
      "Iteration 119, loss = 0.48855716\n",
      "Iteration 24, loss = 0.48901430\n",
      "\n",
      "Iteration 44, loss = 0.51384685\n",
      "Iteration 111, loss = 0.47593423\n",
      "Validation score: 0.746269Iteration 94, loss = 0.48525493\n",
      "Validation score: 0.805970\n",
      "\n",
      "Validation score: 0.761194\n",
      "Iteration 120, loss = 0.48855429\n",
      "Iteration 25, loss = 0.49008023\n",
      "Iteration 45, loss = 0.51421950\n",
      "Iteration 95, loss = 0.48525202\n",
      "Iteration 98, loss = 0.49737481Validation score: 0.805970\n",
      "\n",
      "Iteration 121, loss = 0.48855535\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036\n",
      "Validation score: 0.746269\n",
      "Iteration 141, loss = 0.51000145Iteration 26, loss = 0.48800853\n",
      "\n",
      "Iteration 46, loss = 0.51388821\n",
      "Validation score: 0.761194\n",
      "Iteration 112, loss = 0.47593597\n",
      "\n",
      "Iteration 122, loss = 0.48855283\n",
      "Iteration 99, loss = 0.49737420\n",
      "Iteration 27, loss = 0.48627146\n",
      "Iteration 47, loss = 0.51782106\n",
      "Validation score: 0.746269\n",
      "Iteration 112, loss = 0.48626343\n",
      "Iteration 96, loss = 0.48524884Iteration 113, loss = 0.47594479\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036\n",
      "Iteration 142, loss = 0.51000144\n",
      "Validation score: 0.761194\n",
      "Iteration 123, loss = 0.48854859\n",
      "Iteration 100, loss = 0.49737352\n",
      "Iteration 28, loss = 0.48601067\n",
      "Iteration 48, loss = 0.51353134Iteration 113, loss = 0.48625935\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 143, loss = 0.51000143\n",
      "\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.805970\n",
      "Iteration 124, loss = 0.48854853\n",
      "Iteration 101, loss = 0.49737282\n",
      "Iteration 49, loss = 0.51488557\n",
      "Iteration 144, loss = 0.51000142Iteration 29, loss = 0.48533856\n",
      "\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.761194\n",
      "Iteration 114, loss = 0.48625469\n",
      "Iteration 125, loss = 0.48854857\n",
      "Iteration 114, loss = 0.47592869\n",
      "Iteration 145, loss = 0.51000141Iteration 50, loss = 0.51554448\n",
      "\n",
      "Iteration 102, loss = 0.49737213\n",
      "Iteration 30, loss = 0.48612310Validation score: 0.761194\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 115, loss = 0.48625380\n",
      "Iteration 126, loss = 0.48854900\n",
      "Iteration 115, loss = 0.47592721\n",
      "Iteration 146, loss = 0.51000140\n",
      "Iteration 51, loss = 0.51530759\n",
      "Validation score: 0.761194\n",
      "Iteration 103, loss = 0.49737143\n",
      "Iteration 116, loss = 0.48625412\n",
      "Validation score: 0.746269\n",
      "Iteration 31, loss = 0.48727336\n",
      "Iteration 127, loss = 0.48854726\n",
      "Iteration 116, loss = 0.47592807\n",
      "Iteration 147, loss = 0.51000139\n",
      "Validation score: 0.761194\n",
      "Iteration 52, loss = 0.51490994\n",
      "Iteration 104, loss = 0.49737073\n",
      "Iteration 117, loss = 0.48625486\n",
      "Validation score: 0.746269Iteration 32, loss = 0.48545701\n",
      "\n",
      "Iteration 128, loss = 0.48854612Iteration 117, loss = 0.47592382\n",
      "\n",
      "Iteration 148, loss = 0.51000137\n",
      "Iteration 97, loss = 0.48524559\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.805970\n",
      "Iteration 53, loss = 0.51342458\n",
      "Iteration 105, loss = 0.49737003\n",
      "Validation score: 0.746269\n",
      "Iteration 129, loss = 0.48854666\n",
      "Iteration 33, loss = 0.48454356\n",
      "Iteration 149, loss = 0.51000136\n",
      "Iteration 98, loss = 0.48524236\n",
      "Validation score: 0.761194\n",
      "Iteration 54, loss = 0.51297697Validation score: 0.805970\n",
      "\n",
      "Iteration 106, loss = 0.49736934\n",
      "Validation score: 0.746269\n",
      "Iteration 130, loss = 0.48854912\n",
      "Iteration 118, loss = 0.47592673Iteration 34, loss = 0.48544041\n",
      "\n",
      "Iteration 150, loss = 0.51000135\n",
      "Iteration 99, loss = 0.48523914\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.805970Iteration 55, loss = 0.51376970\n",
      "\n",
      "Iteration 107, loss = 0.49736865\n",
      "Iteration 131, loss = 0.48854676Validation score: 0.746269\n",
      "\n",
      "Iteration 35, loss = 0.48411287\n",
      "Iteration 119, loss = 0.47592281\n",
      "Iteration 151, loss = 0.51000134\n",
      "Iteration 118, loss = 0.48625562\n",
      "Validation score: 0.761194\n",
      "Iteration 56, loss = 0.51300485\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Iteration 108, loss = 0.49736795\n",
      "Iteration 132, loss = 0.48854913Validation score: 0.746269\n",
      "\n",
      "Iteration 120, loss = 0.47592273Iteration 36, loss = 0.48449907\n",
      "\n",
      "Iteration 152, loss = 0.51000133\n",
      "Validation score: 0.761194\n",
      "Iteration 119, loss = 0.48625633\n",
      "Iteration 57, loss = 0.51255340\n",
      "Iteration 109, loss = 0.49736726\n",
      "Iteration 153, loss = 0.51000132Iteration 133, loss = 0.48854880\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 100, loss = 0.48523594\n",
      "Validation score: 0.761194\n",
      "Validation score: 0.805970\n",
      "Iteration 58, loss = 0.51249597Iteration 37, loss = 0.48532580\n",
      "\n",
      "Iteration 120, loss = 0.48625575Iteration 110, loss = 0.49736657\n",
      "\n",
      "Iteration 134, loss = 0.48854567\n",
      "Validation score: 0.746269Iteration 154, loss = 0.51000131\n",
      "\n",
      "Validation score: 0.761194Iteration 121, loss = 0.47592155\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 101, loss = 0.48523277\n",
      "Validation score: 0.805970\n",
      "Iteration 59, loss = 0.51220875\n",
      "Iteration 121, loss = 0.48625705\n",
      "Iteration 111, loss = 0.49736589\n",
      "Iteration 38, loss = 0.48362912\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "Iteration 122, loss = 0.47592088\n",
      "Iteration 102, loss = 0.48522962\n",
      "Validation score: 0.805970\n",
      "Iteration 122, loss = 0.48625480\n",
      "Iteration 60, loss = 0.51209020\n",
      "Iteration 112, loss = 0.49736381\n",
      "Validation score: 0.746269\n",
      "Iteration 39, loss = 0.48335822\n",
      "Iteration 1, loss = 0.63757813\n",
      "Iteration 123, loss = 0.48625487\n",
      "Iteration 2, loss = 0.52253794\n",
      "Iteration 61, loss = 0.51181351\n",
      "Iteration 113, loss = 0.49736698\n",
      "Iteration 40, loss = 0.48411604\n",
      "Iteration 135, loss = 0.48854615\n",
      "Validation score: 0.746269\n",
      "Iteration 3, loss = 0.49831835\n",
      "Iteration 124, loss = 0.48625572Iteration 4, loss = 0.49421618\n",
      "Iteration 114, loss = 0.49736773\n",
      "Iteration 62, loss = 0.51178093\n",
      "Iteration 136, loss = 0.48854613\n",
      "Validation score: 0.746269\n",
      "\n",
      "Iteration 5, loss = 0.47808324\n",
      "Iteration 41, loss = 0.48258291\n",
      "Iteration 6, loss = 0.47536540\n",
      "Iteration 103, loss = 0.48522649Iteration 115, loss = 0.49736783\n",
      "Iteration 125, loss = 0.48625450\n",
      "Iteration 63, loss = 0.51186287\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "Iteration 137, loss = 0.48854575\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 7, loss = 0.46983519\n",
      "Validation score: 0.805970\n",
      "Iteration 42, loss = 0.48459393\n",
      "Iteration 8, loss = 0.46166087\n",
      "Iteration 126, loss = 0.48625348\n",
      "Iteration 116, loss = 0.49736776\n",
      "Iteration 64, loss = 0.51183954\n",
      "Iteration 138, loss = 0.48854603\n",
      "Iteration 123, loss = 0.47592182\n",
      "Validation score: 0.746269\n",
      "Iteration 9, loss = 0.46090522\n",
      "Iteration 43, loss = 0.48322549\n",
      "Iteration 10, loss = 0.45926443Iteration 127, loss = 0.48625313\n",
      "\n",
      "Iteration 139, loss = 0.48854651Iteration 124, loss = 0.47592126\n",
      "\n",
      "Iteration 117, loss = 0.49736764\n",
      "Iteration 65, loss = 0.51176836\n",
      "Validation score: 0.746269\n",
      "Iteration 11, loss = 0.45254421\n",
      "Iteration 12, loss = 0.45624587Iteration 44, loss = 0.48311610\n",
      "Iteration 128, loss = 0.48625330\n",
      "\n",
      "Iteration 125, loss = 0.47591993\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "Iteration 140, loss = 0.48854569\n",
      "Iteration 118, loss = 0.49736750\n",
      "Iteration 66, loss = 0.51236822\n",
      "Validation score: 0.746269\n",
      "Iteration 13, loss = 0.45029779\n",
      "Iteration 129, loss = 0.48625313\n",
      "Iteration 14, loss = 0.44829643\n",
      "Iteration 45, loss = 0.48177344\n",
      "Iteration 104, loss = 0.48522339\n",
      "Iteration 141, loss = 0.48854578\n",
      "Validation score: 0.805970Iteration 119, loss = 0.49736737\n",
      "Iteration 67, loss = 0.51164305\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 15, loss = 0.44630541\n",
      "Iteration 142, loss = 0.48854597Iteration 130, loss = 0.48625313\n",
      "Iteration 105, loss = 0.48522031\n",
      "\n",
      "Iteration 46, loss = 0.48318672\n",
      "Iteration 16, loss = 0.44386471\n",
      "Iteration 68, loss = 0.51171246\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890\n",
      "Validation score: 0.805970\n",
      "Iteration 120, loss = 0.49736723\n",
      "Validation score: 0.746269\n",
      "Iteration 126, loss = 0.47591977\n",
      "Iteration 17, loss = 0.44332914\n",
      "Iteration 131, loss = 0.48625338Iteration 143, loss = 0.48854589\n",
      "\n",
      "Iteration 106, loss = 0.48521726Iteration 47, loss = 0.48258781\n",
      "Iteration 18, loss = 0.43988148\n",
      "Iteration 69, loss = 0.51143181Iteration 121, loss = 0.49736709\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 127, loss = 0.47592264\n",
      "Iteration 19, loss = 0.44628541\n",
      "Iteration 132, loss = 0.48625278Iteration 144, loss = 0.48854560\n",
      "\n",
      "Iteration 48, loss = 0.48203287\n",
      "Iteration 20, loss = 0.44262865\n",
      "Iteration 70, loss = 0.51154564\n",
      "Iteration 122, loss = 0.49736695\n",
      "Validation score: 0.746269Iteration 128, loss = 0.47592241\n",
      "\n",
      "Iteration 21, loss = 0.43301486\n",
      "Iteration 145, loss = 0.48854552\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 133, loss = 0.48625297\n",
      "Iteration 22, loss = 0.42753420\n",
      "Iteration 49, loss = 0.48410856\n",
      "Iteration 71, loss = 0.51163297Iteration 123, loss = 0.49736681\n",
      "\n",
      "Validation score: 0.746269Iteration 23, loss = 0.43054508\n",
      "\n",
      "Iteration 146, loss = 0.48854567\n",
      "Iteration 24, loss = 0.41633223\n",
      "Iteration 134, loss = 0.48625334\n",
      "Iteration 50, loss = 0.48158362\n",
      "Iteration 72, loss = 0.51152320Iteration 124, loss = 0.49736667\n",
      "\n",
      "Iteration 25, loss = 0.41331394\n",
      "Validation score: 0.746269\n",
      "Iteration 129, loss = 0.47592169\n",
      "Iteration 26, loss = 0.42142978Iteration 147, loss = 0.48854562\n",
      "\n",
      "Validation score: 0.805970Iteration 135, loss = 0.48625311\n",
      "\n",
      "Iteration 51, loss = 0.48160833\n",
      "Iteration 73, loss = 0.51157448\n",
      "Iteration 125, loss = 0.49736654\n",
      "Validation score: 0.746269\n",
      "Iteration 107, loss = 0.48521423\n",
      "Iteration 130, loss = 0.47592172\n",
      "Iteration 148, loss = 0.48854546\n",
      "Validation score: 0.805970\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000024\n",
      "Iteration 136, loss = 0.48625316\n",
      "Iteration 74, loss = 0.51149443Iteration 52, loss = 0.48122921\n",
      "\n",
      "Iteration 126, loss = 0.49736640\n",
      "Iteration 108, loss = 0.48520546\n",
      "Iteration 131, loss = 0.47592373\n",
      "Validation score: 0.805970Validation score: 0.746269\n",
      "\n",
      "Iteration 149, loss = 0.48854552\n",
      "Iteration 137, loss = 0.48625311\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 75, loss = 0.51146554Iteration 53, loss = 0.48197491\n",
      "\n",
      "Iteration 109, loss = 0.48521981\n",
      "Iteration 127, loss = 0.49736626Validation score: 0.805970\n",
      "\n",
      "Iteration 132, loss = 0.47592313\n",
      "Iteration 150, loss = 0.48854545Validation score: 0.746269\n",
      "\n",
      "Iteration 138, loss = 0.48625282\n",
      "Iteration 110, loss = 0.48522323\n",
      "Iteration 76, loss = 0.51152536\n",
      "Iteration 54, loss = 0.48106804Iteration 128, loss = 0.49736612\n",
      "\n",
      "Iteration 151, loss = 0.48854539Validation score: 0.746269\n",
      "\n",
      "Iteration 27, loss = 0.40172342\n",
      "Iteration 139, loss = 0.48625276Iteration 28, loss = 0.40142859\n",
      "\n",
      "Iteration 77, loss = 0.51151220\n",
      "\n",
      "Iteration 129, loss = 0.49736598Validation score: 0.805970\n",
      "\n",
      "Iteration 29, loss = 0.40622586\n",
      "Iteration 55, loss = 0.48231577\n",
      "Iteration 152, loss = 0.48854543Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "\n",
      "Iteration 133, loss = 0.47592258\n",
      "Iteration 30, loss = 0.39061745\n",
      "Iteration 140, loss = 0.48625273\n",
      "Iteration 111, loss = 0.48522369\n",
      "Validation score: 0.805970\n",
      "Iteration 130, loss = 0.49736557\n",
      "Iteration 31, loss = 0.39166993\n",
      "Iteration 56, loss = 0.48178111\n",
      "Iteration 153, loss = 0.48854536Validation score: 0.746269\n",
      "\n",
      "Iteration 134, loss = 0.47592257\n",
      "Iteration 32, loss = 0.39681550\n",
      "Iteration 141, loss = 0.48625275\n",
      "Iteration 131, loss = 0.49736620\n",
      "Iteration 33, loss = 0.39201799\n",
      "Iteration 135, loss = 0.47592219\n",
      "Iteration 154, loss = 0.48854536\n",
      "Iteration 78, loss = 0.51147922Validation score: 0.746269\n",
      "\n",
      "Iteration 57, loss = 0.48059794\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Iteration 34, loss = 0.38465922\n",
      "Iteration 142, loss = 0.48625274\n",
      "Iteration 35, loss = 0.38823707\n",
      "Iteration 132, loss = 0.49736635\n",
      "Iteration 136, loss = 0.47592162Iteration 155, loss = 0.48854539\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 36, loss = 0.39971025\n",
      "Iteration 79, loss = 0.51145417\n",
      "Iteration 37, loss = 0.38130175\n",
      "Iteration 133, loss = 0.49736637\n",
      "Iteration 156, loss = 0.48854534\n",
      "Validation score: 0.746269\n",
      "Iteration 38, loss = 0.38495246Iteration 112, loss = 0.48522336\n",
      "\n",
      "Iteration 137, loss = 0.47592136\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001Validation score: 0.805970\n",
      "\n",
      "Iteration 58, loss = 0.48182905\n",
      "Iteration 39, loss = 0.38137012\n",
      "Iteration 157, loss = 0.48854536\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 40, loss = 0.37928074\n",
      "Iteration 80, loss = 0.51153601Iteration 59, loss = 0.48169264Iteration 138, loss = 0.47592121\n",
      "\n",
      "Iteration 41, loss = 0.37593627\n",
      "Iteration 158, loss = 0.48854534\n",
      "Iteration 113, loss = 0.48522281\n",
      "Validation score: 0.805970\n",
      "Iteration 42, loss = 0.37161949\n",
      "Iteration 60, loss = 0.48141689\n",
      "Iteration 139, loss = 0.47592057\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178\n",
      "\n",
      "Iteration 43, loss = 0.37616096\n",
      "Iteration 143, loss = 0.48625279\n",
      "Iteration 159, loss = 0.48854532\n",
      "Iteration 44, loss = 0.37580011\n",
      "Iteration 140, loss = 0.47592048\n",
      "Iteration 81, loss = 0.51140850Iteration 45, loss = 0.36425168\n",
      "Iteration 61, loss = 0.48133658\n",
      "Iteration 144, loss = 0.48625281\n",
      "Iteration 160, loss = 0.48854530\n",
      "\n",
      "Iteration 46, loss = 0.39395291\n",
      "Iteration 47, loss = 0.36824098\n",
      "Iteration 141, loss = 0.47592047\n",
      "Iteration 161, loss = 0.48854531\n",
      "Iteration 114, loss = 0.48522221\n",
      "Iteration 48, loss = 0.35607571Iteration 145, loss = 0.48625280\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 49, loss = 0.35390798\n",
      "Iteration 62, loss = 0.47997830\n",
      "Iteration 142, loss = 0.47592038\n",
      "Iteration 162, loss = 0.48854531\n",
      "Iteration 115, loss = 0.48522159Iteration 146, loss = 0.48625276\n",
      "Iteration 50, loss = 0.36887058\n",
      "Iteration 82, loss = 0.51144178\n",
      "Iteration 63, loss = 0.47983853Iteration 51, loss = 0.35391271\n",
      "\n",
      "Iteration 163, loss = 0.48854530\n",
      "\n",
      "Iteration 52, loss = 0.36510527\n",
      "Validation score: 0.805970\n",
      "Iteration 147, loss = 0.48625275Iteration 64, loss = 0.47979536\n",
      "\n",
      "Iteration 53, loss = 0.35768623\n",
      "Iteration 164, loss = 0.48854531\n",
      "Iteration 54, loss = 0.34605071\n",
      "Iteration 116, loss = 0.48522097Iteration 83, loss = 0.51142429\n",
      "\n",
      "Iteration 55, loss = 0.35241655Validation score: 0.805970\n",
      "\n",
      "Iteration 134, loss = 0.49736636\n",
      "Iteration 148, loss = 0.48625272\n",
      "Iteration 65, loss = 0.47971570\n",
      "Validation score: 0.746269\n",
      "Iteration 143, loss = 0.47592044\n",
      "Iteration 56, loss = 0.36038483\n",
      "Iteration 84, loss = 0.51146785Iteration 57, loss = 0.34737970Iteration 117, loss = 0.48522035\n",
      "\n",
      "Iteration 149, loss = 0.48625272\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Validation score: 0.805970\n",
      "Iteration 135, loss = 0.49736633\n",
      "Iteration 66, loss = 0.47969003\n",
      "Iteration 58, loss = 0.33551921Validation score: 0.746269\n",
      "\n",
      "Iteration 165, loss = 0.48854532\n",
      "Iteration 59, loss = 0.33876255\n",
      "Iteration 118, loss = 0.48521973\n",
      "Iteration 150, loss = 0.48625269\n",
      "Iteration 136, loss = 0.49736631Validation score: 0.805970\n",
      "\n",
      "\n",
      "Iteration 60, loss = 0.35744321\n",
      "Validation score: 0.746269Iteration 67, loss = 0.47954411\n",
      "\n",
      "Iteration 144, loss = 0.47592028\n",
      "Iteration 61, loss = 0.33626577\n",
      "Iteration 119, loss = 0.48521911Iteration 151, loss = 0.48625270\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 85, loss = 0.51142159\n",
      "Iteration 137, loss = 0.49736628\n",
      "Iteration 62, loss = 0.34638252\n",
      "Iteration 68, loss = 0.48129306\n",
      "Validation score: 0.746269\n",
      "Iteration 63, loss = 0.33974360\n",
      "Iteration 120, loss = 0.48521849\n",
      "Iteration 86, loss = 0.51141370\n",
      "Iteration 152, loss = 0.48625269\n",
      "Iteration 145, loss = 0.47592022\n",
      "Validation score: 0.805970\n",
      "Iteration 138, loss = 0.49736625Iteration 64, loss = 0.34447879\n",
      "\n",
      "Iteration 69, loss = 0.47997980\n",
      "Validation score: 0.746269\n",
      "Iteration 65, loss = 0.33948152\n",
      "Iteration 87, loss = 0.51141941Iteration 121, loss = 0.48521787\n",
      "Iteration 66, loss = 0.32727954Iteration 153, loss = 0.48625271\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 139, loss = 0.49736622\n",
      "Iteration 166, loss = 0.48854530\n",
      "Validation score: 0.746269\n",
      "Iteration 67, loss = 0.32773097\n",
      "Iteration 70, loss = 0.47933478\n",
      "Iteration 146, loss = 0.47592011\n",
      "Iteration 122, loss = 0.48521725Iteration 154, loss = 0.48625271\n",
      "\n",
      "Iteration 68, loss = 0.33320292\n",
      "Validation score: 0.805970\n",
      "Iteration 167, loss = 0.48854530\n",
      "Iteration 69, loss = 0.32237369\n",
      "Iteration 71, loss = 0.47932653\n",
      "Iteration 140, loss = 0.49736620\n",
      "Validation score: 0.746269Iteration 147, loss = 0.47592011\n",
      "Iteration 155, loss = 0.48625270\n",
      "\n",
      "Iteration 70, loss = 0.33181238\n",
      "Iteration 123, loss = 0.48521664\n",
      "Iteration 168, loss = 0.48854530\n",
      "Validation score: 0.805970\n",
      "Iteration 71, loss = 0.32096050\n",
      "Iteration 141, loss = 0.49736617\n",
      "Validation score: 0.746269\n",
      "Iteration 72, loss = 0.33247393\n",
      "Iteration 169, loss = 0.48854530\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 156, loss = 0.48625270\n",
      "Iteration 148, loss = 0.47592008Iteration 124, loss = 0.48521602\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 73, loss = 0.32034242\n",
      "Iteration 142, loss = 0.49736614\n",
      "\n",
      "Validation score: 0.746269\n",
      "Iteration 74, loss = 0.32042430\n",
      "Iteration 157, loss = 0.48625270\n",
      "Iteration 125, loss = 0.48521541\n",
      "Validation score: 0.805970Iteration 75, loss = 0.32561557\n",
      "\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000005\n",
      "Iteration 149, loss = 0.47592005\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 88, loss = 0.51143588\n",
      "Iteration 143, loss = 0.49736611\n",
      "Iteration 76, loss = 0.31990050\n",
      "Validation score: 0.746269\n",
      "Iteration 158, loss = 0.48625268\n",
      "Iteration 72, loss = 0.47985237\n",
      "Iteration 77, loss = 0.31982200\n",
      "Iteration 126, loss = 0.48521365\n",
      "Iteration 89, loss = 0.51141348Iteration 78, loss = 0.31811450\n",
      "Iteration 150, loss = 0.47591997\n",
      "Iteration 79, loss = 0.30856752\n",
      "Iteration 151, loss = 0.47591996Iteration 80, loss = 0.33903860\n",
      "\n",
      "Iteration 1, loss = 0.65761033\n",
      "Iteration 81, loss = 0.30532889\n",
      "Iteration 152, loss = 0.47591991\n",
      "Iteration 82, loss = 0.30185265\n",
      "Iteration 2, loss = 0.59649229\n",
      "Iteration 153, loss = 0.47591992\n",
      "Iteration 83, loss = 0.30909727\n",
      "Iteration 3, loss = 0.57587952\n",
      "Iteration 84, loss = 0.31338783\n",
      "Iteration 85, loss = 0.29188785\n",
      "Iteration 90, loss = 0.51143804\n",
      "Iteration 86, loss = 0.32204128\n",
      "Iteration 154, loss = 0.47591988\n",
      "Iteration 87, loss = 0.30185766\n",
      "Iteration 91, loss = 0.51141249\n",
      "Iteration 88, loss = 0.32274835\n",
      "Iteration 155, loss = 0.47591987\n",
      "Iteration 92, loss = 0.51142437\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036\n",
      "Iteration 89, loss = 0.29331441\n",
      "Iteration 73, loss = 0.47979707\n",
      "Iteration 156, loss = 0.47591984\n",
      "Iteration 90, loss = 0.32442752\n",
      "Iteration 93, loss = 0.51141006\n",
      "Iteration 91, loss = 0.29627612\n",
      "Iteration 74, loss = 0.47958917\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890\n",
      "Iteration 157, loss = 0.47591985\n",
      "Iteration 92, loss = 0.30122269\n",
      "Iteration 94, loss = 0.51140734\n",
      "Iteration 93, loss = 0.29133910\n",
      "Iteration 75, loss = 0.47944458\n",
      "Iteration 95, loss = 0.51140594\n",
      "Iteration 94, loss = 0.29868399\n",
      "Validation score: 0.805970\n",
      "Iteration 144, loss = 0.49736609\n",
      "Iteration 95, loss = 0.28531012Validation score: 0.746269\n",
      "\n",
      "Iteration 96, loss = 0.51140563\n",
      "Iteration 96, loss = 0.31507141\n",
      "\n",
      "Iteration 145, loss = 0.49736606\n",
      "Iteration 127, loss = 0.48521651\n",
      "Validation score: 0.746269\n",
      "Validation score: 0.805970\n",
      "Iteration 76, loss = 0.47924803\n",
      "Iteration 158, loss = 0.47591984Iteration 97, loss = 0.29483369\n",
      "Iteration 4, loss = 0.56651326\n",
      "\n",
      "Iteration 98, loss = 0.29685720\n",
      "Iteration 99, loss = 0.31334533\n",
      "Iteration 5, loss = 0.55630017\n",
      "Iteration 159, loss = 0.47591984Iteration 100, loss = 0.29933948\n",
      "Iteration 101, loss = 0.28438039\n",
      "Iteration 6, loss = 0.54934148\n",
      "Iteration 159, loss = 0.48625271\n",
      "Iteration 128, loss = 0.48521719\n",
      "Validation score: 0.805970\n",
      "Iteration 102, loss = 0.31945708\n",
      "\n",
      "Iteration 7, loss = 0.54373480Iteration 103, loss = 0.27833173\n",
      "Iteration 97, loss = 0.51140731\n",
      "Iteration 104, loss = 0.29685944\n",
      "Iteration 105, loss = 0.28465941\n",
      "Iteration 98, loss = 0.51141350\n",
      "Iteration 77, loss = 0.47927972\n",
      "Iteration 106, loss = 0.28876421\n",
      "Iteration 99, loss = 0.51140682\n",
      "Iteration 107, loss = 0.28041266Iteration 78, loss = 0.47920895\n",
      "\n",
      "Iteration 146, loss = 0.49736603\n",
      "Iteration 108, loss = 0.32307601\n",
      "Iteration 100, loss = 0.51140557\n",
      "Validation score: 0.746269\n",
      "Iteration 109, loss = 0.27749420\n",
      "Iteration 160, loss = 0.47591983\n",
      "Iteration 101, loss = 0.51140863\n",
      "Iteration 147, loss = 0.49736600Iteration 160, loss = 0.48625270\n",
      "Iteration 110, loss = 0.28634324\n",
      "Iteration 111, loss = 0.27799429\n",
      "Iteration 161, loss = 0.47591985\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 8, loss = 0.53722739\n",
      "Iteration 161, loss = 0.48625271\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 112, loss = 0.28603663\n",
      "Iteration 129, loss = 0.48521728\n",
      "Iteration 79, loss = 0.47933347\n",
      "Iteration 113, loss = 0.27521505\n",
      "Validation score: 0.805970\n",
      "Iteration 9, loss = 0.53466668\n",
      "Iteration 80, loss = 0.47925353\n",
      "Iteration 114, loss = 0.28968444\n",
      "Iteration 115, loss = 0.27668192\n",
      "Iteration 10, loss = 0.53102423\n",
      "Iteration 81, loss = 0.47924538\n",
      "Iteration 116, loss = 0.29689280\n",
      "\n",
      "Validation score: 0.746269\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 11, loss = 0.52787561\n",
      "Iteration 130, loss = 0.48521722Iteration 117, loss = 0.27399236\n",
      "\n",
      "Iteration 1, loss = 0.63847399\n",
      "Iteration 82, loss = 0.47929273\n",
      "Iteration 118, loss = 0.27395074\n",
      "Iteration 102, loss = 0.51140494Iteration 1, loss = 0.58435732\n",
      "\n",
      "Iteration 119, loss = 0.34129991\n",
      "Iteration 2, loss = 0.59381053\n",
      "Iteration 83, loss = 0.47922813\n",
      "Validation score: 0.805970\n",
      "Iteration 120, loss = 0.29610529\n",
      "Iteration 12, loss = 0.52358995\n",
      "Iteration 2, loss = 0.55553252\n",
      "Iteration 121, loss = 0.30543505\n",
      "Iteration 3, loss = 0.57308918\n",
      "Iteration 84, loss = 0.47919383\n",
      "Iteration 131, loss = 0.48521711\n",
      "Validation score: 0.805970Iteration 122, loss = 0.27303322\n",
      "\n",
      "Iteration 1, loss = 0.62016757Iteration 13, loss = 0.51923032\n",
      "Iteration 3, loss = 0.54821721\n",
      "Iteration 123, loss = 0.30026380\n",
      "Iteration 132, loss = 0.48521699\n",
      "Iteration 124, loss = 0.27839377\n",
      "Validation score: 0.805970\n",
      "Iteration 14, loss = 0.51875876\n",
      "Iteration 125, loss = 0.27442435\n",
      "Iteration 126, loss = 0.27883483\n",
      "Iteration 103, loss = 0.51140478\n",
      "Iteration 127, loss = 0.29058439\n",
      "Iteration 104, loss = 0.51140531\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "Iteration 128, loss = 0.26916894\n",
      "Iteration 129, loss = 0.30755513\n",
      "Iteration 105, loss = 0.51140264\n",
      "Iteration 130, loss = 0.26890620\n",
      "\n",
      "Iteration 4, loss = 0.56343664Iteration 85, loss = 0.47924706\n",
      "Iteration 131, loss = 0.27879174\n",
      "\n",
      "Iteration 106, loss = 0.51140293\n",
      "Iteration 2, loss = 0.52372037\n",
      "Iteration 132, loss = 0.28495536\n",
      "Iteration 3, loss = 0.51493178\n",
      "Iteration 15, loss = 0.51622786\n",
      "Iteration 86, loss = 0.47918622\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178\n",
      "Iteration 133, loss = 0.26178901\n",
      "Iteration 4, loss = 0.50780788\n",
      "Iteration 134, loss = 0.26991826\n",
      "Iteration 107, loss = 0.51140281\n",
      "Iteration 5, loss = 0.50708945Iteration 135, loss = 0.29254356\n",
      "Iteration 87, loss = 0.47917169\n",
      "Iteration 136, loss = 0.26387107\n",
      "Iteration 133, loss = 0.48521687Iteration 108, loss = 0.51140296\n",
      "\n",
      "Iteration 137, loss = 0.29262335\n",
      "Iteration 88, loss = 0.47918173\n",
      "Validation score: 0.805970\n",
      "\n",
      "Iteration 138, loss = 0.26551251\n",
      "Iteration 89, loss = 0.47916780\n",
      "Iteration 139, loss = 0.28775779\n",
      "Iteration 16, loss = 0.51713085\n",
      "Iteration 140, loss = 0.25564465\n",
      "Iteration 90, loss = 0.47916318\n",
      "Iteration 141, loss = 0.27009858\n",
      "Iteration 17, loss = 0.51364361\n",
      "Iteration 134, loss = 0.48521674\n",
      "Iteration 142, loss = 0.27045985\n",
      "\n",
      "Iteration 91, loss = 0.47916309\n",
      "Iteration 5, loss = 0.55523582\n",
      "Iteration 18, loss = 0.51203387\n",
      "Iteration 143, loss = 0.28550903\n",
      "Validation score: 0.805970Iteration 6, loss = 0.50343901\n",
      "\n",
      "Iteration 144, loss = 0.26423649\n",
      "Iteration 92, loss = 0.47916150\n",
      "Iteration 7, loss = 0.50260053\n",
      "Iteration 6, loss = 0.54655429\n",
      "Iteration 145, loss = 0.27391832\n",
      "Iteration 135, loss = 0.48521662Iteration 4, loss = 0.54329945\n",
      "Iteration 146, loss = 0.28019712\n",
      "Iteration 7, loss = 0.53938469\n",
      "Iteration 147, loss = 0.27266371\n",
      "Iteration 5, loss = 0.53858620\n",
      "Iteration 148, loss = 0.26862281\n",
      "Iteration 8, loss = 0.53396752Iteration 6, loss = 0.53565873Iteration 149, loss = 0.26206273\n",
      "\n",
      "Iteration 93, loss = 0.47916617\n",
      "Validation score: 0.805970\n",
      "Iteration 150, loss = 0.28184768\n",
      "Iteration 151, loss = 0.26684166\n",
      "Iteration 136, loss = 0.48521650\n",
      "\n",
      "Iteration 94, loss = 0.47915832\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 109, loss = 0.51140276\n",
      "Iteration 152, loss = 0.27346007\n",
      "Iteration 8, loss = 0.49974218\n",
      "Iteration 153, loss = 0.28907770Iteration 7, loss = 0.53069491\n",
      "\n",
      "Iteration 9, loss = 0.50054403\n",
      "Iteration 137, loss = 0.48521637\n",
      "Iteration 110, loss = 0.51140253\n",
      "Validation score: 0.805970\n",
      "Iteration 95, loss = 0.47917347\n",
      "Iteration 154, loss = 0.26418949\n",
      "Iteration 10, loss = 0.49505489\n",
      "Iteration 8, loss = 0.52813079\n",
      "Iteration 155, loss = 0.25942918\n",
      "Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "Iteration 111, loss = 0.51140315\n",
      "Iteration 138, loss = 0.48521625\n",
      "Iteration 11, loss = 0.49027160\n",
      "Validation score: 0.805970\n",
      "Iteration 96, loss = 0.47923562\n",
      "Iteration 9, loss = 0.52609732\n",
      "Iteration 12, loss = 0.48794894\n",
      "Iteration 112, loss = 0.51140294Iteration 139, loss = 0.48521613\n",
      "Iteration 13, loss = 0.49115734\n",
      "Iteration 19, loss = 0.51169443\n",
      "Validation score: 0.805970\n",
      "Iteration 97, loss = 0.47916772\n",
      "Iteration 1, loss = 0.67781160\n",
      "Iteration 14, loss = 0.49523045\n",
      "Iteration 2, loss = 0.52907365Iteration 140, loss = 0.48521600\n",
      "\n",
      "Iteration 20, loss = 0.51175707\n",
      "Iteration 15, loss = 0.48898286Iteration 98, loss = 0.47916075\n",
      "\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036\n",
      "Validation score: 0.805970\n",
      "Iteration 3, loss = 0.52726046\n",
      "Iteration 16, loss = 0.47925954\n",
      "Iteration 4, loss = 0.51668405Iteration 21, loss = 0.51289099\n",
      "\n",
      "Iteration 141, loss = 0.48521588\n",
      "Iteration 10, loss = 0.52433151Iteration 99, loss = 0.47915051\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 17, loss = 0.47404013\n",
      "Iteration 5, loss = 0.50990869\n",
      "Iteration 18, loss = 0.47010203\n",
      "Iteration 9, loss = 0.52774644Iteration 22, loss = 0.50833966\n",
      "\n",
      "Iteration 6, loss = 0.50328323\n",
      "Iteration 142, loss = 0.48521576\n",
      "Iteration 11, loss = 0.52229759\n",
      "Validation score: 0.805970\n",
      "Iteration 7, loss = 0.50013777\n",
      "Iteration 100, loss = 0.47915177\n",
      "Iteration 8, loss = 0.50124152\n",
      "Iteration 9, loss = 0.48942866\n",
      "Iteration 101, loss = 0.47914979\n",
      "Iteration 10, loss = 0.48484879\n",
      "Iteration 102, loss = 0.47915220\n",
      "Iteration 11, loss = 0.48428143\n",
      "Iteration 19, loss = 0.46220881\n",
      "Iteration 12, loss = 0.49496376\n",
      "Iteration 12, loss = 0.51980090\n",
      "\n",
      "Iteration 13, loss = 0.47580655\n",
      "Iteration 103, loss = 0.47915398\n",
      "Iteration 20, loss = 0.46500256\n",
      "Iteration 13, loss = 0.52036610\n",
      "Iteration 21, loss = 0.45957045\n",
      "Iteration 14, loss = 0.46955936\n",
      "Iteration 104, loss = 0.47914972\n",
      "Iteration 15, loss = 0.45812105Iteration 22, loss = 0.45979877Iteration 113, loss = 0.51140292\n",
      "Iteration 14, loss = 0.52021795\n",
      "\n",
      "Iteration 105, loss = 0.47914918\n",
      "Iteration 143, loss = 0.48521564\n",
      "Iteration 23, loss = 0.45530933Iteration 16, loss = 0.45187863\n",
      "\n",
      "Validation score: 0.805970\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 10, loss = 0.52414534Iteration 17, loss = 0.45773441\n",
      "\n",
      "Iteration 15, loss = 0.51604462\n",
      "Iteration 24, loss = 0.45993632\n",
      "Iteration 23, loss = 0.50969441\n",
      "Iteration 144, loss = 0.48521528\n",
      "Iteration 18, loss = 0.44926534\n",
      "Validation score: 0.805970\n",
      "Iteration 106, loss = 0.47915662\n",
      "\n",
      "Iteration 19, loss = 0.43691122Iteration 11, loss = 0.52169097Iteration 16, loss = 0.51701657\n",
      "\n",
      "\n",
      "Iteration 25, loss = 0.46338556\n",
      "Iteration 114, loss = 0.51140238\n",
      "Iteration 20, loss = 0.42333166\n",
      "Iteration 26, loss = 0.46478400Iteration 107, loss = 0.47914990\n",
      "\n",
      "Iteration 12, loss = 0.51809766Iteration 17, loss = 0.51628955Iteration 21, loss = 0.43553886\n",
      "\n",
      "Iteration 115, loss = 0.51140298Iteration 145, loss = 0.48521585\n",
      "\n",
      "Iteration 22, loss = 0.41797381\n",
      "Iteration 27, loss = 0.45388114\n",
      "Iteration 24, loss = 0.50913156\n",
      "Iteration 108, loss = 0.47915283\n",
      "Iteration 18, loss = 0.51572204\n",
      "Iteration 23, loss = 0.41538658\n",
      "Validation score: 0.805970\n",
      "Iteration 28, loss = 0.44473320\n",
      "Iteration 116, loss = 0.51140274\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 24, loss = 0.40771359\n",
      "\n",
      "Iteration 19, loss = 0.51391626Iteration 29, loss = 0.44319670\n",
      "\n",
      "Iteration 109, loss = 0.47915059\n",
      "Iteration 25, loss = 0.38995401Iteration 25, loss = 0.51161176\n",
      "\n",
      "Iteration 30, loss = 0.44903959\n",
      "Iteration 117, loss = 0.51140255\n",
      "Iteration 26, loss = 0.40945362\n",
      "Iteration 13, loss = 0.51718478\n",
      "Iteration 20, loss = 0.51500102\n",
      "Iteration 110, loss = 0.47914703\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "Iteration 31, loss = 0.44498613Iteration 146, loss = 0.48521599\n",
      "\n",
      "Iteration 27, loss = 0.39735213\n",
      "Validation score: 0.805970\n",
      "Iteration 118, loss = 0.51140250\n",
      "Iteration 28, loss = 0.38681943Iteration 14, loss = 0.51329268\n",
      "\n",
      "Iteration 21, loss = 0.51321781\n",
      "Iteration 111, loss = 0.47914733\n",
      "Iteration 32, loss = 0.44161341\n",
      "Iteration 29, loss = 0.39699348\n",
      "Iteration 119, loss = 0.51140237\n",
      "Iteration 15, loss = 0.51392339\n",
      "Iteration 30, loss = 0.38027663\n",
      "Iteration 33, loss = 0.44532604\n",
      "Iteration 26, loss = 0.50838688\n",
      "Iteration 22, loss = 0.51588539\n",
      "Iteration 112, loss = 0.47914856\n",
      "Iteration 31, loss = 0.37438429\n",
      "Iteration 147, loss = 0.48521601\n",
      "Validation score: 0.805970\n",
      "Iteration 120, loss = 0.51140236\n",
      "Iteration 16, loss = 0.51047392\n",
      "Iteration 34, loss = 0.44896273\n",
      "Iteration 32, loss = 0.37278416\n",
      "Iteration 23, loss = 0.51267524\n",
      "Iteration 113, loss = 0.47914860\n",
      "Iteration 35, loss = 0.43847375\n",
      "Iteration 33, loss = 0.36803062\n",
      "Iteration 121, loss = 0.51140222\n",
      "Iteration 36, loss = 0.43232418\n",
      "Iteration 34, loss = 0.36158364Iteration 27, loss = 0.50842098\n",
      "\n",
      "Iteration 24, loss = 0.51176347\n",
      "Iteration 37, loss = 0.42503901Iteration 35, loss = 0.35679448\n",
      "\n",
      "Iteration 122, loss = 0.51140235\n",
      "Iteration 114, loss = 0.47914809\n",
      "Iteration 36, loss = 0.37220548\n",
      "Iteration 28, loss = 0.50747646\n",
      "Iteration 25, loss = 0.51277710\n",
      "Iteration 38, loss = 0.43878654\n",
      "Iteration 37, loss = 0.36713901\n",
      "Iteration 123, loss = 0.51140234\n",
      "Iteration 115, loss = 0.47914817\n",
      "Iteration 26, loss = 0.51161862Iteration 38, loss = 0.34718128\n",
      "\n",
      "Iteration 148, loss = 0.48521600\n",
      "Validation score: 0.805970Iteration 39, loss = 0.42853144\n",
      "\n",
      "Iteration 39, loss = 0.34741339\n",
      "Iteration 124, loss = 0.51140232\n",
      "Iteration 116, loss = 0.47914829\n",
      "Iteration 27, loss = 0.51199372\n",
      "Iteration 149, loss = 0.48521598\n",
      "Validation score: 0.805970Iteration 40, loss = 0.35012461\n",
      "Iteration 40, loss = 0.42183853\n",
      "\n",
      "Iteration 125, loss = 0.51140230\n",
      "Iteration 41, loss = 0.35163186\n",
      "Iteration 117, loss = 0.47914880\n",
      "Iteration 28, loss = 0.51240865\n",
      "Iteration 41, loss = 0.42045415\n",
      "Iteration 42, loss = 0.34444324\n",
      "Iteration 29, loss = 0.50740395\n",
      "Iteration 126, loss = 0.51140273\n",
      "Iteration 42, loss = 0.42506498\n",
      "Iteration 29, loss = 0.51093976\n",
      "Iteration 43, loss = 0.34664873\n",
      "Iteration 118, loss = 0.47914814\n",
      "Iteration 44, loss = 0.33089075\n",
      "Iteration 17, loss = 0.51049557\n",
      "Iteration 30, loss = 0.50642192\n",
      "Iteration 127, loss = 0.51140249\n",
      "Iteration 30, loss = 0.51085609\n",
      "Iteration 45, loss = 0.34854596\n",
      "Iteration 119, loss = 0.47914839\n",
      "Iteration 43, loss = 0.42571210\n",
      "Iteration 46, loss = 0.35221014\n",
      "Iteration 18, loss = 0.50855620\n",
      "Iteration 31, loss = 0.50696713\n",
      "Iteration 128, loss = 0.51140230\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 31, loss = 0.51159917\n",
      "Iteration 47, loss = 0.35241758\n",
      "Iteration 44, loss = 0.42600871\n",
      "Iteration 120, loss = 0.47914961\n",
      "Iteration 19, loss = 0.50741549Iteration 48, loss = 0.33855040\n",
      "Iteration 129, loss = 0.51140222Iteration 32, loss = 0.50647256\n",
      "Iteration 32, loss = 0.51236741\n",
      "Iteration 45, loss = 0.41094858\n",
      "Iteration 49, loss = 0.34901096Iteration 150, loss = 0.48521595\n",
      "\n",
      "Iteration 121, loss = 0.47914798\n",
      "Validation score: 0.805970\n",
      "Iteration 33, loss = 0.51074865\n",
      "Iteration 33, loss = 0.50957131\n",
      "Iteration 46, loss = 0.40675721\n",
      "Iteration 20, loss = 0.50621946\n",
      "Iteration 151, loss = 0.48521593\n",
      "Iteration 122, loss = 0.47914796\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001\n",
      "Validation score: 0.805970\n",
      "Iteration 47, loss = 0.41061750\n",
      "Iteration 34, loss = 0.50894871\n",
      "Iteration 34, loss = 0.50671521\n",
      "Iteration 21, loss = 0.50579295\n",
      "Iteration 152, loss = 0.48521590\n",
      "Iteration 48, loss = 0.41241166\n",
      "Validation score: 0.805970\n",
      "Iteration 123, loss = 0.47914753\n",
      "Iteration 35, loss = 0.50889355Iteration 49, loss = 0.39423421\n",
      "\n",
      "Iteration 22, loss = 0.50602871\n",
      "Iteration 35, loss = 0.50954680\n",
      "Iteration 153, loss = 0.48521588\n",
      "Iteration 124, loss = 0.47914787\n",
      "Iteration 50, loss = 0.41442567\n",
      "Iteration 51, loss = 0.42332514\n",
      "Iteration 36, loss = 0.51036397\n",
      "Iteration 36, loss = 0.50906676\n",
      "Iteration 130, loss = 0.51140225\n",
      "Iteration 125, loss = 0.47914779\n",
      "Iteration 37, loss = 0.51067561\n",
      "Iteration 37, loss = 0.51130298\n",
      "Iteration 131, loss = 0.51140230\n",
      "Iteration 52, loss = 0.41476173\n",
      "Iteration 126, loss = 0.47914777\n",
      "Iteration 38, loss = 0.50852214\n",
      "Iteration 132, loss = 0.51140228Iteration 38, loss = 0.50793387\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 53, loss = 0.39151177Iteration 127, loss = 0.47914774\n",
      "\n",
      "Iteration 39, loss = 0.50830660\n",
      "Iteration 39, loss = 0.50798662\n",
      "\n",
      "Iteration 54, loss = 0.39361864\n",
      "Iteration 40, loss = 0.50823150\n",
      "Iteration 128, loss = 0.47914777\n",
      "Iteration 50, loss = 0.32695503\n",
      "Iteration 40, loss = 0.50527042\n",
      "Iteration 55, loss = 0.38405747\n",
      "Iteration 51, loss = 0.33224364\n",
      "Iteration 56, loss = 0.39500956Iteration 41, loss = 0.50763890\n",
      "\n",
      "Iteration 129, loss = 0.47914773\n",
      "Iteration 52, loss = 0.32855753Iteration 41, loss = 0.50860658\n",
      "Iteration 23, loss = 0.50568851\n",
      "Iteration 57, loss = 0.37954791\n",
      "Iteration 154, loss = 0.48521585\n",
      "Validation score: 0.805970\n",
      "\n",
      "Iteration 130, loss = 0.47914776\n",
      "Iteration 24, loss = 0.50519428\n",
      "Iteration 42, loss = 0.50444297\n",
      "Iteration 53, loss = 0.32829630\n",
      "Iteration 155, loss = 0.48521583Iteration 58, loss = 0.40614910\n",
      "\n",
      "Validation score: 0.805970\n",
      "Iteration 54, loss = 0.31709190Iteration 131, loss = 0.47914772\n",
      "Iteration 59, loss = 0.40851480\n",
      "Iteration 25, loss = 0.50334418\n",
      "Iteration 156, loss = 0.48521580Iteration 43, loss = 0.50503587\n",
      "\n",
      "Iteration 60, loss = 0.38475223\n",
      "\n",
      "Iteration 55, loss = 0.32338510\n",
      "Iteration 42, loss = 0.50713131\n",
      "Iteration 61, loss = 0.39728277\n",
      "Iteration 132, loss = 0.47914775\n",
      "Iteration 133, loss = 0.51140224\n",
      "Iteration 44, loss = 0.50526576\n",
      "Validation score: 0.805970\n",
      "Iteration 56, loss = 0.32876717\n",
      "Iteration 62, loss = 0.38398136\n",
      "Iteration 57, loss = 0.35425366Iteration 43, loss = 0.50855178\n",
      "\n",
      "Iteration 133, loss = 0.47914771\n",
      "Iteration 157, loss = 0.48521578\n",
      "Iteration 63, loss = 0.36632197\n",
      "Iteration 45, loss = 0.50474897Iteration 58, loss = 0.31384247\n",
      "\n",
      "Iteration 44, loss = 0.50672610\n",
      "Iteration 59, loss = 0.35355946\n",
      "Iteration 134, loss = 0.47914776\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 134, loss = 0.51140224\n",
      "Iteration 64, loss = 0.39644145\n",
      "Validation score: 0.805970\n",
      "Iteration 46, loss = 0.50498034\n",
      "Iteration 60, loss = 0.32754568\n",
      "Iteration 26, loss = 0.50609724\n",
      "Iteration 65, loss = 0.40841595Iteration 61, loss = 0.33737404\n",
      "\n",
      "Iteration 158, loss = 0.48521575\n",
      "Iteration 135, loss = 0.47914767\n",
      "Iteration 62, loss = 0.32623318Iteration 47, loss = 0.50720045\n",
      "\n",
      "Iteration 66, loss = 0.38154296\n",
      "Iteration 27, loss = 0.50319723Iteration 63, loss = 0.33844899\n",
      "Iteration 67, loss = 0.37348898\n",
      "Iteration 135, loss = 0.51140227\n",
      "Iteration 136, loss = 0.47914772\n",
      "Validation score: 0.805970\n",
      "Iteration 64, loss = 0.31833145\n",
      "Iteration 48, loss = 0.50521468\n",
      "Iteration 68, loss = 0.36925341\n",
      "\n",
      "Iteration 65, loss = 0.30717565\n",
      "Iteration 69, loss = 0.37303149\n",
      "Iteration 66, loss = 0.29065881\n",
      "Iteration 136, loss = 0.51140224\n",
      "Iteration 49, loss = 0.50453118\n",
      "Iteration 137, loss = 0.47914771\n",
      "Iteration 70, loss = 0.36370301\n",
      "Iteration 67, loss = 0.33557744\n",
      "Iteration 28, loss = 0.50291916Iteration 68, loss = 0.30597940Iteration 137, loss = 0.51140225\n",
      "\n",
      "\n",
      "Iteration 45, loss = 0.50689634\n",
      "Iteration 159, loss = 0.48521573\n",
      "Iteration 138, loss = 0.47914769\n",
      "Iteration 69, loss = 0.29080115\n",
      "Iteration 71, loss = 0.39673872\n",
      "Validation score: 0.805970\n",
      "\n",
      "Iteration 138, loss = 0.51140224\n",
      "Iteration 70, loss = 0.31352089\n",
      "Iteration 72, loss = 0.40884819\n",
      "Iteration 139, loss = 0.47914772\n",
      "Iteration 71, loss = 0.28461720\n",
      "Iteration 73, loss = 0.37719399\n",
      "Iteration 29, loss = 0.50124346\n",
      "Iteration 139, loss = 0.51140225\n",
      "Iteration 50, loss = 0.50502532\n",
      "Iteration 72, loss = 0.29061062\n",
      "Iteration 140, loss = 0.47914769\n",
      "Iteration 74, loss = 0.36875190\n",
      "Iteration 73, loss = 0.29350691\n",
      "Iteration 30, loss = 0.50535935Iteration 140, loss = 0.51140223\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 51, loss = 0.50619892\n",
      "Iteration 74, loss = 0.29402743Iteration 75, loss = 0.35591212\n",
      "\n",
      "Iteration 46, loss = 0.50611389\n",
      "Iteration 141, loss = 0.47914770\n",
      "Iteration 75, loss = 0.28005013Iteration 76, loss = 0.35420269\n",
      "\n",
      "Iteration 52, loss = 0.50765741Iteration 47, loss = 0.50707999\n",
      "Iteration 76, loss = 0.28486108\n",
      "Iteration 77, loss = 0.35464329\n",
      "Iteration 160, loss = 0.48521571\n",
      "Iteration 142, loss = 0.47914768\n",
      "Iteration 77, loss = 0.28935111\n",
      "Validation score: 0.805970\n",
      "Iteration 48, loss = 0.50944924\n",
      "Iteration 78, loss = 0.29948292\n",
      "Iteration 1, loss = 0.72888057\n",
      "Iteration 78, loss = 0.35624649\n",
      "Iteration 161, loss = 0.48521568\n",
      "Iteration 79, loss = 0.29744050\n",
      "Validation score: 0.805970\n",
      "Validation score did not improve more than tol=0.000251 for 17 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 79, loss = 0.39283438Iteration 143, loss = 0.47914768\n",
      "\n",
      "Iteration 80, loss = 0.28648714\n",
      "Iteration 2, loss = 0.59744147\n",
      "Iteration 31, loss = 0.50174204\n",
      "\n",
      "Iteration 80, loss = 0.37397043\n",
      "Iteration 81, loss = 0.27092544\n",
      "\n",
      "Iteration 144, loss = 0.47914772\n",
      "Iteration 81, loss = 0.36692974\n",
      "Iteration 53, loss = 0.50453075Iteration 3, loss = 0.56581101\n",
      "Iteration 82, loss = 0.29867770\n",
      "Iteration 145, loss = 0.47914769\n",
      "Iteration 49, loss = 0.50790751Iteration 82, loss = 0.35278041\n",
      "Iteration 4, loss = 0.55021050\n",
      "Iteration 83, loss = 0.30184158\n",
      "\n",
      "Iteration 83, loss = 0.35046586\n",
      "Iteration 32, loss = 0.50063465\n",
      "Iteration 146, loss = 0.47914770\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 5, loss = 0.53837506\n",
      "Iteration 84, loss = 0.34584242\n",
      "Iteration 50, loss = 0.50649622\n",
      "Iteration 54, loss = 0.50485073Iteration 85, loss = 0.36861934\n",
      "Iteration 6, loss = 0.53192391\n",
      "Iteration 1, loss = 0.66309453\n",
      "Iteration 33, loss = 0.50174448\n",
      "Iteration 86, loss = 0.38746482\n",
      "Iteration 2, loss = 0.54428565\n",
      "Iteration 84, loss = 0.29837040\n",
      "Iteration 7, loss = 0.52095358\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Iteration 34, loss = 0.50123273\n",
      "Iteration 3, loss = 0.51922483\n",
      "Iteration 85, loss = 0.26824923\n",
      "Iteration 1, loss = 0.69568143\n",
      "Iteration 87, loss = 0.36292088\n",
      "Iteration 4, loss = 0.51081705\n",
      "Iteration 86, loss = 0.29574026\n",
      "Iteration 51, loss = 0.50568001Iteration 55, loss = 0.50389127\n",
      "\n",
      "Iteration 88, loss = 0.35425517\n",
      "\n",
      "Iteration 87, loss = 0.28779821\n",
      "Iteration 5, loss = 0.50497852\n",
      "Iteration 2, loss = 0.61079058\n",
      "Iteration 89, loss = 0.36132202\n",
      "Iteration 88, loss = 0.27401199\n",
      "Iteration 52, loss = 0.50642467\n",
      "Iteration 6, loss = 0.49806972\n",
      "Iteration 8, loss = 0.51564246\n",
      "Iteration 89, loss = 0.38694112\n",
      "Iteration 90, loss = 0.34632270Iteration 3, loss = 0.58828617\n",
      "\n",
      "Iteration 7, loss = 0.49542240\n",
      "Iteration 90, loss = 0.31959319\n",
      "Iteration 35, loss = 0.50081298\n",
      "Iteration 9, loss = 0.51095432\n",
      "Iteration 91, loss = 0.34667667\n",
      "Iteration 8, loss = 0.49449369\n",
      "Iteration 91, loss = 0.29340380\n",
      "Iteration 4, loss = 0.57396203\n",
      "Iteration 92, loss = 0.36104170\n",
      "Iteration 9, loss = 0.49555665\n",
      "Iteration 36, loss = 0.50010379Iteration 92, loss = 0.27534585\n",
      "\n",
      "Iteration 10, loss = 0.50734929Iteration 93, loss = 0.36643321\n",
      "Iteration 10, loss = 0.48186361\n",
      "Iteration 93, loss = 0.25336122\n",
      "\n",
      "Iteration 5, loss = 0.55883601\n",
      "Iteration 37, loss = 0.50358731\n",
      "Iteration 94, loss = 0.36187504\n",
      "Iteration 11, loss = 0.48123779\n",
      "Iteration 94, loss = 0.30039181\n",
      "Iteration 56, loss = 0.50381031\n",
      "Iteration 12, loss = 0.47303072\n",
      "Iteration 95, loss = 0.29163326\n",
      "Iteration 53, loss = 0.50724968Iteration 6, loss = 0.54727024\n",
      "\n",
      "Iteration 38, loss = 0.50139900\n",
      "Iteration 95, loss = 0.35004298\n",
      "Iteration 57, loss = 0.50373445\n",
      "Iteration 13, loss = 0.47175551\n",
      "Iteration 96, loss = 0.27225298\n",
      "Iteration 96, loss = 0.37416019\n",
      "Iteration 54, loss = 0.50690517Iteration 97, loss = 0.27039869\n",
      "\n",
      "Iteration 7, loss = 0.53800156\n",
      "Iteration 39, loss = 0.50261784Iteration 14, loss = 0.47336773\n",
      "\n",
      "Iteration 58, loss = 0.50380761\n",
      "Iteration 98, loss = 0.26394216\n",
      "Iteration 97, loss = 0.37031836\n",
      "Iteration 15, loss = 0.46325702\n",
      "Iteration 99, loss = 0.28620587Iteration 55, loss = 0.50790448\n",
      "\n",
      "Iteration 40, loss = 0.50263730\n",
      "Iteration 59, loss = 0.50378819\n",
      "Iteration 98, loss = 0.34978096\n",
      "Iteration 16, loss = 0.45806240\n",
      "Iteration 100, loss = 0.28530579\n",
      "Iteration 8, loss = 0.52964440\n",
      "Iteration 99, loss = 0.34667135Iteration 56, loss = 0.50473949\n",
      "\n",
      "Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "Iteration 101, loss = 0.26661229\n",
      "Iteration 17, loss = 0.45918507\n",
      "Iteration 41, loss = 0.50131897Iteration 60, loss = 0.50383787\n",
      "\n",
      "Iteration 102, loss = 0.29235800\n",
      "Iteration 9, loss = 0.52214438\n",
      "Iteration 18, loss = 0.45458628\n",
      "\n",
      "Iteration 103, loss = 0.27814539\n",
      "Iteration 42, loss = 0.50462642\n",
      "Iteration 19, loss = 0.45052455\n",
      "Iteration 104, loss = 0.25622944\n",
      "Iteration 10, loss = 0.51588595\n",
      "Iteration 11, loss = 0.50456247\n",
      "Iteration 20, loss = 0.44935350\n",
      "Iteration 105, loss = 0.25469272Iteration 43, loss = 0.50496997\n",
      "\n",
      "Iteration 21, loss = 0.44370107\n",
      "Iteration 1, loss = 0.65391523\n",
      "Iteration 106, loss = 0.27173022\n",
      "Iteration 11, loss = 0.51174137\n",
      "Iteration 12, loss = 0.50199031\n",
      "Iteration 44, loss = 0.49729791\n",
      "Iteration 107, loss = 0.24735165Iteration 22, loss = 0.43830960\n",
      "\n",
      "Iteration 2, loss = 0.53311565\n",
      "Iteration 108, loss = 0.24713612Iteration 13, loss = 0.49903905\n",
      "\n",
      "Iteration 23, loss = 0.43458414Iteration 12, loss = 0.50871023\n",
      "\n",
      "Iteration 45, loss = 0.50092615\n",
      "Iteration 3, loss = 0.51817940\n",
      "Iteration 109, loss = 0.27181691\n",
      "Iteration 24, loss = 0.43324419\n",
      "Iteration 4, loss = 0.51214683\n",
      "Iteration 14, loss = 0.50057064Iteration 110, loss = 0.30790904\n",
      "Iteration 46, loss = 0.49851081\n",
      "Iteration 25, loss = 0.42476739\n",
      "Iteration 13, loss = 0.50355066Iteration 57, loss = 0.50806397\n",
      "\n",
      "Iteration 5, loss = 0.50989946Iteration 61, loss = 0.50403967\n",
      "\n",
      "Iteration 111, loss = 0.30841022\n",
      "Iteration 26, loss = 0.42892092\n",
      "Iteration 112, loss = 0.29842998Iteration 47, loss = 0.49975496\n",
      "Iteration 6, loss = 0.50998092\n",
      "\n",
      "Iteration 58, loss = 0.50725477Iteration 62, loss = 0.50435027Iteration 14, loss = 0.50022692\n",
      "\n",
      "\n",
      "Iteration 113, loss = 0.25664770Iteration 27, loss = 0.42270388\n",
      "\n",
      "Iteration 7, loss = 0.50896901\n",
      "Iteration 48, loss = 0.49765804Iteration 114, loss = 0.23726382\n",
      "\n",
      "Iteration 28, loss = 0.41269365\n",
      "Iteration 59, loss = 0.50520167Iteration 15, loss = 0.49713752\n",
      "\n",
      "Iteration 63, loss = 0.50405505\n",
      "Iteration 115, loss = 0.23928286\n",
      "Iteration 8, loss = 0.50441632Iteration 29, loss = 0.40798119\n",
      "\n",
      "Iteration 116, loss = 0.27855231Iteration 49, loss = 0.49963027\n",
      "\n",
      "Iteration 30, loss = 0.42621452Iteration 9, loss = 0.50162872\n",
      "\n",
      "Iteration 60, loss = 0.50644292\n",
      "Iteration 64, loss = 0.50373860\n",
      "Iteration 117, loss = 0.28636299Iteration 16, loss = 0.49437292\n",
      "\n",
      "Iteration 50, loss = 0.49922677Iteration 31, loss = 0.41888093\n",
      "\n",
      "Iteration 118, loss = 0.28069796\n",
      "Iteration 10, loss = 0.49808053\n",
      "\n",
      "Iteration 32, loss = 0.40856201Iteration 17, loss = 0.49289346\n",
      "\n",
      "Iteration 11, loss = 0.49196360\n",
      "Iteration 119, loss = 0.25571537\n",
      "Iteration 51, loss = 0.49904856\n",
      "Iteration 33, loss = 0.39268556\n",
      "Iteration 15, loss = 0.50155669\n",
      "Iteration 12, loss = 0.49144801\n",
      "Iteration 120, loss = 0.24735189\n",
      "Iteration 34, loss = 0.39891126\n",
      "Iteration 52, loss = 0.49741413\n",
      "Iteration 121, loss = 0.29013377Iteration 13, loss = 0.48534518\n",
      "\n",
      "Iteration 18, loss = 0.49072731\n",
      "Iteration 16, loss = 0.49595983\n",
      "Iteration 35, loss = 0.39481680\n",
      "Iteration 122, loss = 0.24906737\n",
      "Iteration 14, loss = 0.48494640\n",
      "Iteration 53, loss = 0.49841682\n",
      "Iteration 61, loss = 0.50695117\n",
      "Iteration 36, loss = 0.41905072\n",
      "Iteration 123, loss = 0.24677681\n",
      "Iteration 19, loss = 0.48923337\n",
      "Iteration 17, loss = 0.49454569Iteration 65, loss = 0.50404456\n",
      "Iteration 124, loss = 0.24793061Iteration 37, loss = 0.39606767\n",
      "\n",
      "Iteration 15, loss = 0.48367462\n",
      "Iteration 54, loss = 0.49772389\n",
      "Iteration 62, loss = 0.50692523\n",
      "Iteration 20, loss = 0.48715091\n",
      "Iteration 125, loss = 0.25517144\n",
      "Iteration 38, loss = 0.38277293\n",
      "Iteration 16, loss = 0.48362368\n",
      "Iteration 66, loss = 0.50358518\n",
      "Iteration 39, loss = 0.37483021\n",
      "Iteration 63, loss = 0.50701150\n",
      "Iteration 17, loss = 0.47191372\n",
      "Iteration 55, loss = 0.49889624\n",
      "Iteration 126, loss = 0.24464697\n",
      "Iteration 21, loss = 0.48880270\n",
      "Iteration 40, loss = 0.38877321\n",
      "Iteration 18, loss = 0.47266265\n",
      "Iteration 18, loss = 0.49382427\n",
      "Iteration 127, loss = 0.22812232\n",
      "Iteration 64, loss = 0.50585748\n",
      "Iteration 56, loss = 0.49883213\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Iteration 41, loss = 0.38822603\n",
      "Iteration 19, loss = 0.47346761\n",
      "Iteration 22, loss = 0.48408643\n",
      "Iteration 128, loss = 0.25014811\n",
      "Iteration 42, loss = 0.41849488\n",
      "Iteration 67, loss = 0.50405454\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890\n",
      "Iteration 20, loss = 0.47330540\n",
      "Iteration 129, loss = 0.27678191\n",
      "Iteration 65, loss = 0.50492917\n",
      "Iteration 57, loss = 0.50258926\n",
      "Iteration 43, loss = 0.36817899\n",
      "Iteration 23, loss = 0.48597050\n",
      "Iteration 130, loss = 0.29126984\n",
      "Iteration 21, loss = 0.46121041\n",
      "Iteration 68, loss = 0.50339450\n",
      "Iteration 131, loss = 0.27398146\n",
      "Iteration 44, loss = 0.39173498\n",
      "Iteration 58, loss = 0.50078098\n",
      "Iteration 22, loss = 0.46892882\n",
      "Iteration 24, loss = 0.48346954Iteration 132, loss = 0.22488248\n",
      "\n",
      "Iteration 45, loss = 0.35454317\n",
      "Iteration 19, loss = 0.49534370\n",
      "Iteration 59, loss = 0.49877159\n",
      "Iteration 23, loss = 0.47022685\n",
      "Iteration 133, loss = 0.24204719\n",
      "Iteration 46, loss = 0.37334755\n",
      "Iteration 134, loss = 0.24708271\n",
      "Iteration 24, loss = 0.46997405Iteration 25, loss = 0.48286029\n",
      "\n",
      "Iteration 20, loss = 0.49232011\n",
      "Iteration 47, loss = 0.39292858\n",
      "Iteration 60, loss = 0.49716390\n",
      "Iteration 48, loss = 0.35392383Iteration 25, loss = 0.45850716\n",
      "\n",
      "Iteration 26, loss = 0.48169581\n",
      "Iteration 69, loss = 0.50340208Iteration 61, loss = 0.49668052\n",
      "\n",
      "Iteration 26, loss = 0.45656432\n",
      "Iteration 49, loss = 0.35441010\n",
      "Iteration 27, loss = 0.45455778\n",
      "Iteration 62, loss = 0.49621419\n",
      "Iteration 50, loss = 0.39262619\n",
      "Iteration 21, loss = 0.49207032\n",
      "Iteration 27, loss = 0.48182773\n",
      "Iteration 51, loss = 0.33738814\n",
      "Iteration 135, loss = 0.25609873\n",
      "Iteration 28, loss = 0.45714713\n",
      "Iteration 66, loss = 0.50593156\n",
      "Iteration 63, loss = 0.49605643\n",
      "Iteration 52, loss = 0.35117873Iteration 22, loss = 0.49009242\n",
      "\n",
      "Iteration 136, loss = 0.24709172\n",
      "Iteration 28, loss = 0.48280199\n",
      "Iteration 29, loss = 0.47350796\n",
      "Iteration 137, loss = 0.27304522\n",
      "Iteration 53, loss = 0.35702251Iteration 67, loss = 0.50383853\n",
      "\n",
      "Iteration 30, loss = 0.44264235\n",
      "Iteration 23, loss = 0.49194553\n",
      "Iteration 138, loss = 0.25003217\n",
      "Iteration 64, loss = 0.49623245\n",
      "Iteration 54, loss = 0.34142694Iteration 29, loss = 0.48084020\n",
      "\n",
      "Iteration 31, loss = 0.43912981\n",
      "Iteration 139, loss = 0.28467306\n",
      "Iteration 68, loss = 0.50568190Iteration 55, loss = 0.36283462\n",
      "Iteration 70, loss = 0.50338789\n",
      "Iteration 65, loss = 0.49605077\n",
      "Iteration 30, loss = 0.48047187\n",
      "Iteration 56, loss = 0.34427659Iteration 32, loss = 0.43225843\n",
      "\n",
      "Iteration 71, loss = 0.50340058\n",
      "Iteration 33, loss = 0.45047928\n",
      "Iteration 57, loss = 0.36588733\n",
      "Iteration 140, loss = 0.23181727\n",
      "Iteration 66, loss = 0.49603089\n",
      "Iteration 58, loss = 0.36066505\n",
      "Iteration 34, loss = 0.43302255\n",
      "\n",
      "Iteration 141, loss = 0.26631307\n",
      "Iteration 72, loss = 0.50345394\n",
      "Iteration 31, loss = 0.47935396\n",
      "Iteration 59, loss = 0.38396261\n",
      "Iteration 142, loss = 0.22913709\n",
      "Iteration 35, loss = 0.43214561\n",
      "Iteration 67, loss = 0.49597650\n",
      "Iteration 69, loss = 0.50454366Iteration 143, loss = 0.23469802\n",
      "\n",
      "Iteration 24, loss = 0.48887468\n",
      "Iteration 36, loss = 0.41851477\n",
      "Iteration 60, loss = 0.37790391\n",
      "Iteration 32, loss = 0.47878552\n",
      "Iteration 144, loss = 0.24069601\n",
      "Iteration 68, loss = 0.49598876\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890\n",
      "Iteration 37, loss = 0.40925367\n",
      "Iteration 61, loss = 0.33741355\n",
      "Iteration 145, loss = 0.22257080\n",
      "Iteration 70, loss = 0.50778135\n",
      "Iteration 25, loss = 0.49013337\n",
      "Iteration 38, loss = 0.41374092\n",
      "Iteration 146, loss = 0.26855484\n",
      "Iteration 33, loss = 0.48035546\n",
      "Iteration 62, loss = 0.35346102\n",
      "Iteration 69, loss = 0.49588081\n",
      "Iteration 147, loss = 0.22448653\n",
      "Iteration 71, loss = 0.50795617\n",
      "Iteration 63, loss = 0.33504758\n",
      "Iteration 73, loss = 0.50336532\n",
      "Iteration 39, loss = 0.40853516\n",
      "Iteration 148, loss = 0.26732691\n",
      "Iteration 70, loss = 0.49575096\n",
      "Iteration 40, loss = 0.41156337\n",
      "Iteration 34, loss = 0.47941358\n",
      "Iteration 72, loss = 0.50650640Iteration 149, loss = 0.24564809\n",
      "\n",
      "Iteration 26, loss = 0.48887546\n",
      "Iteration 41, loss = 0.41983123\n",
      "Iteration 150, loss = 0.24821214\n",
      "Iteration 71, loss = 0.49574611\n",
      "Iteration 64, loss = 0.33544554\n",
      "Iteration 35, loss = 0.47872026\n",
      "Iteration 151, loss = 0.21609135\n",
      "Iteration 73, loss = 0.50486872\n",
      "Iteration 42, loss = 0.47566937\n",
      "Iteration 74, loss = 0.50340387\n",
      "Iteration 65, loss = 0.35445774\n",
      "Iteration 152, loss = 0.24554308\n",
      "Iteration 72, loss = 0.49580562\n",
      "Iteration 43, loss = 0.41385641\n",
      "Iteration 153, loss = 0.27007060\n",
      "Iteration 74, loss = 0.50510138\n",
      "Iteration 66, loss = 0.42858640\n",
      "Iteration 27, loss = 0.48979744\n",
      "Iteration 44, loss = 0.42559281\n",
      "Iteration 36, loss = 0.47696929\n",
      "Iteration 154, loss = 0.28856019\n",
      "Iteration 73, loss = 0.49571250\n",
      "Iteration 67, loss = 0.34142138\n",
      "Iteration 45, loss = 0.39895005\n",
      "Iteration 155, loss = 0.24231981Iteration 75, loss = 0.50400108\n",
      "\n",
      "Iteration 75, loss = 0.50339384\n",
      "Iteration 68, loss = 0.33832544\n",
      "Iteration 37, loss = 0.47692191\n",
      "Iteration 46, loss = 0.39875765\n",
      "Iteration 156, loss = 0.23504453\n",
      "Iteration 74, loss = 0.49575081\n",
      "Iteration 69, loss = 0.35473814\n",
      "Iteration 76, loss = 0.50408963\n",
      "Iteration 157, loss = 0.24711070\n",
      "Iteration 76, loss = 0.50352449\n",
      "Iteration 38, loss = 0.47661016\n",
      "Iteration 70, loss = 0.33398841\n",
      "Iteration 47, loss = 0.41251793Iteration 75, loss = 0.49569568\n",
      "Iteration 158, loss = 0.24509529\n",
      "\n",
      "Iteration 77, loss = 0.50419779\n",
      "Iteration 71, loss = 0.34072610\n",
      "Iteration 159, loss = 0.23066337Iteration 48, loss = 0.40201833\n",
      "\n",
      "Iteration 28, loss = 0.48825798\n",
      "Iteration 39, loss = 0.47671531\n",
      "Iteration 76, loss = 0.49569254\n",
      "Iteration 72, loss = 0.32569873\n",
      "Iteration 49, loss = 0.38662828\n",
      "Iteration 160, loss = 0.23440518\n",
      "Iteration 78, loss = 0.50494671\n",
      "Iteration 73, loss = 0.33859986\n",
      "Iteration 161, loss = 0.21916248\n",
      "Iteration 29, loss = 0.48810655\n",
      "Iteration 50, loss = 0.39164554\n",
      "Iteration 77, loss = 0.49571184\n",
      "Iteration 40, loss = 0.47686493\n",
      "Iteration 162, loss = 0.20685983\n",
      "Iteration 74, loss = 0.37570309\n",
      "Iteration 51, loss = 0.39428239\n",
      "Iteration 79, loss = 0.50405919\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Iteration 163, loss = 0.22749766Iteration 30, loss = 0.48798346\n",
      "\n",
      "Iteration 52, loss = 0.38748696Iteration 75, loss = 0.33826619\n",
      "\n",
      "Iteration 78, loss = 0.49572299\n",
      "Iteration 41, loss = 0.47853565\n",
      "Iteration 164, loss = 0.21699502\n",
      "Iteration 80, loss = 0.50363431\n",
      "Iteration 76, loss = 0.34766099\n",
      "Iteration 53, loss = 0.38989365\n",
      "Iteration 31, loss = 0.48872615Iteration 165, loss = 0.25373888\n",
      "\n",
      "Iteration 79, loss = 0.49569703\n",
      "Iteration 77, loss = 0.34748898Iteration 54, loss = 0.41919869\n",
      "\n",
      "Iteration 42, loss = 0.47710846\n",
      "Iteration 166, loss = 0.24010404\n",
      "Iteration 81, loss = 0.50364433\n",
      "Iteration 78, loss = 0.32570270\n",
      "Iteration 167, loss = 0.22582132\n",
      "Iteration 77, loss = 0.50336546\n",
      "Iteration 80, loss = 0.49564104\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178\n",
      "Iteration 55, loss = 0.38530192\n",
      "Iteration 43, loss = 0.47657751\n",
      "Iteration 168, loss = 0.24392531\n",
      "Iteration 79, loss = 0.33994613\n",
      "Iteration 82, loss = 0.50371531\n",
      "Iteration 56, loss = 0.38189990\n",
      "Iteration 78, loss = 0.50337181\n",
      "Iteration 169, loss = 0.25334658\n",
      "Iteration 80, loss = 0.32121234\n",
      "Iteration 81, loss = 0.49563311\n",
      "Iteration 170, loss = 0.23517194Iteration 44, loss = 0.47671280\n",
      "\n",
      "Iteration 57, loss = 0.38701013\n",
      "Iteration 83, loss = 0.50367026Iteration 81, loss = 0.36009686\n",
      "\n",
      "Iteration 171, loss = 0.22569769\n",
      "Iteration 79, loss = 0.50338216\n",
      "Iteration 58, loss = 0.37438396\n",
      "Iteration 82, loss = 0.49564000\n",
      "Iteration 82, loss = 0.38766133\n",
      "Iteration 172, loss = 0.22828319\n",
      "Iteration 84, loss = 0.50260057\n",
      "Iteration 59, loss = 0.36754978Iteration 45, loss = 0.47724804\n",
      "\n",
      "Iteration 83, loss = 0.32519820\n",
      "Iteration 173, loss = 0.22545917\n",
      "Iteration 32, loss = 0.48806445\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178\n",
      "Iteration 83, loss = 0.49564131\n",
      "Iteration 60, loss = 0.39086342Iteration 84, loss = 0.34172665\n",
      "Iteration 174, loss = 0.23519158\n",
      "\n",
      "Iteration 85, loss = 0.50301924\n",
      "Iteration 46, loss = 0.47465962\n",
      "Iteration 175, loss = 0.24117443\n",
      "Iteration 85, loss = 0.32957933\n",
      "Iteration 61, loss = 0.44071754\n",
      "Iteration 80, loss = 0.50333278\n",
      "Iteration 84, loss = 0.49565536\n",
      "Iteration 176, loss = 0.21708412\n",
      "Iteration 86, loss = 0.36632625\n",
      "Iteration 86, loss = 0.50264356\n",
      "Iteration 177, loss = 0.24886182Iteration 47, loss = 0.47506776\n",
      "\n",
      "Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.48963901\n",
      "Iteration 87, loss = 0.32285896\n",
      "Iteration 85, loss = 0.49564442\n",
      "Iteration 62, loss = 0.37693548\n",
      "Iteration 87, loss = 0.50241877\n",
      "Iteration 88, loss = 0.38621227\n",
      "Iteration 48, loss = 0.47457917\n",
      "Iteration 81, loss = 0.50333366\n",
      "Iteration 86, loss = 0.49563766\n",
      "Iteration 89, loss = 0.39032380\n",
      "Iteration 63, loss = 0.38963485\n",
      "Iteration 88, loss = 0.50254917\n",
      "Iteration 90, loss = 0.35825926\n",
      "Iteration 82, loss = 0.50333370Iteration 49, loss = 0.47408251\n",
      "\n",
      "Iteration 87, loss = 0.49564564Iteration 64, loss = 0.37467873\n",
      "\n",
      "Iteration 1, loss = 0.68160774\n",
      "Iteration 91, loss = 0.33205543\n",
      "Iteration 65, loss = 0.36944361Iteration 2, loss = 0.54507452\n",
      "\n",
      "Iteration 89, loss = 0.50271629\n",
      "Iteration 34, loss = 0.48753697\n",
      "Iteration 92, loss = 0.33200957\n",
      "Iteration 88, loss = 0.49563250\n",
      "Iteration 3, loss = 0.53374485\n",
      "Iteration 66, loss = 0.46524742\n",
      "Iteration 50, loss = 0.47375051\n",
      "Iteration 93, loss = 0.35452546Iteration 4, loss = 0.53339894\n",
      "\n",
      "Iteration 90, loss = 0.50261536\n",
      "Iteration 67, loss = 0.38424398\n",
      "Iteration 35, loss = 0.48771318\n",
      "Iteration 5, loss = 0.52623503\n",
      "Iteration 89, loss = 0.49563318\n",
      "Iteration 94, loss = 0.33953898\n",
      "Iteration 51, loss = 0.47431366\n",
      "Iteration 91, loss = 0.50262891\n",
      "Iteration 68, loss = 0.38171319Iteration 95, loss = 0.35341309\n",
      "Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 6, loss = 0.51929773\n",
      "Iteration 83, loss = 0.50331522Iteration 90, loss = 0.49562446\n",
      "\n",
      "Iteration 7, loss = 0.51579576\n",
      "Iteration 52, loss = 0.47434289Iteration 69, loss = 0.36649621\n",
      "\n",
      "Iteration 92, loss = 0.50278589\n",
      "Iteration 8, loss = 0.50549198\n",
      "Iteration 36, loss = 0.48710914Iteration 91, loss = 0.49562854\n",
      "\n",
      "Iteration 70, loss = 0.36180857\n",
      "Iteration 93, loss = 0.50236663Iteration 53, loss = 0.47453169\n",
      "\n",
      "Iteration 71, loss = 0.39783214\n",
      "Iteration 92, loss = 0.49561999Iteration 1, loss = 0.67504122\n",
      "\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036Iteration 84, loss = 0.50332071\n",
      "\n",
      "Iteration 9, loss = 0.50882718\n",
      "Iteration 72, loss = 0.36591615\n",
      "Iteration 2, loss = 0.55443814\n",
      "Iteration 54, loss = 0.47323082\n",
      "Iteration 10, loss = 0.50287498\n",
      "Iteration 93, loss = 0.49562090\n",
      "Iteration 37, loss = 0.49043542\n",
      "Iteration 73, loss = 0.36161351\n",
      "Iteration 11, loss = 0.51095001\n",
      "Iteration 3, loss = 0.53036969\n",
      "Iteration 12, loss = 0.49922413Iteration 74, loss = 0.37360392\n",
      "\n",
      "Iteration 4, loss = 0.52730746\n",
      "Iteration 94, loss = 0.49561708\n",
      "Iteration 38, loss = 0.48903195\n",
      "Iteration 55, loss = 0.47290401\n",
      "Iteration 13, loss = 0.49202827\n",
      "Iteration 5, loss = 0.52054523\n",
      "Iteration 75, loss = 0.36670932\n",
      "Iteration 14, loss = 0.48618914\n",
      "Iteration 95, loss = 0.49561647\n",
      "Iteration 6, loss = 0.51241995\n",
      "Iteration 76, loss = 0.37501707\n",
      "Iteration 85, loss = 0.50331733\n",
      "Iteration 56, loss = 0.47331374\n",
      "Iteration 15, loss = 0.48510340\n",
      "Iteration 77, loss = 0.37392570\n",
      "Iteration 7, loss = 0.51407908\n",
      "Iteration 16, loss = 0.48546424\n",
      "Iteration 96, loss = 0.49561918\n",
      "Iteration 86, loss = 0.50331742\n",
      "Iteration 78, loss = 0.34375010Iteration 17, loss = 0.48654152\n",
      "Iteration 57, loss = 0.47272462\n",
      "Iteration 8, loss = 0.50524893\n",
      "\n",
      "Iteration 94, loss = 0.50239125Iteration 18, loss = 0.47595857\n",
      "Iteration 79, loss = 0.35166846Iteration 9, loss = 0.50025500\n",
      "\n",
      "Iteration 97, loss = 0.49561651\n",
      "Iteration 19, loss = 0.46409060\n",
      "Iteration 39, loss = 0.48775833\n",
      "Iteration 58, loss = 0.47393077\n",
      "Iteration 80, loss = 0.38158908\n",
      "Iteration 10, loss = 0.49996871\n",
      "Iteration 20, loss = 0.46505688\n",
      "Iteration 98, loss = 0.49561800\n",
      "Iteration 11, loss = 0.49568918\n",
      "Iteration 40, loss = 0.48728596\n",
      "Iteration 81, loss = 0.35036251\n",
      "Iteration 59, loss = 0.47455070\n",
      "Iteration 12, loss = 0.48555471\n",
      "Iteration 99, loss = 0.49561623\n",
      "\n",
      "Iteration 13, loss = 0.47666658\n",
      "Iteration 87, loss = 0.50333670\n",
      "Iteration 82, loss = 0.36384893\n",
      "Iteration 60, loss = 0.47288327\n",
      "Iteration 100, loss = 0.49561627\n",
      "Iteration 14, loss = 0.46702903\n",
      "Iteration 83, loss = 0.40112212\n",
      "Iteration 95, loss = 0.50236601\n",
      "Iteration 88, loss = 0.50331570\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Iteration 21, loss = 0.45705592\n",
      "Iteration 84, loss = 0.35565107Iteration 15, loss = 0.46656870\n",
      "Iteration 101, loss = 0.49562072\n",
      "\n",
      "Iteration 22, loss = 0.45313407\n",
      "Iteration 85, loss = 0.38586444\n",
      "Iteration 96, loss = 0.50231940\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890\n",
      "Iteration 41, loss = 0.49257759\n",
      "Iteration 16, loss = 0.46801929\n",
      "Iteration 23, loss = 0.45231722\n",
      "Iteration 102, loss = 0.49561396Iteration 86, loss = 0.37279005\n",
      "Iteration 24, loss = 0.45782689\n",
      "Iteration 17, loss = 0.46713156\n",
      "Iteration 97, loss = 0.50225541Iteration 87, loss = 0.35733630\n",
      "\n",
      "Iteration 89, loss = 0.50331591\n",
      "\n",
      "Iteration 25, loss = 0.46438545\n",
      "Iteration 18, loss = 0.45058636\n",
      "Iteration 26, loss = 0.44932786\n",
      "Iteration 98, loss = 0.50225061Iteration 90, loss = 0.50330573\n",
      "Iteration 88, loss = 0.33967099\n",
      "\n",
      "Iteration 19, loss = 0.45798145Iteration 103, loss = 0.49561477\n",
      "Iteration 27, loss = 0.45306419\n",
      "Iteration 89, loss = 0.36845087\n",
      "Iteration 20, loss = 0.44735686\n",
      "Iteration 61, loss = 0.47339171\n",
      "Iteration 99, loss = 0.50219643\n",
      "Iteration 91, loss = 0.50330812\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036\n",
      "Iteration 28, loss = 0.45692688\n",
      "Iteration 21, loss = 0.45285814\n",
      "Iteration 90, loss = 0.33722214\n",
      "Iteration 29, loss = 0.44806685\n",
      "\n",
      "\n",
      "Iteration 100, loss = 0.50219604\n",
      "Iteration 62, loss = 0.47287376\n",
      "Iteration 22, loss = 0.43060746\n",
      "Iteration 91, loss = 0.33378737\n",
      "Iteration 30, loss = 0.44440176\n",
      "Iteration 23, loss = 0.43081328\n",
      "Iteration 31, loss = 0.46232890\n",
      "Iteration 92, loss = 0.35222128\n",
      "Iteration 101, loss = 0.50221838\n",
      "Iteration 42, loss = 0.48963711\n",
      "Iteration 63, loss = 0.47364314\n",
      "Iteration 24, loss = 0.43499192\n",
      "Iteration 32, loss = 0.42971282\n",
      "Iteration 102, loss = 0.50221115\n",
      "Iteration 33, loss = 0.43854858\n",
      "Iteration 25, loss = 0.43350743\n",
      "Iteration 43, loss = 0.48852615\n",
      "Iteration 34, loss = 0.42919098\n",
      "Iteration 26, loss = 0.41809811\n",
      "Iteration 93, loss = 0.36989443Iteration 103, loss = 0.50227639\n",
      "\n",
      "Iteration 64, loss = 0.47252221\n",
      "Iteration 44, loss = 0.48852878\n",
      "Iteration 35, loss = 0.42455994\n",
      "Iteration 27, loss = 0.41579939\n",
      "Iteration 94, loss = 0.42447120\n",
      "Iteration 92, loss = 0.50329927\n",
      "Iteration 104, loss = 0.49561473Iteration 36, loss = 0.44080178Iteration 104, loss = 0.50225886\n",
      "\n",
      "\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "Iteration 45, loss = 0.48817438\n",
      "Iteration 95, loss = 0.37505159Iteration 37, loss = 0.43281341\n",
      "\n",
      "Iteration 65, loss = 0.47156939\n",
      "Iteration 93, loss = 0.50329902Iteration 105, loss = 0.50230665\n",
      "Iteration 105, loss = 0.49561286\n",
      "Iteration 38, loss = 0.40989783\n",
      "Iteration 96, loss = 0.40227019\n",
      "Iteration 28, loss = 0.40443721\n",
      "Iteration 46, loss = 0.48776459\n",
      "Iteration 39, loss = 0.42939820\n",
      "Iteration 97, loss = 0.39171451\n",
      "Iteration 66, loss = 0.47443175\n",
      "Iteration 29, loss = 0.40976578\n",
      "Iteration 106, loss = 0.50225571Iteration 106, loss = 0.49561298\n",
      "\n",
      "Iteration 40, loss = 0.41460064\n",
      "\n",
      "Iteration 98, loss = 0.38602945\n",
      "Iteration 47, loss = 0.48679141\n",
      "Iteration 41, loss = 0.40507131\n",
      "Iteration 107, loss = 0.50227105\n",
      "Iteration 30, loss = 0.40021839\n",
      "Iteration 42, loss = 0.42280220\n",
      "Iteration 67, loss = 0.47305566Iteration 48, loss = 0.48656965\n",
      "\n",
      "Iteration 94, loss = 0.50329892Iteration 43, loss = 0.42605766Iteration 107, loss = 0.49561265\n",
      "\n",
      "Iteration 108, loss = 0.50223680\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178\n",
      "Iteration 31, loss = 0.39170355Iteration 44, loss = 0.40304853\n",
      "\n",
      "Iteration 49, loss = 0.48622715\n",
      "Iteration 99, loss = 0.40967075\n",
      "\n",
      "Iteration 108, loss = 0.49561272Iteration 45, loss = 0.38822862\n",
      "Iteration 109, loss = 0.50215497\n",
      "\n",
      "Iteration 68, loss = 0.47290467Iteration 50, loss = 0.48590794\n",
      "\n",
      "Iteration 100, loss = 0.38680407\n",
      "Iteration 95, loss = 0.50329774Iteration 110, loss = 0.50216399\n",
      "Iteration 32, loss = 0.43434973\n",
      "Iteration 51, loss = 0.48582535\n",
      "Iteration 109, loss = 0.49561285\n",
      "Iteration 101, loss = 0.38714676\n",
      "\n",
      "Iteration 111, loss = 0.50216219\n",
      "Iteration 46, loss = 0.40700730Iteration 52, loss = 0.48562313\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890\n",
      "Iteration 110, loss = 0.49561261\n",
      "\n",
      "Iteration 69, loss = 0.47218453\n",
      "Iteration 96, loss = 0.50329707Iteration 33, loss = 0.42319918\n",
      "Iteration 102, loss = 0.37045507\n",
      "Iteration 47, loss = 0.40273298\n",
      "Iteration 53, loss = 0.48498734Iteration 111, loss = 0.49561273Iteration 112, loss = 0.50214922\n",
      "\n",
      "\n",
      "Iteration 34, loss = 0.40476244Iteration 103, loss = 0.38295613\n",
      "\n",
      "Iteration 70, loss = 0.47387390\n",
      "Iteration 48, loss = 0.40364768\n",
      "Iteration 35, loss = 0.39261003\n",
      "Iteration 104, loss = 0.39447508\n",
      "Iteration 49, loss = 0.42432430\n",
      "Iteration 113, loss = 0.50215451Iteration 112, loss = 0.49561262\n",
      "\n",
      "Iteration 54, loss = 0.48521255\n",
      "Iteration 105, loss = 0.39482623Iteration 36, loss = 0.39712589\n",
      "Iteration 71, loss = 0.47304752\n",
      "\n",
      "Iteration 50, loss = 0.38738025\n",
      "Iteration 37, loss = 0.38467945Iteration 51, loss = 0.37893271\n",
      "\n",
      "Iteration 113, loss = 0.49561232\n",
      "Iteration 114, loss = 0.50215701\n",
      "Iteration 55, loss = 0.48521564\n",
      "Iteration 52, loss = 0.38291940Iteration 72, loss = 0.47263968\n",
      "\n",
      "Iteration 97, loss = 0.50329694\n",
      "Iteration 115, loss = 0.50214706Iteration 114, loss = 0.49561269\n",
      "\n",
      "Iteration 56, loss = 0.48518192\n",
      "Iteration 53, loss = 0.37835946\n",
      "Iteration 106, loss = 0.36876054\n",
      "Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 54, loss = 0.40266770\n",
      "Iteration 98, loss = 0.50329728\n",
      "Iteration 116, loss = 0.50215927Iteration 115, loss = 0.49561266\n",
      "\n",
      "Iteration 55, loss = 0.38300863\n",
      "Iteration 38, loss = 0.38875004\n",
      "Iteration 73, loss = 0.47165572\n",
      "Iteration 56, loss = 0.39286228Iteration 39, loss = 0.42869077\n",
      "Iteration 116, loss = 0.49561223Iteration 99, loss = 0.50329778\n",
      "\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 117, loss = 0.50216981\n",
      "Iteration 40, loss = 0.37767719\n",
      "Iteration 74, loss = 0.47142009\n",
      "Iteration 41, loss = 0.38055013Iteration 100, loss = 0.50329693\n",
      "Iteration 118, loss = 0.50216380\n",
      "Iteration 117, loss = 0.49561207\n",
      "\n",
      "Iteration 57, loss = 0.48519330Iteration 75, loss = 0.47556311\n",
      "\n",
      "Iteration 1, loss = 0.74976171\n",
      "Iteration 57, loss = 0.40396627\n",
      "Iteration 101, loss = 0.50329774\n",
      "Iteration 119, loss = 0.50215265\n",
      "Iteration 2, loss = 0.53502452\n",
      "Iteration 118, loss = 0.49561217\n",
      "Iteration 58, loss = 0.38221419Iteration 58, loss = 0.48511385\n",
      "\n",
      "Iteration 102, loss = 0.50329739Iteration 120, loss = 0.50216032\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036\n",
      "\n",
      "Iteration 119, loss = 0.49561201Iteration 76, loss = 0.47342878\n",
      "\n",
      "Iteration 59, loss = 0.48515765\n",
      "Iteration 3, loss = 0.51583583\n",
      "Iteration 121, loss = 0.50215279\n",
      "Iteration 103, loss = 0.50329708\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "Iteration 4, loss = 0.49888713\n",
      "Iteration 60, loss = 0.48515758\n",
      "Iteration 120, loss = 0.49561201\n",
      "Iteration 77, loss = 0.47252515\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.004451\n",
      "Iteration 5, loss = 0.49731323\n",
      "Iteration 122, loss = 0.50215105\n",
      "Iteration 104, loss = 0.50329608\n",
      "Iteration 61, loss = 0.48510053\n",
      "Iteration 121, loss = 0.49561224\n",
      "Iteration 78, loss = 0.47037010\n",
      "Iteration 6, loss = 0.49079193\n",
      "Iteration 59, loss = 0.38895892\n",
      "Iteration 123, loss = 0.50215129\n",
      "Iteration 105, loss = 0.50329569\n",
      "Iteration 62, loss = 0.48507793\n",
      "Iteration 122, loss = 0.49561200\n",
      "Iteration 7, loss = 0.49087167\n",
      "Iteration 60, loss = 0.38261178\n",
      "Iteration 79, loss = 0.47040893\n",
      "Iteration 61, loss = 0.36502774\n",
      "Iteration 8, loss = 0.48381382\n",
      "Iteration 124, loss = 0.50215093\n",
      "Iteration 106, loss = 0.50329568\n",
      "\n",
      "Iteration 123, loss = 0.49561199\n",
      "Iteration 62, loss = 0.38406650\n",
      "Iteration 42, loss = 0.37610861\n",
      "Iteration 125, loss = 0.50215062\n",
      "Iteration 107, loss = 0.50329580\n",
      "Iteration 9, loss = 0.48865443\n",
      "Iteration 124, loss = 0.49561199\n",
      "Iteration 43, loss = 0.39533345\n",
      "Iteration 10, loss = 0.49107703\n",
      "Iteration 44, loss = 0.39021025\n",
      "Iteration 126, loss = 0.50214963\n",
      "Iteration 108, loss = 0.50329549\n",
      "Iteration 125, loss = 0.49561201\n",
      "Iteration 11, loss = 0.48009104\n",
      "Iteration 45, loss = 0.35669289\n",
      "Iteration 12, loss = 0.47366401\n",
      "Iteration 63, loss = 0.41045193\n",
      "Iteration 46, loss = 0.36301974\n",
      "Iteration 127, loss = 0.50214858\n",
      "Iteration 126, loss = 0.49561192\n",
      "Iteration 64, loss = 0.39890273\n",
      "Iteration 13, loss = 0.47371703\n",
      "Iteration 109, loss = 0.50329580\n",
      "Iteration 47, loss = 0.36232568\n",
      "Iteration 65, loss = 0.36531550\n",
      "Iteration 14, loss = 0.48130934\n",
      "Iteration 128, loss = 0.50215103\n",
      "Iteration 48, loss = 0.36102957\n",
      "Iteration 66, loss = 0.38676764\n",
      "Iteration 127, loss = 0.49561198\n",
      "Iteration 80, loss = 0.47024487\n",
      "Iteration 110, loss = 0.50329565\n",
      "Iteration 15, loss = 0.48435651\n",
      "Iteration 49, loss = 0.37765357\n",
      "Iteration 67, loss = 0.35544926\n",
      "Iteration 129, loss = 0.50214916\n",
      "Iteration 16, loss = 0.46189884\n",
      "Iteration 50, loss = 0.35130098\n",
      "Iteration 68, loss = 0.36781256\n",
      "Iteration 128, loss = 0.49561192\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 111, loss = 0.50329612\n",
      "Iteration 69, loss = 0.37355335Iteration 51, loss = 0.34934812\n",
      "\n",
      "Iteration 81, loss = 0.47065076\n",
      "Iteration 130, loss = 0.50215137\n",
      "Iteration 52, loss = 0.34704628Iteration 129, loss = 0.49561187\n",
      "\n",
      "Iteration 17, loss = 0.46387434\n",
      "Iteration 112, loss = 0.50329556\n",
      "Iteration 53, loss = 0.33701669\n",
      "Iteration 131, loss = 0.50215811\n",
      "Iteration 18, loss = 0.45615977\n",
      "Iteration 130, loss = 0.49561188\n",
      "Iteration 54, loss = 0.35841826\n",
      "Iteration 113, loss = 0.50329527\n",
      "Iteration 63, loss = 0.48513583\n",
      "Iteration 82, loss = 0.47030549\n",
      "Iteration 19, loss = 0.46025424\n",
      "Iteration 132, loss = 0.50214821\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "Iteration 131, loss = 0.49561190\n",
      "Iteration 20, loss = 0.46594966\n",
      "Iteration 64, loss = 0.48508153\n",
      "Iteration 114, loss = 0.50329579\n",
      "Iteration 83, loss = 0.47022370\n",
      "Iteration 21, loss = 0.45668264\n",
      "Iteration 133, loss = 0.50214705\n",
      "Iteration 132, loss = 0.49561189\n",
      "Iteration 22, loss = 0.44279944\n",
      "Iteration 70, loss = 0.37684158\n",
      "Iteration 65, loss = 0.48517250\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178\n",
      "Iteration 115, loss = 0.50329614\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 84, loss = 0.47054171\n",
      "Iteration 134, loss = 0.50214717\n",
      "Iteration 71, loss = 0.37044630\n",
      "Iteration 23, loss = 0.44115597\n",
      "Iteration 133, loss = 0.49561188\n",
      "Iteration 72, loss = 0.36449493\n",
      "Iteration 66, loss = 0.48504351\n",
      "Iteration 116, loss = 0.50329499\n",
      "Iteration 135, loss = 0.50214721\n",
      "Iteration 134, loss = 0.49561190\n",
      "Iteration 24, loss = 0.43973925\n",
      "Iteration 67, loss = 0.48499799\n",
      "Iteration 117, loss = 0.50329502\n",
      "Iteration 25, loss = 0.45266055\n",
      "Iteration 136, loss = 0.50214686\n",
      "Iteration 85, loss = 0.47058488\n",
      "Iteration 135, loss = 0.49561190\n",
      "Iteration 26, loss = 0.45895140\n",
      "Iteration 118, loss = 0.50329511\n",
      "Iteration 137, loss = 0.50214673\n",
      "Iteration 55, loss = 0.34424446\n",
      "Iteration 136, loss = 0.49561188\n",
      "Iteration 86, loss = 0.47045054\n",
      "Iteration 27, loss = 0.44036017\n",
      "Iteration 56, loss = 0.32704026\n",
      "Iteration 28, loss = 0.42123498\n",
      "Iteration 119, loss = 0.50329497\n",
      "Iteration 138, loss = 0.50214695\n",
      "Iteration 137, loss = 0.49561189\n",
      "Iteration 57, loss = 0.39177003\n",
      "Iteration 29, loss = 0.42388299\n",
      "Iteration 87, loss = 0.47015826\n",
      "Iteration 58, loss = 0.34222690\n",
      "Iteration 30, loss = 0.41762891\n",
      "Iteration 120, loss = 0.50329502\n",
      "Iteration 139, loss = 0.50214658\n",
      "Iteration 138, loss = 0.49561186\n",
      "Iteration 59, loss = 0.33182080\n",
      "Iteration 73, loss = 0.36689759\n",
      "Iteration 88, loss = 0.47011603\n",
      "Iteration 74, loss = 0.36294936Iteration 60, loss = 0.34088891\n",
      "\n",
      "Iteration 121, loss = 0.50329520Iteration 140, loss = 0.50214734\n",
      "\n",
      "Iteration 61, loss = 0.32916960\n",
      "Iteration 75, loss = 0.37712442\n",
      "Iteration 62, loss = 0.32550283\n",
      "Iteration 141, loss = 0.50214745\n",
      "Iteration 76, loss = 0.36344992Iteration 31, loss = 0.43521132\n",
      "\n",
      "Iteration 122, loss = 0.50329494\n",
      "Iteration 63, loss = 0.32380534\n",
      "Iteration 89, loss = 0.47005126\n",
      "Iteration 77, loss = 0.36489958Iteration 68, loss = 0.48500196\n",
      "Iteration 32, loss = 0.45156166\n",
      "Iteration 142, loss = 0.50214709Iteration 139, loss = 0.49561190\n",
      "\n",
      "Iteration 123, loss = 0.50329492\n",
      "Iteration 33, loss = 0.41417542\n",
      "\n",
      "Iteration 90, loss = 0.47009741\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000890\n",
      "Iteration 69, loss = 0.48501469\n",
      "Iteration 34, loss = 0.41485617Iteration 78, loss = 0.35533882\n",
      "\n",
      "Iteration 140, loss = 0.49561187Iteration 143, loss = 0.50214694\n",
      "Iteration 124, loss = 0.50329492\n",
      "Iteration 79, loss = 0.34337019Iteration 35, loss = 0.40270017\n",
      "Iteration 70, loss = 0.48499944Iteration 91, loss = 0.46992496\n",
      "\n",
      "\n",
      "Iteration 144, loss = 0.50214700\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 36, loss = 0.41477774\n",
      "Iteration 125, loss = 0.50329491\n",
      "Iteration 80, loss = 0.35557094\n",
      "\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.Iteration 71, loss = 0.48499313\n",
      "Iteration 92, loss = 0.47006359\n",
      "Iteration 37, loss = 0.42314718\n",
      "Iteration 145, loss = 0.50214649\n",
      "\n",
      "Iteration 126, loss = 0.50329493\n",
      "Iteration 72, loss = 0.48498452Iteration 81, loss = 0.34650792\n",
      "Iteration 146, loss = 0.50214638\n",
      "Iteration 64, loss = 0.36171723\n",
      "Iteration 82, loss = 0.34267035\n",
      "Iteration 127, loss = 0.50329484\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 65, loss = 0.32054557\n",
      "Iteration 38, loss = 0.42279364Iteration 83, loss = 0.34937672\n",
      "\n",
      "Iteration 66, loss = 0.30496371\n",
      "Iteration 84, loss = 0.33904949\n",
      "Iteration 93, loss = 0.46994478\n",
      "Iteration 147, loss = 0.50214643\n",
      "Iteration 39, loss = 0.41072799\n",
      "Iteration 128, loss = 0.50329482\n",
      "Iteration 85, loss = 0.35517413\n",
      "Iteration 40, loss = 0.39438519\n",
      "Iteration 73, loss = 0.48499398Iteration 86, loss = 0.35729622\n",
      "\n",
      "Iteration 94, loss = 0.46990066Iteration 148, loss = 0.50214638\n",
      "\n",
      "Iteration 129, loss = 0.50329484\n",
      "Iteration 41, loss = 0.39115888\n",
      "Iteration 87, loss = 0.36104449\n",
      "Iteration 67, loss = 0.32954823\n",
      "Iteration 42, loss = 0.38635346\n",
      "Iteration 149, loss = 0.50214639\n",
      "Iteration 95, loss = 0.46989672\n",
      "Iteration 130, loss = 0.50329484\n",
      "Iteration 43, loss = 0.39716330\n",
      "Iteration 74, loss = 0.48498130\n",
      "Iteration 150, loss = 0.50214635\n",
      "Iteration 68, loss = 0.31392603\n",
      "Iteration 131, loss = 0.50329484\n",
      "Iteration 151, loss = 0.50214635\n",
      "Iteration 75, loss = 0.48498694\n",
      "Iteration 132, loss = 0.50329483\n",
      "Iteration 44, loss = 0.42382437\n",
      "Iteration 69, loss = 0.31760662\n",
      "Iteration 152, loss = 0.50214646\n",
      "Iteration 45, loss = 0.39257699\n",
      "Iteration 133, loss = 0.50329484\n",
      "Iteration 70, loss = 0.34078200\n",
      "Iteration 46, loss = 0.37620872\n",
      "Iteration 88, loss = 0.34440854\n",
      "Iteration 71, loss = 0.29846209\n",
      "Iteration 153, loss = 0.50214619\n",
      "Iteration 47, loss = 0.37186339Iteration 134, loss = 0.50329488\n",
      "\n",
      "Iteration 89, loss = 0.37152400Iteration 96, loss = 0.46990516Iteration 72, loss = 0.29806394\n",
      "\n",
      "Iteration 48, loss = 0.37538129\n",
      "Iteration 73, loss = 0.31577151\n",
      "Iteration 135, loss = 0.50329482\n",
      "\n",
      "Iteration 97, loss = 0.46985960\n",
      "Iteration 154, loss = 0.50214619\n",
      "Iteration 49, loss = 0.37384337\n",
      "Iteration 74, loss = 0.29590672\n",
      "\n",
      "Iteration 90, loss = 0.33820639\n",
      "Iteration 75, loss = 0.29740673\n",
      "Iteration 155, loss = 0.50214616\n",
      "Iteration 91, loss = 0.35406636\n",
      "Iteration 76, loss = 0.33155399\n",
      "Iteration 76, loss = 0.48497877\n",
      "Iteration 98, loss = 0.46988829\n",
      "Iteration 92, loss = 0.36918081Iteration 77, loss = 0.29796100\n",
      "\n",
      "Iteration 78, loss = 0.32588263\n",
      "Iteration 99, loss = 0.46994082\n",
      "Iteration 136, loss = 0.50329482\n",
      "Iteration 93, loss = 0.33359988\n",
      "Iteration 79, loss = 0.27886783\n",
      "Iteration 156, loss = 0.50214614\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 94, loss = 0.34530880Iteration 100, loss = 0.46988224\n",
      "Iteration 80, loss = 0.28799458\n",
      "Iteration 77, loss = 0.48498323\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036\n",
      "Iteration 81, loss = 0.28296655\n",
      "Iteration 50, loss = 0.39928700\n",
      "Iteration 157, loss = 0.50214608\n",
      "Iteration 82, loss = 0.29246136\n",
      "Iteration 51, loss = 0.36703484\n",
      "Iteration 101, loss = 0.46985811\n",
      "Iteration 78, loss = 0.48497553Iteration 83, loss = 0.30307019\n",
      "\n",
      "Iteration 158, loss = 0.50214606\n",
      "Iteration 137, loss = 0.50329481\n",
      "Iteration 52, loss = 0.36871684\n",
      "Iteration 95, loss = 0.35427315\n",
      "Iteration 84, loss = 0.30557384\n",
      "Iteration 96, loss = 0.32837974\n",
      "Iteration 85, loss = 0.31088662Iteration 159, loss = 0.50214608\n",
      "\n",
      "\n",
      "Iteration 102, loss = 0.46995654\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000178\n",
      "Iteration 97, loss = 0.35893713Iteration 86, loss = 0.34458647\n",
      "\n",
      "Iteration 160, loss = 0.50214604\n",
      "Iteration 79, loss = 0.48497530\n",
      "Iteration 53, loss = 0.38070304\n",
      "Iteration 87, loss = 0.28944690\n",
      "Iteration 103, loss = 0.46992190\n",
      "Iteration 98, loss = 0.34090392\n",
      "Iteration 54, loss = 0.35535038\n",
      "Iteration 88, loss = 0.31394561\n",
      "Iteration 161, loss = 0.50214605\n",
      "Iteration 138, loss = 0.50329481\n",
      "Iteration 99, loss = 0.34999786\n",
      "Iteration 55, loss = 0.34871934\n",
      "Iteration 89, loss = 0.30232869\n",
      "Iteration 104, loss = 0.46982945\n",
      "Iteration 100, loss = 0.32700628\n",
      "Iteration 56, loss = 0.36296866\n",
      "Iteration 90, loss = 0.28796235\n",
      "Iteration 162, loss = 0.50214606\n",
      "Iteration 101, loss = 0.34386537\n",
      "Iteration 139, loss = 0.50329482\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 57, loss = 0.37083090\n",
      "Iteration 105, loss = 0.46982615\n",
      "Iteration 91, loss = 0.27635045\n",
      "Iteration 102, loss = 0.35221696\n",
      "Iteration 80, loss = 0.48497509\n",
      "Iteration 58, loss = 0.34186995\n",
      "Iteration 92, loss = 0.26879039\n",
      "Iteration 103, loss = 0.32817030\n",
      "Iteration 163, loss = 0.50214606\n",
      "Iteration 93, loss = 0.28654388\n",
      "Iteration 104, loss = 0.32797897\n",
      "Iteration 94, loss = 0.33261430\n",
      "Iteration 164, loss = 0.50214606\n",
      "Iteration 106, loss = 0.46982423\n",
      "Iteration 81, loss = 0.48497349\n",
      "Iteration 105, loss = 0.33176507\n",
      "Iteration 95, loss = 0.28727633\n",
      "Iteration 106, loss = 0.34247016\n",
      "Iteration 165, loss = 0.50214606Iteration 59, loss = 0.34714057\n",
      "\n",
      "Iteration 96, loss = 0.33501511\n",
      "Iteration 107, loss = 0.46982366\n",
      "Iteration 107, loss = 0.34641341\n",
      "Iteration 97, loss = 0.27397874\n",
      "Iteration 60, loss = 0.37016310\n",
      "Iteration 108, loss = 0.34518794\n",
      "Iteration 98, loss = 0.27632523\n",
      "Iteration 166, loss = 0.50214605\n",
      "Iteration 61, loss = 0.34940238\n",
      "Iteration 109, loss = 0.36461194\n",
      "Iteration 108, loss = 0.46983873\n",
      "Iteration 99, loss = 0.30067443\n",
      "Iteration 110, loss = 0.30686436\n",
      "Iteration 62, loss = 0.33762920\n",
      "Iteration 167, loss = 0.50214608\n",
      "Iteration 100, loss = 0.27227143\n",
      "Iteration 111, loss = 0.33220995\n",
      "Iteration 63, loss = 0.36370690\n",
      "Iteration 101, loss = 0.28302133\n",
      "Iteration 109, loss = 0.46981906\n",
      "Iteration 112, loss = 0.31882526\n",
      "Iteration 64, loss = 0.35586671\n",
      "Iteration 168, loss = 0.50214606\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 102, loss = 0.26615013\n",
      "Iteration 113, loss = 0.32818101\n",
      "Iteration 114, loss = 0.33814504\n",
      "Iteration 103, loss = 0.25686770\n",
      "Iteration 110, loss = 0.46982182\n",
      "Iteration 104, loss = 0.35065112\n",
      "Iteration 105, loss = 0.27337143\n",
      "Iteration 82, loss = 0.48497404\n",
      "Iteration 65, loss = 0.33010006\n",
      "Iteration 111, loss = 0.46982454\n",
      "Iteration 106, loss = 0.27940691\n",
      "Iteration 115, loss = 0.33138143\n",
      "Iteration 66, loss = 0.36189866\n",
      "Iteration 83, loss = 0.48497671\n",
      "Iteration 107, loss = 0.28914644\n",
      "Iteration 116, loss = 0.32714286\n",
      "Iteration 67, loss = 0.35872969\n",
      "Iteration 112, loss = 0.46982587\n",
      "Iteration 108, loss = 0.26431523\n",
      "Iteration 117, loss = 0.37014960\n",
      "Iteration 84, loss = 0.48497294\n",
      "Iteration 68, loss = 0.32833922\n",
      "Iteration 109, loss = 0.27870384\n",
      "Iteration 118, loss = 0.31742134\n",
      "Iteration 113, loss = 0.46983705\n",
      "Iteration 69, loss = 0.32696233\n",
      "Iteration 110, loss = 0.30799118\n",
      "Iteration 85, loss = 0.48497406\n",
      "Iteration 119, loss = 0.32347676\n",
      "Iteration 70, loss = 0.35433975\n",
      "Iteration 111, loss = 0.32929804\n",
      "Iteration 112, loss = 0.27519460\n",
      "Iteration 86, loss = 0.48497356\n",
      "Iteration 114, loss = 0.46982431\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000036\n",
      "Iteration 113, loss = 0.25961421Iteration 71, loss = 0.33462732\n",
      "\n",
      "Iteration 87, loss = 0.48497220\n",
      "Iteration 72, loss = 0.31748713Iteration 114, loss = 0.30141013\n",
      "\n",
      "Iteration 120, loss = 0.30918713\n",
      "Iteration 115, loss = 0.46981306\n",
      "Iteration 115, loss = 0.27760894\n",
      "Iteration 121, loss = 0.34434825\n",
      "Iteration 88, loss = 0.48497273\n",
      "Iteration 116, loss = 0.25504439\n",
      "Iteration 122, loss = 0.31106328\n",
      "Iteration 73, loss = 0.35812252\n",
      "Iteration 116, loss = 0.46981255\n",
      "Iteration 117, loss = 0.26588425\n",
      "Iteration 123, loss = 0.30419568Iteration 89, loss = 0.48497008\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "\n",
      "Iteration 74, loss = 0.34124735\n",
      "Iteration 118, loss = 0.26657029\n",
      "Iteration 117, loss = 0.46981470\n",
      "Iteration 124, loss = 0.34224819\n",
      "Iteration 75, loss = 0.32808001\n",
      "Iteration 119, loss = 0.30176893\n",
      "Iteration 90, loss = 0.48496915\n",
      "Iteration 125, loss = 0.33148402\n",
      "Iteration 76, loss = 0.33655900\n",
      "Iteration 120, loss = 0.27960192Iteration 118, loss = 0.46981302\n",
      "\n",
      "Iteration 126, loss = 0.36035554\n",
      "Iteration 91, loss = 0.48496886\n",
      "Iteration 121, loss = 0.26435430\n",
      "Iteration 127, loss = 0.30786403\n",
      "Iteration 122, loss = 0.25231228\n",
      "Iteration 92, loss = 0.48496886Iteration 128, loss = 0.31070110\n",
      "\n",
      "Iteration 77, loss = 0.35206878\n",
      "Iteration 123, loss = 0.24984444\n",
      "Iteration 129, loss = 0.31898389\n",
      "Iteration 78, loss = 0.30482050Iteration 124, loss = 0.29817149\n",
      "\n",
      "Iteration 93, loss = 0.48496858\n",
      "Iteration 130, loss = 0.31554082\n",
      "Iteration 125, loss = 0.27334052\n",
      "Iteration 79, loss = 0.32350639\n",
      "Iteration 131, loss = 0.31875886\n",
      "Iteration 126, loss = 0.31057026\n",
      "Iteration 94, loss = 0.48496856\n",
      "Iteration 80, loss = 0.34201895\n",
      "Iteration 132, loss = 0.33457644\n",
      "Iteration 127, loss = 0.25070028\n",
      "Iteration 81, loss = 0.32591039\n",
      "Iteration 119, loss = 0.46981293\n",
      "Iteration 133, loss = 0.35058376\n",
      "Iteration 95, loss = 0.48496884\n",
      "Iteration 128, loss = 0.24467373\n",
      "Iteration 82, loss = 0.31617312\n",
      "Iteration 129, loss = 0.28343075\n",
      "Iteration 120, loss = 0.46981420\n",
      "Iteration 134, loss = 0.29820683\n",
      "Iteration 96, loss = 0.48496824\n",
      "Iteration 130, loss = 0.25703059\n",
      "Iteration 131, loss = 0.26335625\n",
      "Iteration 135, loss = 0.33639936\n",
      "Iteration 97, loss = 0.48496831Iteration 121, loss = 0.46981559\n",
      "\n",
      "Iteration 132, loss = 0.25012037\n",
      "Iteration 122, loss = 0.46981803\n",
      "Iteration 98, loss = 0.48496835\n",
      "Iteration 133, loss = 0.24597970\n",
      "Iteration 136, loss = 0.30060096\n",
      "Iteration 134, loss = 0.34341579\n",
      "Iteration 123, loss = 0.46981249\n",
      "Iteration 99, loss = 0.48496846\n",
      "Iteration 135, loss = 0.27657490\n",
      "Iteration 83, loss = 0.30323933\n",
      "Iteration 136, loss = 0.24736961\n",
      "Iteration 100, loss = 0.48496863\n",
      "Iteration 84, loss = 0.34092636\n",
      "Iteration 137, loss = 0.23772566\n",
      "Iteration 124, loss = 0.46981247\n",
      "Iteration 101, loss = 0.48496779\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 138, loss = 0.26795770\n",
      "Iteration 139, loss = 0.27325161\n",
      "Iteration 125, loss = 0.46981501\n",
      "Iteration 140, loss = 0.26669124\n",
      "Iteration 141, loss = 0.24512212\n",
      "Iteration 85, loss = 0.33572832\n",
      "Iteration 137, loss = 0.33587449Iteration 126, loss = 0.46981323\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000007\n",
      "Iteration 142, loss = 0.24234472\n",
      "Iteration 86, loss = 0.30225545\n",
      "Iteration 143, loss = 0.30093338\n",
      "Iteration 102, loss = 0.48496766\n",
      "Iteration 144, loss = 0.26800341\n",
      "Iteration 127, loss = 0.46980835\n",
      "Iteration 103, loss = 0.48496755Iteration 145, loss = 0.24354498\n",
      "\n",
      "Iteration 146, loss = 0.23510502\n",
      "\n",
      "Iteration 104, loss = 0.48496749\n",
      "Iteration 147, loss = 0.31557963\n",
      "Iteration 128, loss = 0.46981138Iteration 138, loss = 0.29888846\n",
      "\n",
      "Iteration 148, loss = 0.24852377\n",
      "Iteration 139, loss = 0.29491196\n",
      "Iteration 105, loss = 0.48496759\n",
      "Iteration 149, loss = 0.23907882\n",
      "Iteration 129, loss = 0.46981170\n",
      "Iteration 140, loss = 0.31219381\n",
      "Iteration 150, loss = 0.26394863\n",
      "Iteration 141, loss = 0.33456287\n",
      "Iteration 151, loss = 0.24212162Iteration 130, loss = 0.46981170\n",
      "\n",
      "Iteration 106, loss = 0.48496758\n",
      "Iteration 142, loss = 0.31828634\n",
      "Iteration 152, loss = 0.24050260\n",
      "Iteration 107, loss = 0.48496751\n",
      "Iteration 143, loss = 0.30429678\n",
      "Iteration 153, loss = 0.24879717\n",
      "Iteration 131, loss = 0.46981143\n",
      "Iteration 154, loss = 0.23952691\n",
      "Iteration 144, loss = 0.28987629\n",
      "Iteration 87, loss = 0.30737520\n",
      "Iteration 108, loss = 0.48496743\n",
      "Iteration 155, loss = 0.30074814\n",
      "Iteration 145, loss = 0.32362488\n",
      "Iteration 156, loss = 0.25479920\n",
      "Iteration 132, loss = 0.46981197\n",
      "Iteration 109, loss = 0.48496742Iteration 146, loss = 0.32977218\n",
      "\n",
      "Iteration 157, loss = 0.28352334\n",
      "Iteration 88, loss = 0.33120372Iteration 147, loss = 0.30845689\n",
      "\n",
      "Iteration 133, loss = 0.46981143\n",
      "Iteration 158, loss = 0.24837735\n",
      "Iteration 110, loss = 0.48496748\n",
      "Iteration 148, loss = 0.30644754\n",
      "Iteration 159, loss = 0.23566517\n",
      "Iteration 160, loss = 0.26999349\n",
      "Iteration 89, loss = 0.32877443Iteration 149, loss = 0.35810052\n",
      "\n",
      "Iteration 161, loss = 0.23704999\n",
      "Iteration 111, loss = 0.48496739Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 134, loss = 0.46981104\n",
      "Iteration 150, loss = 0.31982081\n",
      "Iteration 90, loss = 0.29609469\n",
      "Iteration 151, loss = 0.32391222Iteration 91, loss = 0.30247890\n",
      "\n",
      "Iteration 112, loss = 0.48496739\n",
      "Iteration 152, loss = 0.30571665\n",
      "Iteration 92, loss = 0.36456529\n",
      "Iteration 153, loss = 0.29742160\n",
      "Iteration 113, loss = 0.48496740\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 93, loss = 0.30735075\n",
      "Iteration 154, loss = 0.30255947\n",
      "Iteration 114, loss = 0.48496733\n",
      "Iteration 155, loss = 0.29518967\n",
      "Iteration 94, loss = 0.29917836\n",
      "Iteration 156, loss = 0.30251061\n",
      "Iteration 95, loss = 0.30981095Iteration 115, loss = 0.48496733\n",
      "\n",
      "Iteration 157, loss = 0.30514219\n",
      "Iteration 96, loss = 0.35684558\n",
      "Iteration 158, loss = 0.32063530\n",
      "Iteration 116, loss = 0.48496735\n",
      "Iteration 135, loss = 0.46981142\n",
      "Iteration 97, loss = 0.30253061\n",
      "Iteration 159, loss = 0.29195771\n",
      "Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "Iteration 136, loss = 0.46981125\n",
      "Iteration 98, loss = 0.30562756\n",
      "Iteration 99, loss = 0.31896026\n",
      "Iteration 137, loss = 0.46981119\n",
      "Iteration 1, loss = 0.63598492\n",
      "Iteration 2, loss = 0.52551994\n",
      "Iteration 3, loss = 0.49669535\n",
      "Iteration 117, loss = 0.48496733\n",
      "Iteration 100, loss = 0.31981303\n",
      "Iteration 4, loss = 0.48664392\n",
      "Iteration 101, loss = 0.28701606\n",
      "Iteration 118, loss = 0.48496732\n",
      "Iteration 5, loss = 0.48581399\n",
      "Iteration 102, loss = 0.31413000\n",
      "Iteration 6, loss = 0.47301962\n",
      "Iteration 103, loss = 0.31570617\n",
      "Iteration 119, loss = 0.48496733\n",
      "Iteration 7, loss = 0.47255984\n",
      "Iteration 138, loss = 0.46981128\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 8, loss = 0.48108564\n",
      "Iteration 104, loss = 0.29950428\n",
      "Iteration 120, loss = 0.48496741\n",
      "Iteration 139, loss = 0.46981085\n",
      "Iteration 9, loss = 0.46566398\n",
      "Iteration 121, loss = 0.48496731\n",
      "Iteration 140, loss = 0.46981096\n",
      "Iteration 10, loss = 0.44960062\n",
      "Iteration 122, loss = 0.48496730\n",
      "Iteration 11, loss = 0.45233124\n",
      "Iteration 123, loss = 0.48496731\n",
      "Iteration 12, loss = 0.44727207\n",
      "Iteration 13, loss = 0.44565042\n",
      "Iteration 124, loss = 0.48496732\n",
      "Iteration 14, loss = 0.44404560\n",
      "Iteration 141, loss = 0.46981071\n",
      "Iteration 15, loss = 0.44514598\n",
      "Iteration 105, loss = 0.29917980\n",
      "Iteration 125, loss = 0.48496730\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 16, loss = 0.43533823Iteration 142, loss = 0.46981075\n",
      "\n",
      "Iteration 106, loss = 0.32184855Iteration 17, loss = 0.42858055\n",
      "Iteration 143, loss = 0.46981083\n",
      "\n",
      "Iteration 18, loss = 0.42888061\n",
      "Iteration 107, loss = 0.29293332\n",
      "Iteration 144, loss = 0.46981071\n",
      "Iteration 108, loss = 0.28694035\n",
      "Iteration 109, loss = 0.33113478Iteration 145, loss = 0.46981077\n",
      "\n",
      "Iteration 146, loss = 0.46981074\n",
      "Iteration 110, loss = 0.31860091\n",
      "Iteration 111, loss = 0.30510548\n",
      "Iteration 147, loss = 0.46981082\n",
      "Iteration 112, loss = 0.32091899\n",
      "Iteration 19, loss = 0.41863799\n",
      "Iteration 113, loss = 0.31857310Iteration 148, loss = 0.46981074\n",
      "\n",
      "Iteration 20, loss = 0.41798521\n",
      "Iteration 114, loss = 0.31028844\n",
      "Iteration 21, loss = 0.41352991Iteration 149, loss = 0.46981071\n",
      "Iteration 115, loss = 0.28540580\n",
      "\n",
      "Iteration 116, loss = 0.34184533\n",
      "Iteration 22, loss = 0.41344974\n",
      "Iteration 150, loss = 0.46981071\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 151, loss = 0.46981068\n",
      "Iteration 23, loss = 0.41226594Iteration 117, loss = 0.28888101\n",
      "\n",
      "Iteration 24, loss = 0.41085743Iteration 118, loss = 0.28420411\n",
      "\n",
      "Iteration 152, loss = 0.46981069\n",
      "Iteration 119, loss = 0.30799292Iteration 25, loss = 0.40263139\n",
      "\n",
      "Iteration 26, loss = 0.39553724Iteration 153, loss = 0.46981069\n",
      "Iteration 120, loss = 0.31950190\n",
      "\n",
      "Iteration 121, loss = 0.27833345Iteration 27, loss = 0.39575634\n",
      "\n",
      "Iteration 154, loss = 0.46981069\n",
      "Iteration 28, loss = 0.38953538\n",
      "Iteration 122, loss = 0.32365614\n",
      "Iteration 155, loss = 0.46981069\n",
      "Iteration 29, loss = 0.39230805\n",
      "Iteration 123, loss = 0.30856752\n",
      "Iteration 156, loss = 0.46981069\n",
      "Iteration 30, loss = 0.39178570Iteration 124, loss = 0.29007905\n",
      "\n",
      "Iteration 1, loss = 0.70640649\n",
      "Iteration 31, loss = 0.39762533Iteration 125, loss = 0.33411023\n",
      "\n",
      "Iteration 2, loss = 0.63073475\n",
      "Iteration 157, loss = 0.46981070\n",
      "Iteration 126, loss = 0.33470478\n",
      "Iteration 32, loss = 0.39257683\n",
      "Iteration 3, loss = 0.58771704\n",
      "Iteration 127, loss = 0.35603458\n",
      "Iteration 33, loss = 0.39018235\n",
      "Iteration 4, loss = 0.58141651\n",
      "Iteration 158, loss = 0.46981069\n",
      "Iteration 34, loss = 0.38953709Iteration 128, loss = 0.32745007\n",
      "\n",
      "Iteration 5, loss = 0.57046115\n",
      "Iteration 129, loss = 0.26795870\n",
      "Iteration 35, loss = 0.36848854\n",
      "Iteration 159, loss = 0.46981073Iteration 6, loss = 0.54733442\n",
      "\n",
      "Iteration 130, loss = 0.27939937\n",
      "Iteration 36, loss = 0.38574412\n",
      "Iteration 7, loss = 0.51620404\n",
      "Iteration 37, loss = 0.36799856Iteration 160, loss = 0.46981068\n",
      "Iteration 8, loss = 0.49619374\n",
      "\n",
      "Iteration 131, loss = 0.30641882Iteration 161, loss = 0.46981068\n",
      "Iteration 38, loss = 0.38207900\n",
      "Iteration 9, loss = 0.49580142\n",
      "\n",
      "Iteration 10, loss = 0.49804726\n",
      "Iteration 132, loss = 0.27055579\n",
      "Iteration 39, loss = 0.36508582\n",
      "Iteration 162, loss = 0.46981069\n",
      "Training loss did not improve more than tol=0.000514 for 11 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 11, loss = 0.49442274\n",
      "Iteration 1, loss = 0.65662322Iteration 133, loss = 0.29018253Iteration 40, loss = 0.37121503\n",
      "\n",
      "Iteration 12, loss = 0.49161945\n",
      "Iteration 134, loss = 0.31114749Iteration 41, loss = 0.36809997\n",
      "\n",
      "Iteration 13, loss = 0.49254927\n",
      "Iteration 135, loss = 0.27355061\n",
      "Iteration 14, loss = 0.49299084\n",
      "Iteration 15, loss = 0.49404289\n",
      "Iteration 136, loss = 0.28049205\n",
      "Iteration 1, loss = 0.67174799\n",
      "Iteration 16, loss = 0.49157948\n",
      "Iteration 137, loss = 0.32280350\n",
      "Iteration 2, loss = 0.61148627\n",
      "\n",
      "Iteration 17, loss = 0.49623635\n",
      "Iteration 2, loss = 0.60727808\n",
      "Iteration 3, loss = 0.58976605\n",
      "Iteration 138, loss = 0.27812339\n",
      "Iteration 3, loss = 0.58030327Iteration 18, loss = 0.49858721\n",
      "Iteration 139, loss = 0.27769546Iteration 42, loss = 0.35899714\n",
      "\n",
      "Iteration 4, loss = 0.59933467\n",
      "Iteration 19, loss = 0.49048433\n",
      "Iteration 140, loss = 0.29146563\n",
      "Iteration 5, loss = 0.56857088\n",
      "Iteration 43, loss = 0.36875338\n",
      "Iteration 20, loss = 0.48491641\n",
      "Iteration 141, loss = 0.28042336Iteration 6, loss = 0.53163126\n",
      "\n",
      "Iteration 44, loss = 0.36397413\n",
      "\n",
      "Iteration 21, loss = 0.49211788\n",
      "Iteration 142, loss = 0.30643511\n",
      "Iteration 45, loss = 0.36368714\n",
      "Iteration 7, loss = 0.51579557\n",
      "Iteration 4, loss = 0.58442522\n",
      "Iteration 22, loss = 0.49714355\n",
      "Iteration 5, loss = 0.57444811\n",
      "Iteration 46, loss = 0.35315189\n",
      "Iteration 8, loss = 0.51455158\n",
      "Iteration 23, loss = 0.48909293\n",
      "Iteration 6, loss = 0.54292851\n",
      "Iteration 143, loss = 0.36342668\n",
      "Iteration 47, loss = 0.34324335\n",
      "Iteration 9, loss = 0.50815134\n",
      "Iteration 7, loss = 0.52080704Iteration 24, loss = 0.48257391\n",
      "\n",
      "Iteration 48, loss = 0.36784610\n",
      "Iteration 144, loss = 0.28358379\n",
      "Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.49976052\n",
      "Iteration 25, loss = 0.48712451\n",
      "Iteration 8, loss = 0.51140513\n",
      "Iteration 49, loss = 0.34837341\n",
      "Iteration 11, loss = 0.49631797\n",
      "Iteration 26, loss = 0.49312259\n",
      "Iteration 50, loss = 0.34950547\n",
      "Iteration 12, loss = 0.49567924\n",
      "Iteration 27, loss = 0.48757564\n",
      "Iteration 13, loss = 0.49479726\n",
      "Iteration 51, loss = 0.36246657\n",
      "Iteration 28, loss = 0.48182490\n",
      "Iteration 1, loss = 0.67495125\n",
      "Iteration 14, loss = 0.49461177\n",
      "Iteration 52, loss = 0.36330853\n",
      "Iteration 1, loss = 0.71235665\n",
      "Iteration 29, loss = 0.48443776\n",
      "Iteration 2, loss = 0.51222468\n",
      "Iteration 2, loss = 0.62264581\n",
      "Iteration 15, loss = 0.49474405\n",
      "Iteration 53, loss = 0.36109575\n",
      "Iteration 30, loss = 0.48279564\n",
      "Iteration 9, loss = 0.51001629\n",
      "Iteration 16, loss = 0.49568713\n",
      "Iteration 31, loss = 0.48452319\n",
      "Iteration 3, loss = 0.51499465\n",
      "Iteration 54, loss = 0.34088895\n",
      "Iteration 10, loss = 0.51126667\n",
      "Iteration 17, loss = 0.49550379\n",
      "Iteration 32, loss = 0.48281120\n",
      "Iteration 11, loss = 0.51162771\n",
      "Iteration 4, loss = 0.49281722\n",
      "Iteration 55, loss = 0.33695944\n",
      "Iteration 12, loss = 0.50452652\n",
      "Iteration 18, loss = 0.49835040\n",
      "Iteration 33, loss = 0.48237356\n",
      "Iteration 56, loss = 0.33928266\n",
      "Iteration 5, loss = 0.48516675\n",
      "Iteration 13, loss = 0.49995095\n",
      "Iteration 19, loss = 0.49563261\n",
      "Iteration 34, loss = 0.48171957\n",
      "Iteration 57, loss = 0.33354029\n",
      "Iteration 6, loss = 0.48240002\n",
      "Iteration 3, loss = 0.60383204\n",
      "Iteration 20, loss = 0.49147927\n",
      "Iteration 58, loss = 0.34929365Iteration 35, loss = 0.48135955\n",
      "\n",
      "Iteration 7, loss = 0.47353222\n",
      "Iteration 4, loss = 0.64944203\n",
      "Iteration 14, loss = 0.49773488\n",
      "Iteration 59, loss = 0.34666891Iteration 8, loss = 0.46868534\n",
      "\n",
      "Iteration 36, loss = 0.48192773\n",
      "Iteration 15, loss = 0.49775997\n",
      "Iteration 21, loss = 0.49056718\n",
      "Iteration 60, loss = 0.34329154\n",
      "Iteration 9, loss = 0.45824035\n",
      "Iteration 37, loss = 0.48114837\n",
      "Iteration 16, loss = 0.49606092\n",
      "Iteration 17, loss = 0.49548597\n",
      "Iteration 22, loss = 0.49111545\n",
      "Iteration 38, loss = 0.48110642\n",
      "Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.49838851\n",
      "Iteration 10, loss = 0.45810492\n",
      "Iteration 23, loss = 0.48903094\n",
      "Iteration 61, loss = 0.32394527\n",
      "Iteration 19, loss = 0.49737820\n",
      "Iteration 11, loss = 0.46024652\n",
      "Iteration 20, loss = 0.49530843\n",
      "Iteration 24, loss = 0.49245441\n",
      "Iteration 62, loss = 0.32845077\n",
      "Iteration 12, loss = 0.44303295\n",
      "Iteration 5, loss = 0.64594702\n",
      "Iteration 63, loss = 0.32642714\n",
      "Iteration 25, loss = 0.49260706\n",
      "Iteration 13, loss = 0.45249529\n",
      "Iteration 1, loss = 0.75884995\n",
      "Iteration 6, loss = 0.59017232\n",
      "Iteration 64, loss = 0.31824392\n",
      "Iteration 26, loss = 0.49170027\n",
      "Iteration 14, loss = 0.44538454\n",
      "Iteration 7, loss = 0.54474689\n",
      "Iteration 21, loss = 0.49641850\n",
      "Iteration 65, loss = 0.34681761\n",
      "Iteration 27, loss = 0.48788298\n",
      "Iteration 15, loss = 0.44173742\n",
      "Iteration 8, loss = 0.55920751\n",
      "Iteration 22, loss = 0.49621940\n",
      "Iteration 66, loss = 0.34650552\n",
      "Iteration 28, loss = 0.48717283\n",
      "Iteration 16, loss = 0.43209270\n",
      "Iteration 9, loss = 0.57996970\n",
      "Iteration 23, loss = 0.49638019\n",
      "Iteration 67, loss = 0.34520489\n",
      "Iteration 17, loss = 0.43764243\n",
      "Iteration 24, loss = 0.49451716\n",
      "Iteration 10, loss = 0.54709339\n",
      "Iteration 29, loss = 0.51040745\n",
      "Iteration 25, loss = 0.49462607\n",
      "Iteration 11, loss = 0.51927724\n",
      "Iteration 30, loss = 0.50285576\n",
      "Iteration 18, loss = 0.43603443\n",
      "Iteration 12, loss = 0.52502784\n",
      "Iteration 68, loss = 0.34901894\n",
      "Iteration 31, loss = 0.49147878\n",
      "Iteration 19, loss = 0.43997493\n",
      "Iteration 13, loss = 0.53996301\n",
      "Iteration 69, loss = 0.31631726\n",
      "Iteration 32, loss = 0.49923821\n",
      "Iteration 14, loss = 0.53905522\n",
      "Iteration 20, loss = 0.41882807\n",
      "Iteration 70, loss = 0.32087978\n",
      "Iteration 15, loss = 0.52985627\n",
      "Iteration 33, loss = 0.50764582\n",
      "Iteration 21, loss = 0.43189454\n",
      "Iteration 71, loss = 0.32516000\n",
      "Iteration 16, loss = 0.51808064\n",
      "Iteration 34, loss = 0.50015557\n",
      "Iteration 22, loss = 0.41764446\n",
      "Iteration 2, loss = 0.61984041\n",
      "Iteration 17, loss = 0.51691149\n",
      "Iteration 72, loss = 0.32136728\n",
      "Iteration 35, loss = 0.49392826\n",
      "Iteration 23, loss = 0.42848655\n",
      "Iteration 3, loss = 0.62575413\n",
      "Iteration 18, loss = 0.51558007\n",
      "Iteration 73, loss = 0.34414001\n",
      "Iteration 19, loss = 0.51522090\n",
      "Iteration 24, loss = 0.40659082\n",
      "Iteration 36, loss = 0.48931757\n",
      "Iteration 4, loss = 0.64968538\n",
      "Iteration 74, loss = 0.31521311\n",
      "Iteration 20, loss = 0.51574369\n",
      "Iteration 25, loss = 0.40974080\n",
      "Iteration 37, loss = 0.49697641\n",
      "Iteration 5, loss = 0.59520442\n",
      "Iteration 21, loss = 0.51909966\n",
      "Iteration 6, loss = 0.53128644\n",
      "Iteration 22, loss = 0.51870144\n",
      "Iteration 38, loss = 0.49803594\n",
      "Iteration 7, loss = 0.52370233\n",
      "Iteration 23, loss = 0.52014740\n",
      "Iteration 75, loss = 0.31143755\n",
      "Iteration 39, loss = 0.49094583\n",
      "Iteration 24, loss = 0.51347805\n",
      "Iteration 8, loss = 0.53196776\n",
      "Iteration 76, loss = 0.30457228\n",
      "Iteration 26, loss = 0.49669095\n",
      "Iteration 25, loss = 0.51832368\n",
      "Iteration 40, loss = 0.48820224\n",
      "Iteration 9, loss = 0.52649402\n",
      "Iteration 77, loss = 0.30528555\n",
      "Iteration 27, loss = 0.49529739\n",
      "Iteration 26, loss = 0.41847745\n",
      "Iteration 26, loss = 0.51852953\n",
      "Iteration 41, loss = 0.48838919\n",
      "Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "Iteration 78, loss = 0.33006640\n",
      "Iteration 10, loss = 0.50916828\n",
      "Iteration 28, loss = 0.49123472\n",
      "Iteration 27, loss = 0.41645563\n",
      "Iteration 27, loss = 0.51711606\n",
      "Iteration 79, loss = 0.28849894\n",
      "Iteration 11, loss = 0.51409756\n",
      "Iteration 29, loss = 0.49643031\n",
      "Iteration 28, loss = 0.42945011\n",
      "Iteration 80, loss = 0.34226146\n",
      "Iteration 28, loss = 0.51426021\n",
      "Iteration 12, loss = 0.53330655\n",
      "Iteration 29, loss = 0.38357228Iteration 30, loss = 0.50239546\n",
      "\n",
      "Iteration 29, loss = 0.51386107\n",
      "Iteration 13, loss = 0.52761261\n",
      "Iteration 31, loss = 0.50684747\n",
      "Iteration 30, loss = 0.39760239\n",
      "Iteration 1, loss = 0.80692857\n",
      "Iteration 30, loss = 0.51351432\n",
      "Iteration 14, loss = 0.51328071\n",
      "Iteration 32, loss = 0.49976451\n",
      "Iteration 2, loss = 0.64731288\n",
      "Iteration 31, loss = 0.38458508\n",
      "Iteration 31, loss = 0.51355964\n",
      "Iteration 15, loss = 0.50320449\n",
      "Iteration 33, loss = 0.48959862\n",
      "Iteration 32, loss = 0.40053577\n",
      "Iteration 32, loss = 0.51306492\n",
      "Iteration 3, loss = 0.61028490\n",
      "Iteration 16, loss = 0.50385724\n",
      "Iteration 33, loss = 0.41968628\n",
      "Iteration 33, loss = 0.51302213\n",
      "Iteration 4, loss = 0.64509744\n",
      "Iteration 34, loss = 0.38703169\n",
      "Iteration 34, loss = 0.51184146\n",
      "Iteration 17, loss = 0.50487841\n",
      "Iteration 35, loss = 0.51454231\n",
      "Iteration 35, loss = 0.38315698\n",
      "Iteration 5, loss = 0.65093024\n",
      "Iteration 18, loss = 0.50407315\n",
      "Iteration 36, loss = 0.51285485\n",
      "Iteration 36, loss = 0.39528656\n",
      "Iteration 6, loss = 0.60042322\n",
      "Iteration 19, loss = 0.50407143\n",
      "Iteration 37, loss = 0.51459690\n",
      "Iteration 37, loss = 0.37257173\n",
      "Iteration 7, loss = 0.53875876\n",
      "Iteration 38, loss = 0.51707447\n",
      "Iteration 20, loss = 0.50125860\n",
      "Iteration 38, loss = 0.36341996\n",
      "Iteration 39, loss = 0.52219616\n",
      "Iteration 8, loss = 0.53571586\n",
      "Iteration 21, loss = 0.50140379\n",
      "Iteration 39, loss = 0.36920480\n",
      "Iteration 40, loss = 0.53039788\n",
      "Iteration 81, loss = 0.29145971\n",
      "Iteration 9, loss = 0.55705827\n",
      "Iteration 22, loss = 0.50228383\n",
      "Iteration 40, loss = 0.37190444\n",
      "Iteration 41, loss = 0.52783900\n",
      "Iteration 82, loss = 0.29483522\n",
      "Iteration 34, loss = 0.50058280\n",
      "Iteration 10, loss = 0.55356559\n",
      "Iteration 23, loss = 0.50117314\n",
      "Iteration 41, loss = 0.40633005\n",
      "Iteration 42, loss = 0.51966059\n",
      "Iteration 83, loss = 0.28865393\n",
      "Iteration 35, loss = 0.50368345\n",
      "Iteration 42, loss = 0.37540400\n",
      "Iteration 11, loss = 0.53322417\n",
      "Iteration 24, loss = 0.50139031\n",
      "Iteration 43, loss = 0.51514231\n",
      "Iteration 84, loss = 0.31567778\n",
      "Iteration 36, loss = 0.49652301\n",
      "Iteration 43, loss = 0.38842863\n",
      "Iteration 12, loss = 0.51619277\n",
      "Iteration 25, loss = 0.50070676\n",
      "Iteration 44, loss = 0.51139358\n",
      "Iteration 85, loss = 0.32160797\n",
      "Iteration 37, loss = 0.49297214\n",
      "Iteration 44, loss = 0.39023552\n",
      "Iteration 26, loss = 0.49970992\n",
      "Iteration 13, loss = 0.51269464\n",
      "Iteration 45, loss = 0.51947090\n",
      "Iteration 86, loss = 0.30222476\n",
      "Iteration 45, loss = 0.34492576\n",
      "Iteration 38, loss = 0.49994544\n",
      "Iteration 27, loss = 0.50131137\n",
      "Iteration 14, loss = 0.51167011\n",
      "Iteration 46, loss = 0.52058794\n",
      "Iteration 46, loss = 0.33495628\n",
      "Iteration 39, loss = 0.50252767\n",
      "Iteration 28, loss = 0.50094038\n",
      "Iteration 47, loss = 0.52038991\n",
      "Iteration 15, loss = 0.51290405\n",
      "Iteration 40, loss = 0.49527206\n",
      "Iteration 47, loss = 0.33885040\n",
      "Iteration 29, loss = 0.49947119\n",
      "Iteration 48, loss = 0.51906399\n",
      "Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "Iteration 87, loss = 0.33988892\n",
      "Iteration 48, loss = 0.38178040\n",
      "Iteration 41, loss = 0.49281314\n",
      "Iteration 16, loss = 0.51790672\n",
      "Iteration 30, loss = 0.50075893\n",
      "Iteration 1, loss = 0.70384777\n",
      "Iteration 88, loss = 0.30185496\n",
      "Iteration 49, loss = 0.33090811\n",
      "Iteration 17, loss = 0.52161338\n",
      "Iteration 31, loss = 0.50093245\n",
      "Iteration 2, loss = 0.62157659\n",
      "Iteration 89, loss = 0.29178604\n",
      "Iteration 18, loss = 0.51989869\n",
      "Iteration 3, loss = 0.58173251\n",
      "Iteration 1, loss = 0.58922997\n",
      "Iteration 90, loss = 0.29813283\n",
      "Iteration 32, loss = 0.49956005\n",
      "Iteration 2, loss = 0.57681524\n",
      "Iteration 50, loss = 0.34444838Iteration 4, loss = 0.58956865\n",
      "\n",
      "Iteration 91, loss = 0.28621677\n",
      "Iteration 33, loss = 0.49801501\n",
      "Iteration 92, loss = 0.31551348Iteration 3, loss = 0.56112146\n",
      "Iteration 5, loss = 0.56807103\n",
      "\n",
      "Iteration 34, loss = 0.49815946\n",
      "Iteration 4, loss = 0.54837156\n",
      "Iteration 6, loss = 0.52914414\n",
      "Iteration 51, loss = 0.33365048\n",
      "Iteration 35, loss = 0.49791403\n",
      "Iteration 5, loss = 0.53711147\n",
      "Iteration 7, loss = 0.51389757\n",
      "Iteration 52, loss = 0.36459390\n",
      "Iteration 6, loss = 0.52301837\n",
      "Iteration 36, loss = 0.49724012\n",
      "Iteration 8, loss = 0.50952366Iteration 93, loss = 0.31463089\n",
      "\n",
      "Iteration 53, loss = 0.31955985\n",
      "Iteration 7, loss = 0.51266597\n",
      "Iteration 37, loss = 0.49813265Iteration 42, loss = 0.49770957\n",
      "\n",
      "Iteration 94, loss = 0.28724363\n",
      "Iteration 9, loss = 0.50549455\n",
      "Iteration 54, loss = 0.33853609\n",
      "Iteration 8, loss = 0.50542726\n",
      "Iteration 43, loss = 0.49509598\n",
      "Iteration 38, loss = 0.49711492\n",
      "Iteration 95, loss = 0.28740238\n",
      "Iteration 55, loss = 0.32047052Iteration 10, loss = 0.50179296\n",
      "\n",
      "Iteration 9, loss = 0.50039072\n",
      "Iteration 39, loss = 0.49783095\n",
      "Iteration 56, loss = 0.34263848\n",
      "Iteration 11, loss = 0.49876359\n",
      "Iteration 96, loss = 0.28887570\n",
      "Iteration 10, loss = 0.49796834\n",
      "Iteration 40, loss = 0.49890009\n",
      "Iteration 12, loss = 0.49583218Iteration 57, loss = 0.29821012\n",
      "\n",
      "Iteration 97, loss = 0.29604089\n",
      "Iteration 19, loss = 0.51361129\n",
      "Iteration 11, loss = 0.49532993\n",
      "Iteration 41, loss = 0.49690815\n",
      "Iteration 58, loss = 0.31641862\n",
      "Iteration 13, loss = 0.50143121\n",
      "Iteration 12, loss = 0.49459378\n",
      "Iteration 20, loss = 0.51009395\n",
      "Iteration 98, loss = 0.31474073Iteration 44, loss = 0.49303882\n",
      "\n",
      "Iteration 42, loss = 0.49682724\n",
      "Iteration 59, loss = 0.31567547Iteration 14, loss = 0.50682451\n",
      "\n",
      "Iteration 13, loss = 0.49500525\n",
      "Iteration 21, loss = 0.51084663\n",
      "Iteration 45, loss = 0.49141287\n",
      "Iteration 43, loss = 0.49783721\n",
      "Iteration 15, loss = 0.49371051\n",
      "Iteration 14, loss = 0.49604712\n",
      "Iteration 22, loss = 0.51047196\n",
      "Iteration 99, loss = 0.32316213\n",
      "Iteration 46, loss = 0.49086928\n",
      "Iteration 60, loss = 0.33434254Iteration 16, loss = 0.49236532\n",
      "Iteration 44, loss = 0.49831481\n",
      "Iteration 23, loss = 0.50907522\n",
      "Iteration 15, loss = 0.49360581\n",
      "Iteration 100, loss = 0.28124212\n",
      "\n",
      "Iteration 47, loss = 0.49009665Iteration 45, loss = 0.49807931Iteration 17, loss = 0.52777859\n",
      "\n",
      "Iteration 16, loss = 0.49290593\n",
      "Iteration 101, loss = 0.30328922\n",
      "Iteration 24, loss = 0.50783626\n",
      "Iteration 61, loss = 0.29684287Iteration 18, loss = 0.53250896Iteration 46, loss = 0.49602568\n",
      "\n",
      "Iteration 17, loss = 0.49252511\n",
      "Iteration 102, loss = 0.27770533\n",
      "Iteration 47, loss = 0.49955450\n",
      "Iteration 19, loss = 0.50542449Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 25, loss = 0.50829040\n",
      "\n",
      "Iteration 18, loss = 0.49182601\n",
      "Iteration 20, loss = 0.48695835\n",
      "Iteration 26, loss = 0.51112948\n",
      "Iteration 62, loss = 0.31637633\n",
      "Iteration 103, loss = 0.28296711\n",
      "Iteration 19, loss = 0.49151386\n",
      "Iteration 20, loss = 0.49161053Iteration 21, loss = 0.48689127\n",
      "\n",
      "Iteration 27, loss = 0.51030815\n",
      "Iteration 21, loss = 0.48976317Iteration 28, loss = 0.50790536Iteration 22, loss = 0.49559917\n",
      "\n",
      "\n",
      "\n",
      "Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.49566411Iteration 22, loss = 0.48797259\n",
      "\n",
      "Iteration 104, loss = 0.41886517\n",
      "Iteration 29, loss = 0.50643628\n",
      "Iteration 30, loss = 0.51073987\n",
      "Iteration 105, loss = 0.31514964\n",
      "Iteration 23, loss = 0.48882223\n",
      "Iteration 24, loss = 0.48817084\n",
      "Iteration 106, loss = 0.27761095Iteration 31, loss = 0.51457069\n",
      "\n",
      "Iteration 24, loss = 0.49777311\n",
      "Iteration 25, loss = 0.48378784\n",
      "Iteration 1, loss = 0.85023202\n",
      "Iteration 32, loss = 0.51013305\n",
      "Iteration 1, loss = 0.58830758\n",
      "Iteration 25, loss = 0.49860994\n",
      "Iteration 26, loss = 0.49225011\n",
      "Validation score: 0.761905\n",
      "Iteration 33, loss = 0.50722493Iteration 2, loss = 0.65393452\n",
      "\n",
      "Iteration 26, loss = 0.49034332\n",
      "Iteration 63, loss = 0.31513699Iteration 107, loss = 0.26648217\n",
      "\n",
      "Iteration 27, loss = 0.49404972\n",
      "Iteration 3, loss = 0.60097832Iteration 34, loss = 0.50669630\n",
      "Iteration 64, loss = 0.33924771\n",
      "Iteration 27, loss = 0.48593526\n",
      "Iteration 28, loss = 0.48398332\n",
      "\n",
      "Iteration 2, loss = 0.53236671\n",
      "Iteration 28, loss = 0.48640974\n",
      "Iteration 65, loss = 0.30356774\n",
      "Iteration 29, loss = 0.48536229\n",
      "Iteration 4, loss = 0.65625777\n",
      "Iteration 35, loss = 0.50623209\n",
      "Iteration 29, loss = 0.48954358\n",
      "Iteration 66, loss = 0.30851519\n",
      "Iteration 30, loss = 0.49222000\n",
      "Iteration 5, loss = 0.65329928\n",
      "Iteration 36, loss = 0.50604094\n",
      "Iteration 67, loss = 0.32260035\n",
      "Iteration 30, loss = 0.49052874\n",
      "Iteration 31, loss = 0.49554066\n",
      "Iteration 6, loss = 0.58276930\n",
      "Iteration 68, loss = 0.32436444\n",
      "Iteration 31, loss = 0.48940265\n",
      "Iteration 32, loss = 0.49275224\n",
      "Iteration 37, loss = 0.50856529\n",
      "Iteration 7, loss = 0.53424022\n",
      "Iteration 69, loss = 0.28307076Iteration 32, loss = 0.48552247\n",
      "Iteration 33, loss = 0.48522610\n",
      "Iteration 8, loss = 0.53282113\n",
      "Iteration 38, loss = 0.51342267\n",
      "Iteration 33, loss = 0.48741545\n",
      "Validation score: 0.714286\n",
      "Iteration 34, loss = 0.48350692\n",
      "Iteration 39, loss = 0.51380463\n",
      "Iteration 34, loss = 0.48705297\n",
      "\n",
      "Iteration 35, loss = 0.48459250Iteration 40, loss = 0.50927492\n",
      "\n",
      "Iteration 35, loss = 0.48440237\n",
      "Iteration 1, loss = 0.67472894\n",
      "Iteration 36, loss = 0.48535277\n",
      "Iteration 41, loss = 0.50787323\n",
      "Validation score: 0.761905Iteration 36, loss = 0.49281424\n",
      "Iteration 3, loss = 0.53100691\n",
      "\n",
      "Validation score: 0.714286\n",
      "Iteration 70, loss = 0.28696262\n",
      "Iteration 37, loss = 0.48659256\n",
      "Iteration 42, loss = 0.51350903\n",
      "Iteration 37, loss = 0.48478068\n",
      "Iteration 71, loss = 0.30098950Iteration 38, loss = 0.48726090Iteration 38, loss = 0.48792515\n",
      "\n",
      "Iteration 43, loss = 0.51575072\n",
      "Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "Iteration 108, loss = 0.27075260\n",
      "Iteration 2, loss = 0.57503755\n",
      "Iteration 39, loss = 0.49605311Validation score: 0.761905\n",
      "Iteration 39, loss = 0.48613736\n",
      "Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 109, loss = 0.29235876\n",
      "Iteration 9, loss = 0.54180728\n",
      "Iteration 4, loss = 0.51414708Iteration 110, loss = 0.30460688\n",
      "Iteration 10, loss = 0.53650002\n",
      "\n",
      "Iteration 72, loss = 0.34220313\n",
      "Validation score: 0.714286\n",
      "\n",
      "Iteration 40, loss = 0.49127533\n",
      "Iteration 3, loss = 0.57234726\n",
      "Iteration 111, loss = 0.27524424\n",
      "Validation score: 0.738095Iteration 73, loss = 0.30443535\n",
      "\n",
      "Iteration 41, loss = 0.48824683\n",
      "Iteration 112, loss = 0.26704575\n",
      "Iteration 42, loss = 0.48790493Iteration 1, loss = 0.64931836\n",
      "Iteration 11, loss = 0.51849029\n",
      "Iteration 2, loss = 0.60652188\n",
      "Iteration 4, loss = 0.55241803\n",
      "\n",
      "Validation score: 0.738095\n",
      "Iteration 1, loss = 0.56181386\n",
      "Iteration 113, loss = 0.27972913\n",
      "Iteration 3, loss = 0.57560798\n",
      "Validation score: 0.690476\n",
      "Iteration 5, loss = 0.50909401\n",
      "Iteration 43, loss = 0.48819167Iteration 12, loss = 0.50951801\n",
      "Iteration 74, loss = 0.28967319\n",
      "Iteration 4, loss = 0.56749816\n",
      "Iteration 13, loss = 0.50749306\n",
      "Validation score: 0.714286\n",
      "Iteration 5, loss = 0.54583391Iteration 114, loss = 0.30931921\n",
      "\n",
      "Iteration 14, loss = 0.50655144\n",
      "Iteration 2, loss = 0.54297993\n",
      "Validation score: 0.690476\n",
      "Iteration 5, loss = 0.54174002\n",
      "Iteration 115, loss = 0.28500292Iteration 15, loss = 0.50507036\n",
      "\n",
      "Validation score: 0.738095\n",
      "Iteration 6, loss = 0.52368462\n",
      "Iteration 75, loss = 0.32242837\n",
      "Iteration 116, loss = 0.31178621Iteration 44, loss = 0.48932780\n",
      "\n",
      "Iteration 7, loss = 0.50445913\n",
      "Iteration 76, loss = 0.28169737\n",
      "Iteration 45, loss = 0.48860305\n",
      "Iteration 117, loss = 0.28929288\n",
      "Iteration 8, loss = 0.50132824\n",
      "Iteration 6, loss = 0.54139643\n",
      "Iteration 46, loss = 0.48884276\n",
      "Iteration 3, loss = 0.53834711\n",
      "Iteration 77, loss = 0.26788701Validation score: 0.738095\n",
      "\n",
      "Validation score: 0.714286\n",
      "Iteration 9, loss = 0.49793538\n",
      "Iteration 47, loss = 0.48728319\n",
      "Iteration 6, loss = 0.51607188\n",
      "Validation score: 0.714286\n",
      "Iteration 48, loss = 0.48547186\n",
      "Iteration 118, loss = 0.25853663Iteration 10, loss = 0.49477296\n",
      "\n",
      "Iteration 78, loss = 0.26611524Iteration 49, loss = 0.48566581\n",
      "Iteration 7, loss = 0.53341101\n",
      "Iteration 11, loss = 0.48397107Iteration 119, loss = 0.27384171\n",
      "\n",
      "Iteration 4, loss = 0.52730719\n",
      "Validation score: 0.738095\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.50330468\n",
      "Validation score: 0.714286\n",
      "Iteration 120, loss = 0.26644559\n",
      "Iteration 12, loss = 0.48217855\n",
      "\n",
      "Iteration 17, loss = 0.50650732\n",
      "Iteration 121, loss = 0.29888746\n",
      "Iteration 13, loss = 0.48320470\n",
      "Iteration 18, loss = 0.50814219\n",
      "Iteration 79, loss = 0.31565348\n",
      "Iteration 5, loss = 0.51886368\n",
      "Iteration 19, loss = 0.51033037\n",
      "Iteration 80, loss = 0.26952195Validation score: 0.714286\n",
      "\n",
      "Iteration 14, loss = 0.48329400\n",
      "Iteration 122, loss = 0.29735018\n",
      "Iteration 20, loss = 0.50773570\n",
      "Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "Iteration 81, loss = 0.28098387Iteration 15, loss = 0.48051487\n",
      "Iteration 123, loss = 0.27417744\n",
      "Iteration 7, loss = 0.51661103\n",
      "Validation score: 0.714286\n",
      "\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 6, loss = 0.51954187\n",
      "Validation score: 0.690476\n",
      "Iteration 124, loss = 0.28322139\n",
      "Iteration 16, loss = 0.48203481\n",
      "Iteration 21, loss = 0.50861900Iteration 82, loss = 0.27560341Iteration 17, loss = 0.48716866\n",
      "Iteration 125, loss = 0.26236637\n",
      "Iteration 7, loss = 0.51311222\n",
      "Iteration 18, loss = 0.49224057\n",
      "Validation score: 0.714286\n",
      "Iteration 126, loss = 0.30510752\n",
      "Iteration 19, loss = 0.48386998\n",
      "Iteration 1, loss = 0.63128484\n",
      "Iteration 127, loss = 0.29774322\n",
      "\n",
      "Validation score: 0.738095\n",
      "Iteration 20, loss = 0.47674786\n",
      "\n",
      "Iteration 22, loss = 0.51043599\n",
      "Iteration 128, loss = 0.28226843Iteration 21, loss = 0.47461711\n",
      "Iteration 8, loss = 0.50726778\n",
      "\n",
      "Iteration 83, loss = 0.29072785\n",
      "Iteration 23, loss = 0.50994981\n",
      "Validation score: 0.690476\n",
      "Iteration 129, loss = 0.25105874Iteration 22, loss = 0.47554870\n",
      "Iteration 24, loss = 0.50767179Iteration 84, loss = 0.25836766\n",
      "\n",
      "Iteration 23, loss = 0.47530724\n",
      "Iteration 1, loss = 0.60280310\n",
      "Iteration 85, loss = 0.26103455\n",
      "Validation score: 0.833333\n",
      "Iteration 24, loss = 0.47519687\n",
      "Iteration 9, loss = 0.50395404\n",
      "Iteration 25, loss = 0.50618061\n",
      "Iteration 130, loss = 0.25475787Validation score: 0.690476\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "\n",
      "\n",
      "Iteration 86, loss = 0.27091288\n",
      "Iteration 2, loss = 0.59005585\n",
      "Iteration 25, loss = 0.47449354\n",
      "Iteration 131, loss = 0.26952876Validation score: 0.738095\n",
      "\n",
      "Iteration 26, loss = 0.47577570\n",
      "Iteration 1, loss = 0.64782680Iteration 132, loss = 0.26688937\n",
      "Iteration 2, loss = 0.62666713Iteration 87, loss = 0.29412590\n",
      "Iteration 27, loss = 0.47425472\n",
      "Iteration 133, loss = 0.26668869\n",
      "Iteration 26, loss = 0.50447572\n",
      "Iteration 88, loss = 0.31465144\n",
      "Iteration 3, loss = 0.57547948\n",
      "Iteration 28, loss = 0.47410947\n",
      "Iteration 27, loss = 0.50422313\n",
      "Validation score: 0.738095\n",
      "Validation score: 0.738095\n",
      "Iteration 134, loss = 0.25563497\n",
      "Iteration 28, loss = 0.50696014\n",
      "\n",
      "Iteration 29, loss = 0.47467774\n",
      "Validation score: 0.809524\n",
      "Iteration 30, loss = 0.47373213\n",
      "Iteration 89, loss = 0.30415363Iteration 4, loss = 0.56970853\n",
      "\n",
      "Iteration 1, loss = 0.64140936\n",
      "Validation score: 0.785714\n",
      "Iteration 31, loss = 0.48167692\n",
      "Validation score: 0.738095\n",
      "Iteration 3, loss = 0.59080854\n",
      "Iteration 32, loss = 0.48687801\n",
      "Iteration 29, loss = 0.50779370\n",
      "Iteration 90, loss = 0.27285274\n",
      "Iteration 33, loss = 0.47370375Iteration 30, loss = 0.50604519\n",
      "Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 2, loss = 0.57916237\n",
      "Validation score: 0.785714\n",
      "\n",
      "Iteration 2, loss = 0.59518580\n",
      "Validation score: 0.785714\n",
      "Iteration 34, loss = 0.47764874\n",
      "Validation score: 0.809524Iteration 5, loss = 0.56997300\n",
      "\n",
      "Iteration 91, loss = 0.25458730Validation score: 0.738095\n",
      "\n",
      "Iteration 35, loss = 0.48175856\n",
      "Training loss did not improve more than tol=0.000932 for 13 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.56556974\n",
      "Iteration 4, loss = 0.59153674Validation score: 0.785714\n",
      "\n",
      "Validation score: 0.785714\n",
      "Iteration 135, loss = 0.38469242\n",
      "Iteration 92, loss = 0.25376404\n",
      "Iteration 93, loss = 0.25077989\n",
      "Iteration 4, loss = 0.55837712\n",
      "Validation score: 0.785714\n",
      "Iteration 136, loss = 0.29136496\n",
      "Iteration 3, loss = 0.59763864\n",
      "Validation score: 0.738095Iteration 6, loss = 0.56715303\n",
      "\n",
      "Validation score: 0.738095Iteration 137, loss = 0.26891993\n",
      "\n",
      "Iteration 94, loss = 0.29636191\n",
      "Iteration 5, loss = 0.55973899\n",
      "Iteration 5, loss = 0.59028194\n",
      "Iteration 138, loss = 0.25785502\n",
      "Validation score: 0.785714\n",
      "Validation score: 0.809524Iteration 95, loss = 0.27635826\n",
      "Iteration 139, loss = 0.25949940\n",
      "Iteration 96, loss = 0.31708156\n",
      "Iteration 1, loss = 0.60643104Iteration 7, loss = 0.56173661\n",
      "Iteration 4, loss = 0.58628000\n",
      "\n",
      "Validation score: 0.738095Validation score: 0.785714\n",
      "Iteration 140, loss = 0.28719610\n",
      "Validation score: 0.761905\n",
      "\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "Iteration 97, loss = 0.29458060\n",
      "Iteration 6, loss = 0.55062600\n",
      "\n",
      "Iteration 141, loss = 0.30489131\n",
      "Validation score: 0.785714\n",
      "Iteration 98, loss = 0.28612749\n",
      "Iteration 142, loss = 0.28175196\n",
      "Iteration 99, loss = 0.24787083\n",
      "Iteration 5, loss = 0.58098721\n",
      "Validation score: 0.785714Iteration 2, loss = 0.60187571\n",
      "\n",
      "Iteration 6, loss = 0.58382817\n",
      "Iteration 100, loss = 0.25518821Validation score: 0.571429\n",
      "\n",
      "Validation score: 0.809524\n",
      "Iteration 143, loss = 0.25865547\n",
      "Iteration 101, loss = 0.25850189\n",
      "Iteration 7, loss = 0.55249440\n",
      "Iteration 144, loss = 0.27430860\n",
      "Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "Validation score: 0.785714\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "Iteration 102, loss = 0.26187280\n",
      "Iteration 6, loss = 0.58601383\n",
      "Iteration 3, loss = 0.60837446\n",
      "Validation score: 0.809524\n",
      "Iteration 7, loss = 0.57963196Iteration 103, loss = 0.28284413\n",
      "Iteration 104, loss = 0.23831610\n",
      "Validation score: 0.571429\n",
      "Iteration 7, loss = 0.58393515Iteration 105, loss = 0.27681633\n",
      "\n",
      "Validation score: 0.785714\n",
      "Iteration 106, loss = 0.27099300\n",
      "Iteration 4, loss = 0.62128437\n",
      "Iteration 107, loss = 0.27862990\n",
      "Validation score: 0.571429\n",
      "Iteration 108, loss = 0.24580573\n",
      "Iteration 8, loss = 0.56661152\n",
      "Validation score: 0.761905\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "Iteration 109, loss = 0.28942587\n",
      "Iteration 5, loss = 0.63645078\n",
      "Iteration 110, loss = 0.26704537\n",
      "Validation score: 0.595238\n",
      "\n",
      "Iteration 111, loss = 0.26479588\n",
      "Validation score: 0.761905\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "Iteration 112, loss = 0.27619464\n",
      "Iteration 113, loss = 0.24172507\n",
      "Iteration 6, loss = 0.60559980\n",
      "Validation score: 0.714286\n",
      "Iteration 114, loss = 0.27560687\n",
      "Iteration 115, loss = 0.27657384\n",
      "Iteration 116, loss = 0.26017963\n",
      "Iteration 7, loss = 0.57698628\n",
      "Validation score: 0.642857\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "Iteration 117, loss = 0.23461791\n",
      "Iteration 1, loss = 0.59647310\n",
      "Iteration 118, loss = 0.22832932Validation score: 0.738095\n",
      "\n",
      "Iteration 119, loss = 0.27025108\n",
      "Iteration 120, loss = 0.26742355\n",
      "Iteration 121, loss = 0.24794041\n",
      "Iteration 122, loss = 0.22782659\n",
      "Iteration 1, loss = 0.69670212\n",
      "Validation score: 0.761905\n",
      "Iteration 123, loss = 0.25353317\n",
      "Iteration 124, loss = 0.26843158\n",
      "Iteration 125, loss = 0.23794091\n",
      "Iteration 2, loss = 0.56125845\n",
      "Validation score: 0.809524\n",
      "Iteration 126, loss = 0.24592358\n",
      "Iteration 127, loss = 0.27475142\n",
      "Iteration 128, loss = 0.22987363\n",
      "Iteration 3, loss = 0.55556494\n",
      "Iteration 2, loss = 0.52237550\n",
      "Validation score: 0.809524Validation score: 0.738095\n",
      "\n",
      "Iteration 129, loss = 0.25008665\n",
      "Iteration 130, loss = 0.21381276\n",
      "Iteration 131, loss = 0.26361267Iteration 4, loss = 0.54585876\n",
      "Iteration 3, loss = 0.52787137\n",
      "\n",
      "Validation score: 0.809524Validation score: 0.738095\n",
      "\n",
      "Iteration 132, loss = 0.25598690\n",
      "Iteration 5, loss = 0.53577059Iteration 4, loss = 0.51296007\n",
      "\n",
      "Validation score: 0.738095\n",
      "Validation score: 0.809524\n",
      "Iteration 133, loss = 0.27042228\n",
      "Iteration 134, loss = 0.25550530\n",
      "Iteration 6, loss = 0.53880524\n",
      "Iteration 135, loss = 0.21812677\n",
      "Validation score: 0.809524\n",
      "Iteration 136, loss = 0.20652359\n",
      "Iteration 137, loss = 0.21141733\n",
      "Iteration 7, loss = 0.54193753\n",
      "Iteration 138, loss = 0.22392524\n",
      "Validation score: 0.809524\n",
      "Iteration 139, loss = 0.22800709\n",
      "Iteration 140, loss = 0.21974572\n",
      "Iteration 8, loss = 0.53806936\n",
      "Iteration 141, loss = 0.21946721\n",
      "Iteration 5, loss = 0.50722600\n",
      "Validation score: 0.809524\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "Validation score: 0.738095\n",
      "Iteration 142, loss = 0.26339446\n",
      "Iteration 143, loss = 0.26207380\n",
      "Iteration 6, loss = 0.49932667\n",
      "Iteration 144, loss = 0.22273353\n",
      "Validation score: 0.761905\n",
      "Iteration 145, loss = 0.25764590\n",
      "Iteration 146, loss = 0.26839452\n",
      "Iteration 7, loss = 0.50059940Iteration 147, loss = 0.22017640\n",
      "Iteration 1, loss = 0.61210351\n",
      "Validation score: 0.730769Iteration 148, loss = 0.21693054\n",
      "\n",
      "Iteration 149, loss = 0.19972308\n",
      "Iteration 2, loss = 0.55191039\n",
      "Iteration 150, loss = 0.26735928\n",
      "Validation score: 0.730769\n",
      "Iteration 151, loss = 0.23193411\n",
      "Iteration 152, loss = 0.28322317\n",
      "Iteration 3, loss = 0.54671850\n",
      "Validation score: 0.730769\n",
      "Iteration 153, loss = 0.27800722\n",
      "Iteration 154, loss = 0.20055217\n",
      "Iteration 4, loss = 0.54480106\n",
      "Validation score: 0.730769\n",
      "\n",
      "Iteration 155, loss = 0.22792636\n",
      "Validation score: 0.761905\n",
      "Iteration 156, loss = 0.21764967\n",
      "Iteration 5, loss = 0.54352058\n",
      "Validation score: 0.730769\n",
      "Iteration 157, loss = 0.23035722\n",
      "Iteration 8, loss = 0.49462274Iteration 158, loss = 0.19851137\n",
      "Iteration 6, loss = 0.54266437\n",
      "Validation score: 0.730769\n",
      "Iteration 159, loss = 0.21252252\n",
      "Iteration 160, loss = 0.19728529\n",
      "Iteration 7, loss = 0.54182993\n",
      "Iteration 161, loss = 0.20232768\n",
      "Validation score: 0.730769\n",
      "Iteration 162, loss = 0.24096441\n",
      "\n",
      "Validation score: 0.761905\n",
      "Iteration 8, loss = 0.54108219\n",
      "Iteration 163, loss = 0.22509572\n",
      "Validation score: 0.730769\n",
      "Iteration 164, loss = 0.23127013\n",
      "Iteration 9, loss = 0.49107505\n",
      "Iteration 165, loss = 0.20043468Iteration 9, loss = 0.54050395\n",
      "Validation score: 0.769231Validation score: 0.761905\n",
      "\n",
      "Iteration 10, loss = 0.53997646\n",
      "\n",
      "Validation score: 0.769231\n",
      "Iteration 10, loss = 0.48894885Iteration 166, loss = 0.20154607\n",
      "Iteration 11, loss = 0.53948817\n",
      "Iteration 167, loss = 0.20246109\n",
      "Validation score: 0.769231\n",
      "\n",
      "Validation score: 0.761905Iteration 12, loss = 0.53905482\n",
      "Validation score: 0.769231\n",
      "Iteration 168, loss = 0.21463009\n",
      "Iteration 13, loss = 0.53869035\n",
      "\n",
      "Validation score: 0.769231\n",
      "Iteration 14, loss = 0.53824491\n",
      "Validation score: 0.769231\n",
      "Iteration 11, loss = 0.48489840\n",
      "Validation score: 0.761905\n",
      "Iteration 15, loss = 0.53798059\n",
      "Iteration 169, loss = 0.26304524\n",
      "Validation score: 0.769231\n",
      "Iteration 170, loss = 0.21071802\n",
      "Iteration 12, loss = 0.48118835\n",
      "Validation score: 0.761905\n",
      "Validation score did not improve more than tol=0.000937 for 5 consecutive epochs. Stopping.\n",
      "Iteration 171, loss = 0.21907286\n",
      "Iteration 16, loss = 0.53756776\n",
      "Validation score: 0.769231\n",
      "Iteration 172, loss = 0.20261311\n",
      "Iteration 173, loss = 0.22620432\n",
      "Iteration 17, loss = 0.53722321\n",
      "Validation score: 0.769231\n",
      "Iteration 174, loss = 0.19725797Iteration 18, loss = 0.53691530\n",
      "\n",
      "Validation score: 0.769231\n",
      "Iteration 1, loss = 0.61588609\n",
      "Iteration 175, loss = 0.23761638Validation score: 0.730769\n",
      "\n",
      "Iteration 19, loss = 0.53662983\n",
      "Validation score: 0.769231\n",
      "Iteration 2, loss = 0.55440119\n",
      "Validation score: 0.730769\n",
      "Iteration 20, loss = 0.53635262Iteration 176, loss = 0.22175495\n",
      "\n",
      "Validation score: 0.769231\n",
      "Iteration 3, loss = 0.55121246\n",
      "Iteration 21, loss = 0.53610674\n",
      "Iteration 177, loss = 0.26908550\n",
      "Validation score: 0.769231\n",
      "Validation score: 0.730769\n",
      "Iteration 22, loss = 0.53586436\n",
      "Iteration 178, loss = 0.22871447\n",
      "Validation score: 0.769231\n",
      "Iteration 179, loss = 0.20808969Iteration 4, loss = 0.55020801\n",
      "Validation score: 0.730769\n",
      "Iteration 23, loss = 0.53561235\n",
      "\n",
      "Validation score: 0.769231\n",
      "Iteration 5, loss = 0.54978766\n",
      "Validation score: 0.730769\n",
      "Iteration 180, loss = 0.19234874Iteration 24, loss = 0.53536842\n",
      "\n",
      "Validation score: 0.769231\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "Iteration 181, loss = 0.18716719\n",
      "Iteration 6, loss = 0.54933241\n",
      "Validation score: 0.730769Iteration 182, loss = 0.24309600\n",
      "\n",
      "Iteration 183, loss = 0.19864470\n",
      "Iteration 7, loss = 0.54869470\n",
      "Validation score: 0.730769Iteration 184, loss = 0.25438977\n",
      "\n",
      "Iteration 185, loss = 0.20583149\n",
      "Iteration 8, loss = 0.54828325Iteration 1, loss = 0.56890306\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 186, loss = 0.23183814\n",
      "Iteration 2, loss = 0.52827734\n",
      "Iteration 187, loss = 0.19640930\n",
      "Validation score: 0.730769\n",
      "Iteration 188, loss = 0.20325265\n",
      "Validation score: 0.730769\n",
      "Iteration 189, loss = 0.23574985\n",
      "Iteration 3, loss = 0.52705434\n",
      "Validation score: 0.730769\n",
      "Iteration 190, loss = 0.21714863\n",
      "Iteration 9, loss = 0.54806193\n",
      "Validation score: 0.730769\n",
      "Iteration 191, loss = 0.21803395\n",
      "Iteration 4, loss = 0.52635918\n",
      "Validation score: 0.730769\n",
      "Iteration 192, loss = 0.20060124\n",
      "Iteration 10, loss = 0.54759559\n",
      "Validation score: 0.730769\n",
      "Iteration 193, loss = 0.18515674\n",
      "Iteration 5, loss = 0.52556700\n",
      "Validation score: 0.730769\n",
      "Iteration 194, loss = 0.19995842\n",
      "Iteration 195, loss = 0.23242696\n",
      "Iteration 11, loss = 0.54727513\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.52530936\n",
      "Iteration 196, loss = 0.19484204\n",
      "Validation score: 0.730769\n",
      "Iteration 197, loss = 0.21042705\n",
      "Iteration 12, loss = 0.54705023\n",
      "Validation score: 0.730769\n",
      "Iteration 7, loss = 0.52474386\n",
      "Validation score: 0.730769\n",
      "Iteration 198, loss = 0.21254270\n",
      "Iteration 13, loss = 0.54681497\n",
      "Iteration 199, loss = 0.27234628\n",
      "Validation score: 0.730769\n",
      "Iteration 8, loss = 0.52445241\n",
      "Validation score: 0.730769\n",
      "Iteration 200, loss = 0.25374209\n",
      "Iteration 201, loss = 0.21676753\n",
      "Iteration 9, loss = 0.52425058\n",
      "Iteration 202, loss = 0.25586014\n",
      "Validation score: 0.730769\n",
      "Iteration 203, loss = 0.23493701\n",
      "Iteration 10, loss = 0.52372909\n",
      "Iteration 204, loss = 0.19619982Validation score: 0.730769\n",
      "\n",
      "Iteration 14, loss = 0.54653823\n",
      "Iteration 205, loss = 0.20880567\n",
      "Validation score: 0.730769\n",
      "Iteration 11, loss = 0.52361046\n",
      "Iteration 206, loss = 0.18489066\n",
      "Validation score: 0.730769\n",
      "Iteration 15, loss = 0.54624298\n",
      "Iteration 207, loss = 0.23276323Validation score: 0.730769\n",
      "\n",
      "Iteration 208, loss = 0.20145431\n",
      "Iteration 12, loss = 0.52326483\n",
      "Validation score: 0.730769Iteration 16, loss = 0.54609053\n",
      "\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "Iteration 209, loss = 0.25914868\n",
      "Iteration 210, loss = 0.20007753\n",
      "Iteration 13, loss = 0.52304531\n",
      "Validation score: 0.730769\n",
      "Iteration 211, loss = 0.21907410\n",
      "Iteration 212, loss = 0.18538845\n",
      "Iteration 14, loss = 0.52284130\n",
      "Validation score: 0.730769\n",
      "Iteration 213, loss = 0.18898899\n",
      "Iteration 214, loss = 0.21074114\n",
      "Iteration 15, loss = 0.52266338\n",
      "Validation score: 0.730769\n",
      "Iteration 215, loss = 0.23498369\n",
      "Iteration 216, loss = 0.24417059\n",
      "Iteration 16, loss = 0.52247860\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "Iteration 217, loss = 0.21727181\n",
      "Iteration 218, loss = 0.23208823\n",
      "Iteration 219, loss = 0.20435861\n",
      "Iteration 220, loss = 0.23596558\n",
      "Iteration 1, loss = 0.60271059\n",
      "Validation score: 0.730769\n",
      "Iteration 221, loss = 0.19655597\n",
      "Training loss did not improve more than tol=0.000023 for 14 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.55767963Iteration 1, loss = 0.59237941\n",
      "Validation score: 0.730769\n",
      "Iteration 2, loss = 0.57506563\n",
      "Validation score: 0.730769\n",
      "Iteration 1, loss = 0.65045473\n",
      "Validation score: 0.730769\n",
      "Iteration 3, loss = 0.56911978\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 2, loss = 0.57334323\n",
      "Iteration 4, loss = 0.56319536\n",
      "Iteration 3, loss = 0.55630536\n",
      "Validation score: 0.730769Validation score: 0.730769\n",
      "\n",
      "Iteration 4, loss = 0.55418789\n",
      "Validation score: 0.730769\n",
      "Iteration 5, loss = 0.55894154\n",
      "Validation score: 0.730769\n",
      "Iteration 5, loss = 0.55232777\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.55577111\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.55115111\n",
      "Validation score: 0.730769\n",
      "Iteration 7, loss = 0.55344638\n",
      "Validation score: 0.730769\n",
      "Iteration 8, loss = 0.55158211\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 9, loss = 0.54989041\n",
      "Validation score: 0.730769\n",
      "Iteration 3, loss = 0.56847460\n",
      "Validation score: 0.730769\n",
      "Iteration 10, loss = 0.54849250\n",
      "Validation score: 0.730769\n",
      "Iteration 4, loss = 0.56743457Iteration 11, loss = 0.54720685\n",
      "Validation score: 0.730769Iteration 7, loss = 0.55011021\n",
      "\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 12, loss = 0.54608062\n",
      "Iteration 8, loss = 0.54918848\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 9, loss = 0.54840235\n",
      "Validation score: 0.730769\n",
      "Iteration 13, loss = 0.54509582\n",
      "Validation score: 0.730769\n",
      "Iteration 5, loss = 0.56693167\n",
      "Iteration 10, loss = 0.54786881Validation score: 0.730769Iteration 14, loss = 0.54420191\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 15, loss = 0.54345129Iteration 6, loss = 0.56644415\n",
      "Validation score: 0.730769\n",
      "Iteration 7, loss = 0.56602441\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 16, loss = 0.54267920Iteration 11, loss = 0.54714568\n",
      "Validation score: 0.730769\n",
      "Iteration 12, loss = 0.54663519\n",
      "Validation score: 0.730769\n",
      "Iteration 13, loss = 0.54620032\n",
      "Validation score: 0.730769\n",
      "Iteration 14, loss = 0.54567588\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 15, loss = 0.54534854Iteration 8, loss = 0.56565617Iteration 1, loss = 0.64562597\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 2, loss = 0.57130288Iteration 16, loss = 0.54490568\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "\n",
      "\n",
      "Validation score: 0.730769Validation score: 0.730769\n",
      "\n",
      "Iteration 3, loss = 0.56829643\n",
      "Iteration 9, loss = 0.56537404\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 4, loss = 0.56725283\n",
      "Iteration 10, loss = 0.56494807\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 5, loss = 0.56600324\n",
      "Validation score: 0.730769\n",
      "Iteration 1, loss = 0.59326092Iteration 6, loss = 0.56526174\n",
      "\n",
      "Validation score: 0.730769Validation score: 0.730769\n",
      "\n",
      "Iteration 11, loss = 0.56473405\n",
      "Validation score: 0.730769\n",
      "Iteration 2, loss = 0.55601019Iteration 7, loss = 0.56458189\n",
      "Validation score: 0.730769\n",
      "Iteration 12, loss = 0.56441571\n",
      "\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 3, loss = 0.54285553Iteration 8, loss = 0.56387863\n",
      "Iteration 13, loss = 0.56412090\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 9, loss = 0.56344423\n",
      "Iteration 14, loss = 0.56389272\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 10, loss = 0.56295615\n",
      "Validation score: 0.730769Iteration 15, loss = 0.56370183\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 4, loss = 0.53926817\n",
      "Validation score: 0.730769\n",
      "Iteration 11, loss = 0.56252952Iteration 16, loss = 0.56349146\n",
      "\n",
      "Validation score: 0.730769Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 5, loss = 0.53690590Iteration 12, loss = 0.56209398\n",
      "Validation score: 0.730769\n",
      "Iteration 13, loss = 0.56170036\n",
      "Iteration 1, loss = 0.58696336\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 2, loss = 0.57484156\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 14, loss = 0.56134844\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.53499120\n",
      "Validation score: 0.730769\n",
      "Iteration 15, loss = 0.56107889\n",
      "Validation score: 0.730769\n",
      "Iteration 7, loss = 0.53366626\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 16, loss = 0.56072474\n",
      "Iteration 8, loss = 0.53232087\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "Validation score: 0.730769\n",
      "Iteration 9, loss = 0.53120795Iteration 3, loss = 0.56850024\n",
      "Validation score: 0.730769\n",
      "Iteration 4, loss = 0.56333210\n",
      "Validation score: 0.730769\n",
      "Iteration 5, loss = 0.55983888\n",
      "Validation score: 0.730769\n",
      "\n",
      "Iteration 1, loss = 0.63249868\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.740260\n",
      "Iteration 10, loss = 0.53030433\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.55723652\n",
      "Validation score: 0.730769\n",
      "Iteration 2, loss = 0.58529505\n",
      "Iteration 11, loss = 0.52951663\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.740260\n",
      "Iteration 7, loss = 0.55502226\n",
      "Validation score: 0.730769\n",
      "Iteration 12, loss = 0.52888254Iteration 3, loss = 0.58671972\n",
      "Validation score: 0.740260\n",
      "Iteration 8, loss = 0.55325936\n",
      "Validation score: 0.730769\n",
      "Iteration 4, loss = 0.58498546\n",
      "Validation score: 0.740260\n",
      "Iteration 9, loss = 0.55183818\n",
      "Iteration 5, loss = 0.59755625\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.740260\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 10, loss = 0.55079489\n",
      "Validation score: 0.730769\n",
      "Iteration 13, loss = 0.52810375\n",
      "Validation score: 0.730769\n",
      "Iteration 11, loss = 0.54951972\n",
      "Validation score: 0.730769\n",
      "Iteration 14, loss = 0.52766538\n",
      "Validation score: 0.730769\n",
      "Iteration 15, loss = 0.52701353\n",
      "Iteration 12, loss = 0.54857526\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 16, loss = 0.52657247Iteration 13, loss = 0.54776818\n",
      "Validation score: 0.730769\n",
      "Iteration 14, loss = 0.54709241\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.58972472\n",
      "\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.730769Iteration 15, loss = 0.54634301\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 7, loss = 0.58783789\n",
      "Validation score: 0.740260\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "Iteration 16, loss = 0.54576233\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.57921366Iteration 1, loss = 0.61635425\n",
      "\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.730769\n",
      "Iteration 9, loss = 0.57621824\n",
      "Validation score: 0.740260\n",
      "Iteration 2, loss = 0.54917747\n",
      "Validation score: 0.730769Iteration 10, loss = 0.57768063\n",
      "Validation score: 0.740260\n",
      "\n",
      "Iteration 3, loss = 0.54385803Iteration 11, loss = 0.57653100\n",
      "Validation score: 0.740260\n",
      "Iteration 1, loss = 0.60955335\n",
      "Validation score: 0.730769\n",
      "Iteration 12, loss = 0.57605025\n",
      "Validation score: 0.730769\n",
      "Iteration 2, loss = 0.58087869\n",
      "Iteration 4, loss = 0.54154464\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.740260\n",
      "Iteration 13, loss = 0.57792677Iteration 3, loss = 0.58923853\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.740260\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "Iteration 4, loss = 0.58410412\n",
      "Iteration 14, loss = 0.57623023\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.740260Validation score: 0.730769\n",
      "\n",
      "Iteration 15, loss = 0.57579894Iteration 5, loss = 0.54016523\n",
      "\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.740260\n",
      "Iteration 5, loss = 0.58502975\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.53855732\n",
      "Validation score: 0.730769\n",
      "Iteration 16, loss = 0.57568816\n",
      "Validation score: 0.740260\n",
      "Iteration 7, loss = 0.53761543\n",
      "Validation score: 0.730769Iteration 6, loss = 0.58411692\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 17, loss = 0.57616537\n",
      "Validation score: 0.740260\n",
      "Iteration 8, loss = 0.53668216\n",
      "Validation score: 0.730769\n",
      "Iteration 7, loss = 0.59045516\n",
      "Validation score: 0.730769\n",
      "Iteration 18, loss = 0.57548151Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "\n",
      "Validation score: 0.740260\n",
      "Iteration 9, loss = 0.53593183\n",
      "Validation score: 0.730769\n",
      "Iteration 19, loss = 0.57509323\n",
      "Validation score: 0.740260\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292Iteration 10, loss = 0.53516722\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 8, loss = 0.58674671\n",
      "Iteration 20, loss = 0.57509460\n",
      "Validation score: 0.730769\n",
      "Iteration 11, loss = 0.53456904Validation score: 0.740260\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 9, loss = 0.58249791\n",
      "Iteration 21, loss = 0.57512184\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.740260\n",
      "Iteration 12, loss = 0.53400933\n",
      "Validation score: 0.730769\n",
      "Iteration 22, loss = 0.57511476\n",
      "Iteration 10, loss = 0.57996706\n",
      "Validation score: 0.740260\n",
      "Iteration 13, loss = 0.53335488\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.730769\n",
      "Iteration 23, loss = 0.57502805\n",
      "Iteration 14, loss = 0.53286453\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.730769\n",
      "Iteration 11, loss = 0.57891046\n",
      "Validation score: 0.730769\n",
      "Iteration 24, loss = 0.57503812\n",
      "Iteration 15, loss = 0.53235637\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.740260\n",
      "Iteration 16, loss = 0.53189224Iteration 25, loss = 0.57497537\n",
      "Iteration 12, loss = 0.57823453\n",
      "\n",
      "Validation score: 0.730769Validation score: 0.740260\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score did not improve more than tol=0.000994 for 14 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.57491225\n",
      "Validation score: 0.740260\n",
      "Iteration 27, loss = 0.57492172\n",
      "Iteration 13, loss = 0.57805268\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "Iteration 28, loss = 0.57490986\n",
      "Iteration 14, loss = 0.57714189Validation score: 0.740260\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 1, loss = 0.60720588\n",
      "Iteration 29, loss = 0.57491590\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.740260\n",
      "Iteration 2, loss = 0.58720150\n",
      "Iteration 30, loss = 0.57490055\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.740260\n",
      "Iteration 15, loss = 0.57707206\n",
      "Validation score: 0.730769\n",
      "Iteration 3, loss = 0.58050692\n",
      "Validation score: 0.743590Iteration 31, loss = 0.57489856\n",
      "\n",
      "Validation score: 0.740260\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "Iteration 16, loss = 0.57699398\n",
      "Validation score: 0.730769\n",
      "Iteration 4, loss = 0.58166988\n",
      "Validation score: 0.743590\n",
      "Iteration 32, loss = 0.57489422\n",
      "Validation score: 0.740260\n",
      "Iteration 17, loss = 0.57694215\n",
      "Iteration 5, loss = 0.58923857\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Iteration 33, loss = 0.57489923\n",
      "Validation score: 0.740260\n",
      "Iteration 6, loss = 0.57791342\n",
      "Validation score: 0.743590\n",
      "Iteration 18, loss = 0.57664674\n",
      "Iteration 34, loss = 0.57489796\n",
      "Iteration 7, loss = 0.57689233\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "Iteration 19, loss = 0.57689207Iteration 35, loss = 0.57489394\n",
      "\n",
      "Validation score: 0.740260Iteration 8, loss = 0.58097087\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 20, loss = 0.57660555\n",
      "Iteration 36, loss = 0.57489482\n",
      "Iteration 9, loss = 0.57687024\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.743590\n",
      "Iteration 10, loss = 0.57547644\n",
      "Validation score: 0.743590\n",
      "Iteration 37, loss = 0.57489369\n",
      "Validation score: 0.740260\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 11, loss = 0.57675316\n",
      "Iteration 21, loss = 0.57658532\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Iteration 38, loss = 0.57489163\n",
      "Validation score: 0.740260\n",
      "Iteration 12, loss = 0.57529998\n",
      "Iteration 22, loss = 0.57655269\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Iteration 39, loss = 0.57489157\n",
      "Validation score: 0.740260\n",
      "Iteration 13, loss = 0.57522793\n",
      "Iteration 23, loss = 0.57654736\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "Validation score: 0.730769\n",
      "Iteration 40, loss = 0.57489128\n",
      "Iteration 14, loss = 0.57469195\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.743590\n",
      "Iteration 41, loss = 0.57489098\n",
      "Iteration 15, loss = 0.57438789\n",
      "Validation score: 0.740260\n",
      "Iteration 24, loss = 0.57669861\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Iteration 42, loss = 0.57489152\n",
      "Iteration 16, loss = 0.57439864\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.743590\n",
      "Iteration 25, loss = 0.57654301\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "Iteration 17, loss = 0.57475595\n",
      "Validation score: 0.743590\n",
      "Iteration 26, loss = 0.57650266\n",
      "Iteration 18, loss = 0.57455174Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 27, loss = 0.57649506Iteration 19, loss = 0.57478147\n",
      "\n",
      "Validation score: 0.743590Validation score: 0.730769\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292\n",
      "Iteration 28, loss = 0.57649444\n",
      "Iteration 20, loss = 0.57420369\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Iteration 21, loss = 0.57423027\n",
      "Validation score: 0.743590Iteration 43, loss = 0.57489134\n",
      "\n",
      "Validation score: 0.740260Iteration 29, loss = 0.57651226\n",
      "Validation score: 0.730769\n",
      "Iteration 22, loss = 0.57420154\n",
      "Validation score: 0.743590\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 23, loss = 0.57418568\n",
      "Validation score: 0.743590\n",
      "Iteration 44, loss = 0.57489062\n",
      "Validation score: 0.740260\n",
      "Iteration 24, loss = 0.57429548\n",
      "Validation score: 0.743590\n",
      "Iteration 45, loss = 0.57489064Iteration 25, loss = 0.57424253\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "\n",
      "Validation score: 0.740260Iteration 26, loss = 0.57415345\n",
      "Validation score: 0.743590\n",
      "Iteration 30, loss = 0.57649523\n",
      "Validation score: 0.730769\n",
      "Iteration 27, loss = 0.57415013\n",
      "\n",
      "Iteration 31, loss = 0.57648182\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "Iteration 28, loss = 0.57416675Iteration 46, loss = 0.57489072\n",
      "Validation score: 0.740260\n",
      "Iteration 32, loss = 0.57647511\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 47, loss = 0.57489066Iteration 29, loss = 0.57415040\n",
      "Validation score: 0.740260\n",
      "Iteration 33, loss = 0.57647517\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 48, loss = 0.57489071Iteration 34, loss = 0.57647537\n",
      "Validation score: 0.730769\n",
      "Iteration 30, loss = 0.57416415\n",
      "\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.743590\n",
      "Iteration 35, loss = 0.57647412\n",
      "Iteration 49, loss = 0.57489051\n",
      "Validation score: 0.730769\n",
      "Iteration 31, loss = 0.57415227Validation score: 0.740260\n",
      "Iteration 36, loss = 0.57647517Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Validation score: 0.730769\n",
      "Iteration 32, loss = 0.57414092Iteration 37, loss = 0.57647444\n",
      "\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "Validation score: 0.743590\n",
      "Iteration 1, loss = 0.63210039\n",
      "Iteration 33, loss = 0.57413896\n",
      "Iteration 38, loss = 0.57647313\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769Iteration 39, loss = 0.57647222\n",
      "Iteration 34, loss = 0.57413956\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.743590Iteration 2, loss = 0.59298639\n",
      "Validation score: 0.730769Iteration 40, loss = 0.57647217\n",
      "\n",
      "\n",
      "Validation score: 0.730769Iteration 35, loss = 0.57414001\n",
      "Iteration 3, loss = 0.58003227\n",
      "Validation score: 0.730769\n",
      "Iteration 41, loss = 0.57647178\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 4, loss = 0.58126839\n",
      "Iteration 36, loss = 0.57413852Validation score: 0.730769\n",
      "Iteration 42, loss = 0.57647208\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Iteration 5, loss = 0.58977529\n",
      "Iteration 37, loss = 0.57414001\n",
      "\n",
      "Validation score: 0.743590Validation score: 0.730769Iteration 43, loss = 0.57647187\n",
      "\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 6, loss = 0.58698580\n",
      "Validation score: 0.730769\n",
      "Iteration 44, loss = 0.57647157Iteration 7, loss = 0.58392738\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 45, loss = 0.57647156\n",
      "Iteration 38, loss = 0.57413516\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "Validation score: 0.743590\n",
      "\n",
      "Iteration 39, loss = 0.57413476\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Iteration 8, loss = 0.57825452\n",
      "Validation score: 0.730769\n",
      "Iteration 9, loss = 0.57782626Iteration 46, loss = 0.57647154\n",
      "Validation score: 0.730769\n",
      "Iteration 47, loss = 0.57647151\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.730769Iteration 10, loss = 0.57615800\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 11, loss = 0.57905009Iteration 48, loss = 0.57647148\n",
      "Validation score: 0.730769\n",
      "Iteration 40, loss = 0.57413536\n",
      "Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.730769Iteration 41, loss = 0.57413496\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 49, loss = 0.57647150\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 12, loss = 0.57835955\n",
      "Validation score: 0.730769\n",
      "Iteration 13, loss = 0.57524934\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "Iteration 1, loss = 0.60976385\n",
      "Iteration 42, loss = 0.57413520Validation score: 0.743590\n",
      "Iteration 14, loss = 0.57458557\n",
      "Validation score: 0.730769\n",
      "Iteration 2, loss = 0.58345783\n",
      "Validation score: 0.743590\n",
      "Iteration 15, loss = 0.57482460\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 1, loss = 0.58468034\n",
      "Iteration 3, loss = 0.58495446\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 43, loss = 0.57413579\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 16, loss = 0.57457147\n",
      "Iteration 4, loss = 0.58044254\n",
      "Validation score: 0.730769Iteration 2, loss = 0.58276759\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 44, loss = 0.57413466\n",
      "Validation score: 0.743590\n",
      "Iteration 17, loss = 0.57455922\n",
      "Validation score: 0.730769\n",
      "Iteration 45, loss = 0.57413469\n",
      "Validation score: 0.743590Iteration 3, loss = 0.59104258\n",
      "Validation score: 0.743590\n",
      "Iteration 18, loss = 0.57473520\n",
      "Validation score: 0.730769\n",
      "Iteration 4, loss = 0.58264731\n",
      "Validation score: 0.743590\n",
      "Iteration 19, loss = 0.57469155\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292\n",
      "Iteration 5, loss = 0.58441509\n",
      "Validation score: 0.743590\n",
      "Iteration 20, loss = 0.57430269\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.58151316\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 21, loss = 0.57432063\n",
      "Iteration 5, loss = 0.59104086\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Iteration 46, loss = 0.57413463\n",
      "Validation score: 0.743590\n",
      "Iteration 7, loss = 0.59234884\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "Iteration 47, loss = 0.57413473\n",
      "Iteration 22, loss = 0.57429831\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Iteration 8, loss = 0.57726204\n",
      "Iteration 48, loss = 0.57413461\n",
      "Validation score: 0.743590\n",
      "Iteration 6, loss = 0.58510798\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 23, loss = 0.57432431\n",
      "Validation score: 0.730769\n",
      "Iteration 49, loss = 0.57413485\n",
      "Iteration 9, loss = 0.57622938\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Validation score: 0.743590\n",
      "Iteration 24, loss = 0.57426073\n",
      "Validation score: 0.730769\n",
      "Iteration 10, loss = 0.57729810\n",
      "Validation score: 0.743590\n",
      "Iteration 25, loss = 0.57426996\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "Iteration 11, loss = 0.57641562\n",
      "Iteration 7, loss = 0.59016904\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590Iteration 26, loss = 0.57425425\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "Validation score: 0.730769\n",
      "Iteration 12, loss = 0.57995777\n",
      "Validation score: 0.743590\n",
      "Iteration 27, loss = 0.57426124Iteration 8, loss = 0.58028035\n",
      "\n",
      "Validation score: 0.743590Validation score: 0.730769\n",
      "\n",
      "Iteration 13, loss = 0.57701825\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "Iteration 14, loss = 0.57579945\n",
      "Validation score: 0.743590\n",
      "Iteration 9, loss = 0.58049731Iteration 28, loss = 0.57425284\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Iteration 15, loss = 0.57605947\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 1, loss = 0.60439227\n",
      "Validation score: 0.743590\n",
      "Iteration 29, loss = 0.57425274\n",
      "Iteration 16, loss = 0.57590346\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Iteration 10, loss = 0.57813004\n",
      "Validation score: 0.743590\n",
      "Iteration 17, loss = 0.57605341\n",
      "Iteration 30, loss = 0.57424534\n",
      "Validation score: 0.743590\n",
      "Iteration 11, loss = 0.57785615\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Iteration 31, loss = 0.57424088\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "Iteration 18, loss = 0.57566632\n",
      "Validation score: 0.743590\n",
      "Iteration 2, loss = 0.58580445\n",
      "Validation score: 0.743590\n",
      "Iteration 32, loss = 0.57423347\n",
      "Validation score: 0.730769Iteration 19, loss = 0.57570119\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292\n",
      "Iteration 3, loss = 0.58024819\n",
      "Validation score: 0.743590\n",
      "Iteration 33, loss = 0.57423275\n",
      "Iteration 12, loss = 0.57885997\n",
      "Validation score: 0.730769\n",
      "Iteration 20, loss = 0.57567009\n",
      "Validation score: 0.743590\n",
      "Iteration 4, loss = 0.58864971\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 34, loss = 0.57423502\n",
      "Iteration 13, loss = 0.57624134\n",
      "Validation score: 0.743590Validation score: 0.730769\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "Iteration 5, loss = 0.58423472\n",
      "Iteration 21, loss = 0.57562020\n",
      "Validation score: 0.743590Iteration 1, loss = 0.58751167\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 35, loss = 0.57423353\n",
      "Validation score: 0.730769\n",
      "Iteration 2, loss = 0.58597130\n",
      "Iteration 22, loss = 0.57559554\n",
      "Validation score: 0.743590\n",
      "Iteration 36, loss = 0.57423329Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 14, loss = 0.57585350\n",
      "Validation score: 0.743590\n",
      "Iteration 3, loss = 0.58629750\n",
      "Iteration 23, loss = 0.57560059\n",
      "Validation score: 0.743590\n",
      "Iteration 37, loss = 0.57423268\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 15, loss = 0.57576441\n",
      "Validation score: 0.743590\n",
      "\n",
      "Iteration 24, loss = 0.57568465Iteration 4, loss = 0.58056919\n",
      "\n",
      "Iteration 38, loss = 0.57423072\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.57954047\n",
      "Validation score: 0.743590\n",
      "Iteration 39, loss = 0.57423102\n",
      "Iteration 5, loss = 0.58379392\n",
      "Iteration 25, loss = 0.57568763\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "Iteration 7, loss = 0.58422648\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "Iteration 26, loss = 0.57555594\n",
      "Iteration 40, loss = 0.57423127\n",
      "Iteration 16, loss = 0.57592147\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Iteration 8, loss = 0.58264598Iteration 27, loss = 0.57555075\n",
      "Iteration 41, loss = 0.57423055\n",
      "Iteration 17, loss = 0.57582271Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Iteration 6, loss = 0.58644951\n",
      "Validation score: 0.743590\n",
      "Iteration 42, loss = 0.57423050\n",
      "Validation score: 0.730769\n",
      "Iteration 28, loss = 0.57554514\n",
      "Iteration 7, loss = 0.59017729\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "Iteration 43, loss = 0.57423125\n",
      "\n",
      "Validation score: 0.730769\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Validation score: 0.743590\n",
      "Iteration 29, loss = 0.57554436\n",
      "Iteration 8, loss = 0.57621491\n",
      "Validation score: 0.743590\n",
      "Iteration 44, loss = 0.57423026\n",
      "\n",
      "Iteration 9, loss = 0.57960783\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 30, loss = 0.57554427\n",
      "Iteration 9, loss = 0.57601896\n",
      "Iteration 45, loss = 0.57423023\n",
      "Validation score: 0.743590Iteration 18, loss = 0.57583940\n",
      "\n",
      "Iteration 10, loss = 0.57891262Validation score: 0.743590Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 31, loss = 0.57554699\n",
      "Iteration 46, loss = 0.57423034\n",
      "Iteration 10, loss = 0.57566723\n",
      "Iteration 19, loss = 0.57583349\n",
      "Validation score: 0.730769\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292\n",
      "Iteration 20, loss = 0.57554869\n",
      "Iteration 11, loss = 0.57744427\n",
      "Validation score: 0.743590Iteration 32, loss = 0.57553427\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 47, loss = 0.57423042Iteration 21, loss = 0.57555321\n",
      "Iteration 12, loss = 0.57670039\n",
      "Validation score: 0.743590\n",
      "Iteration 33, loss = 0.57553422\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 11, loss = 0.57949257\n",
      "Validation score: 0.743590\n",
      "Iteration 22, loss = 0.57555558\n",
      "\n",
      "Iteration 13, loss = 0.57551676Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.730769\n",
      "Iteration 34, loss = 0.57553446Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "\n",
      "Validation score: 0.743590Iteration 12, loss = 0.57999560\n",
      "\n",
      "Validation score: 0.743590Iteration 23, loss = 0.57562439\n",
      "Iteration 48, loss = 0.57423027\n",
      "Validation score: 0.743590Iteration 14, loss = 0.57513319\n",
      "Validation score: 0.730769\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 35, loss = 0.57553332\n",
      "Validation score: 0.743590\n",
      "\n",
      "Iteration 24, loss = 0.57556895\n",
      "Validation score: 0.743590\n",
      "Iteration 15, loss = 0.57538953\n",
      "Validation score: 0.743590\n",
      "Iteration 13, loss = 0.57805586\n",
      "Validation score: 0.743590Iteration 36, loss = 0.57553314\n",
      "Iteration 25, loss = 0.57554202\n",
      "Iteration 49, loss = 0.57423034\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "Validation score: 0.730769\n",
      "Iteration 16, loss = 0.57529219\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Validation score: 0.743590\n",
      "Iteration 26, loss = 0.57554024Iteration 37, loss = 0.57553375\n",
      "\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 17, loss = 0.57564638\n",
      "Validation score: 0.743590\n",
      "Iteration 27, loss = 0.57550526\n",
      "Iteration 38, loss = 0.57553033\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 18, loss = 0.57541595\n",
      "Validation score: 0.743590\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "Iteration 28, loss = 0.57550513\n",
      "Validation score: 0.743590\n",
      "Iteration 39, loss = 0.57553122\n",
      "Validation score: 0.743590\n",
      "Iteration 19, loss = 0.57553285\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292\n",
      "Iteration 29, loss = 0.57551047\n",
      "Iteration 14, loss = 0.57689626\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 40, loss = 0.57553045\n",
      "Iteration 20, loss = 0.57510394\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 30, loss = 0.57553245\n",
      "Validation score: 0.743590\n",
      "Iteration 15, loss = 0.57716965\n",
      "Validation score: 0.743590\n",
      "Iteration 41, loss = 0.57553019\n",
      "Iteration 21, loss = 0.57511850\n",
      "Validation score: 0.743590\n",
      "Iteration 31, loss = 0.57550625\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "Iteration 16, loss = 0.57724480\n",
      "Iteration 22, loss = 0.57505216Iteration 42, loss = 0.57553030\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 32, loss = 0.57550168\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 23, loss = 0.57510395Iteration 43, loss = 0.57553112\n",
      "\n",
      "Iteration 33, loss = 0.57550331\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Validation score: 0.743590\n",
      "Iteration 24, loss = 0.57510649\n",
      "Iteration 34, loss = 0.57550188\n",
      "Iteration 44, loss = 0.57553005\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 35, loss = 0.57550309\n",
      "Iteration 25, loss = 0.57504795\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590Iteration 45, loss = 0.57553008\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "Validation score: 0.743590\n",
      "Iteration 17, loss = 0.57725719\n",
      "Iteration 36, loss = 0.57550104\n",
      "Validation score: 0.743590\n",
      "Iteration 26, loss = 0.57501614\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 46, loss = 0.57553007\n",
      "Validation score: 0.743590\n",
      "Iteration 18, loss = 0.57713085\n",
      "Iteration 37, loss = 0.57550045\n",
      "Validation score: 0.743590\n",
      "Iteration 27, loss = 0.57499987\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "Validation score: 0.743590\n",
      "Iteration 47, loss = 0.57553004\n",
      "Iteration 19, loss = 0.57680584Iteration 38, loss = 0.57549955\n",
      "Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292\n",
      "Iteration 28, loss = 0.57500305\n",
      "Validation score: 0.743590\n",
      "Iteration 48, loss = 0.57552994\n",
      "Iteration 39, loss = 0.57549998\n",
      "Iteration 20, loss = 0.57676500\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 29, loss = 0.57500725\n",
      "Validation score: 0.743590\n",
      "Iteration 40, loss = 0.57549956\n",
      "Iteration 49, loss = 0.57552996\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 30, loss = 0.57501565\n",
      "Validation score: 0.743590\n",
      "Iteration 41, loss = 0.57549975\n",
      "Validation score: 0.743590\n",
      "Iteration 31, loss = 0.57500589\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "Iteration 42, loss = 0.57549968\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 21, loss = 0.57675542\n",
      "Validation score: 0.743590Iteration 1, loss = 0.59224028\n",
      "\n",
      "Iteration 43, loss = 0.57549924\n",
      "Iteration 32, loss = 0.57499423\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 22, loss = 0.57671070Iteration 44, loss = 0.57549902\n",
      "Iteration 33, loss = 0.57499694\n",
      "Validation score: 0.743590\n",
      "Iteration 2, loss = 0.58752796\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "\n",
      "Iteration 45, loss = 0.57549908\n",
      "Validation score: 0.743590\n",
      "Iteration 34, loss = 0.57499613\n",
      "Validation score: 0.743590\n",
      "Iteration 3, loss = 0.58543210Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 23, loss = 0.57679241Iteration 46, loss = 0.57549904\n",
      "Iteration 35, loss = 0.57499847\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 4, loss = 0.58963776\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 47, loss = 0.57549898Validation score: 0.743590\n",
      "Iteration 36, loss = 0.57499577\n",
      "Validation score: 0.743590\n",
      "Iteration 5, loss = 0.58592320\n",
      "Iteration 24, loss = 0.57669993Validation score: 0.743590\n",
      "Iteration 37, loss = 0.57499556\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "\n",
      "Validation score: 0.743590\n",
      "\n",
      "Iteration 6, loss = 0.58439739\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 38, loss = 0.57499309\n",
      "Iteration 48, loss = 0.57549901\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Iteration 7, loss = 0.58646512\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "Iteration 49, loss = 0.57549901Iteration 39, loss = 0.57499335\n",
      "Validation score: 0.743590\n",
      "Iteration 8, loss = 0.58065526\n",
      "Validation score: 0.743590\n",
      "Iteration 25, loss = 0.57671865\n",
      "Iteration 40, loss = 0.57499311\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "Validation score: 0.743590\n",
      "Iteration 41, loss = 0.57499312\n",
      "Iteration 9, loss = 0.57945591\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 42, loss = 0.57499306\n",
      "Iteration 10, loss = 0.58128200\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 43, loss = 0.57499301\n",
      "Iteration 11, loss = 0.57795960\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 26, loss = 0.57667096\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 44, loss = 0.57499280\n",
      "Iteration 12, loss = 0.57903261\n",
      "Validation score: 0.743590\n",
      "Iteration 27, loss = 0.57666652\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 45, loss = 0.57499281\n",
      "Iteration 28, loss = 0.57667739\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 13, loss = 0.57902441\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "Iteration 29, loss = 0.57667796Iteration 46, loss = 0.57499285Iteration 14, loss = 0.57807722\n",
      "\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Iteration 47, loss = 0.57499287\n",
      "Iteration 15, loss = 0.57724623\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 48, loss = 0.57499282\n",
      "Iteration 16, loss = 0.57724834\n",
      "Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 49, loss = 0.57499273\n",
      "Iteration 30, loss = 0.57666963\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 17, loss = 0.57730054\n",
      "Validation score: 0.743590\n",
      "Iteration 31, loss = 0.57667361Iteration 18, loss = 0.57707850\n",
      "Validation score: 0.743590\n",
      "Iteration 1, loss = 0.60819637\n",
      "\n",
      "Iteration 19, loss = 0.57708305\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292\n",
      "Iteration 32, loss = 0.57666209Iteration 2, loss = 0.58498705\n",
      "Validation score: 0.743590Iteration 20, loss = 0.57698289\n",
      "\n",
      "Validation score: 0.743590\n",
      "\n",
      "Iteration 3, loss = 0.58055663\n",
      "Validation score: 0.743590\n",
      "Iteration 21, loss = 0.57693298\n",
      "Validation score: 0.743590\n",
      "Iteration 4, loss = 0.58352146\n",
      "Validation score: 0.743590\n",
      "Iteration 22, loss = 0.57699590\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Iteration 5, loss = 0.58139226\n",
      "Validation score: 0.743590\n",
      "Iteration 33, loss = 0.57666470\n",
      "Validation score: 0.743590\n",
      "Iteration 23, loss = 0.57701121\n",
      "Validation score: 0.743590\n",
      "Iteration 6, loss = 0.58626159\n",
      "Validation score: 0.743590\n",
      "Iteration 24, loss = 0.57698651\n",
      "Validation score: 0.743590\n",
      "Iteration 7, loss = 0.59036818\n",
      "Iteration 34, loss = 0.57666196Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.007309\n",
      "Iteration 25, loss = 0.57699633\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "Iteration 8, loss = 0.58170791\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 26, loss = 0.57690694\n",
      "Validation score: 0.743590\n",
      "Iteration 9, loss = 0.58026111\n",
      "Validation score: 0.743590\n",
      "Iteration 27, loss = 0.57690497\n",
      "Validation score: 0.743590\n",
      "Iteration 10, loss = 0.57827353\n",
      "Validation score: 0.743590\n",
      "Iteration 28, loss = 0.57690922\n",
      "Iteration 35, loss = 0.57666386\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590Iteration 11, loss = 0.58014486\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 29, loss = 0.57691257\n",
      "Iteration 36, loss = 0.57666254Iteration 12, loss = 0.58101819\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 30, loss = 0.57691483\n",
      "Iteration 13, loss = 0.57992487\n",
      "Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.001462\n",
      "Iteration 31, loss = 0.57691539\n",
      "Iteration 14, loss = 0.57808191\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "Iteration 37, loss = 0.57666005Iteration 15, loss = 0.57786008\n",
      "\n",
      "Iteration 32, loss = 0.57690507\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "\n",
      "Iteration 38, loss = 0.57666065Iteration 16, loss = 0.57729536Iteration 33, loss = 0.57690161\n",
      "\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 34, loss = 0.57689919\n",
      "Validation score: 0.743590\n",
      "Iteration 17, loss = 0.57727178\n",
      "Validation score: 0.743590Iteration 39, loss = 0.57666014\n",
      "\n",
      "Validation score: 0.743590Iteration 35, loss = 0.57690038\n",
      "Iteration 18, loss = 0.57740065\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Iteration 36, loss = 0.57690030\n",
      "Iteration 19, loss = 0.57719255\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000292\n",
      "\n",
      "Iteration 37, loss = 0.57690174\n",
      "Iteration 20, loss = 0.57713176Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "\n",
      "Validation score: 0.743590\n",
      "Iteration 40, loss = 0.57665987\n",
      "Validation score: 0.743590\n",
      "Iteration 38, loss = 0.57689869\n",
      "Validation score: 0.743590\n",
      "Iteration 21, loss = 0.57702968\n",
      "Iteration 41, loss = 0.57666005Validation score: 0.743590\n",
      "Iteration 39, loss = 0.57689844\n",
      "Validation score: 0.743590\n",
      "Iteration 22, loss = 0.57707217\n",
      "Validation score: 0.743590\n",
      "Iteration 40, loss = 0.57689870\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 42, loss = 0.57665979\n",
      "Iteration 41, loss = 0.57689910\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 23, loss = 0.57702708\n",
      "Validation score: 0.743590\n",
      "Iteration 42, loss = 0.57689901\n",
      "Iteration 43, loss = 0.57665984\n",
      "Validation score: 0.743590\n",
      "Iteration 24, loss = 0.57703432\n",
      "Iteration 43, loss = 0.57689831\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 44, loss = 0.57689819\n",
      "Iteration 44, loss = 0.57665967\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 45, loss = 0.57665980\n",
      "Iteration 45, loss = 0.57689817\n",
      "Iteration 25, loss = 0.57699730\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000058\n",
      "Iteration 46, loss = 0.57689820\n",
      "Iteration 26, loss = 0.57695560\n",
      "Validation score: 0.743590\n",
      "Iteration 46, loss = 0.57665954\n",
      "Validation score: 0.743590\n",
      "Validation score: 0.743590\n",
      "Iteration 47, loss = 0.57689818\n",
      "Validation score: 0.743590\n",
      "Iteration 27, loss = 0.57696546\n",
      "Validation score: 0.743590\n",
      "Iteration 47, loss = 0.57665967\n",
      "Validation score: 0.743590\n",
      "Iteration 48, loss = 0.57689831\n",
      "Iteration 28, loss = 0.57695327\n",
      "Validation score: 0.743590Validation score: 0.743590\n",
      "\n",
      "Iteration 48, loss = 0.57665958\n",
      "Validation score: 0.743590\n",
      "Iteration 49, loss = 0.57689819Iteration 29, loss = 0.57696936\n",
      "\n",
      "Iteration 49, loss = 0.57665962Validation score: 0.743590\n",
      "\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 30, loss = 0.57695487\n",
      "Validation score: 0.743590\n",
      "Iteration 31, loss = 0.57696994\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000012\n",
      "Iteration 32, loss = 0.57694789\n",
      "Validation score: 0.743590\n",
      "Iteration 33, loss = 0.57694777\n",
      "Validation score: 0.743590\n",
      "Iteration 34, loss = 0.57694738\n",
      "Validation score: 0.743590\n",
      "Iteration 35, loss = 0.57694433\n",
      "Validation score: 0.743590\n",
      "Iteration 36, loss = 0.57694607\n",
      "Validation score: 0.743590\n",
      "Iteration 37, loss = 0.57694864\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 38, loss = 0.57694366\n",
      "Validation score: 0.743590\n",
      "Iteration 39, loss = 0.57694380\n",
      "Validation score: 0.743590\n",
      "Iteration 40, loss = 0.57694448\n",
      "Validation score: 0.743590\n",
      "Iteration 41, loss = 0.57694389\n",
      "Validation score: 0.743590\n",
      "Iteration 42, loss = 0.57694423\n",
      "Validation score: 0.743590\n",
      "Iteration 43, loss = 0.57694402\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 44, loss = 0.57694345\n",
      "Validation score: 0.743590\n",
      "Iteration 45, loss = 0.57694365\n",
      "Validation score: 0.743590\n",
      "Iteration 46, loss = 0.57694363\n",
      "Validation score: 0.743590\n",
      "Iteration 47, loss = 0.57694341\n",
      "Validation score: 0.743590\n",
      "Iteration 48, loss = 0.57694340\n",
      "Validation score: 0.743590\n",
      "Iteration 49, loss = 0.57694339\n",
      "Validation score: 0.743590\n",
      "Validation score did not improve more than tol=0.000948 for 5 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 0.57458588\n",
      "Validation score: 0.750000\n",
      "Iteration 2, loss = 0.52410928\n",
      "Validation score: 0.763158\n",
      "Iteration 3, loss = 0.49610336\n",
      "Validation score: 0.763158\n",
      "Iteration 1, loss = 0.59013440\n",
      "Validation score: 0.727273\n",
      "Iteration 4, loss = 0.48658352\n",
      "Validation score: 0.776316\n",
      "Iteration 5, loss = 0.49795571\n",
      "Iteration 2, loss = 0.54496619\n",
      "Validation score: 0.766234\n",
      "Iteration 3, loss = 0.51748829\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.763158\n",
      "Iteration 4, loss = 0.49993620\n",
      "Validation score: 0.727273\n",
      "Iteration 6, loss = 0.48709210\n",
      "Validation score: 0.763158\n",
      "Iteration 5, loss = 0.48782399\n",
      "Validation score: 0.727273\n",
      "Iteration 7, loss = 0.47403559\n",
      "Validation score: 0.763158\n",
      "Iteration 6, loss = 0.46736181\n",
      "Validation score: 0.779221\n",
      "Iteration 8, loss = 0.46917725\n",
      "Validation score: 0.750000\n",
      "Iteration 1, loss = 0.63780863\n",
      "Validation score: 0.740260\n",
      "Iteration 9, loss = 0.45011358\n",
      "Iteration 7, loss = 0.46579526\n",
      "Validation score: 0.750000\n",
      "Validation score: 0.753247\n",
      "Iteration 2, loss = 0.55170786\n",
      "Validation score: 0.740260\n",
      "Iteration 8, loss = 0.46397188\n",
      "Iteration 10, loss = 0.46072962\n",
      "Validation score: 0.750000\n",
      "Validation score: 0.753247\n",
      "Iteration 3, loss = 0.53666025\n",
      "Iteration 11, loss = 0.45288311\n",
      "Validation score: 0.779221\n",
      "Validation score: 0.776316\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.Iteration 9, loss = 0.44996475\n",
      "\n",
      "Validation score: 0.740260\n",
      "Iteration 4, loss = 0.53625557\n",
      "Validation score: 0.818182\n",
      "Iteration 10, loss = 0.47408107\n",
      "Validation score: 0.753247\n",
      "Iteration 5, loss = 0.52336792\n",
      "Validation score: 0.792208\n",
      "Iteration 1, loss = 0.57160816\n",
      "Validation score: 0.727273\n",
      "Iteration 11, loss = 0.45527916\n",
      "Validation score: 0.766234Iteration 6, loss = 0.51540066\n",
      "\n",
      "Validation score: 0.779221\n",
      "Iteration 2, loss = 0.56284667\n",
      "Validation score: 0.727273\n",
      "Iteration 12, loss = 0.44635668\n",
      "Iteration 7, loss = 0.51257147\n",
      "Validation score: 0.766234\n",
      "Validation score: 0.792208\n",
      "Iteration 3, loss = 0.52746254\n",
      "Validation score: 0.727273\n",
      "Iteration 13, loss = 0.43536017\n",
      "Iteration 8, loss = 0.50511045\n",
      "Validation score: 0.753247Validation score: 0.779221\n",
      "\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.51482443\n",
      "Validation score: 0.727273\n",
      "Iteration 9, loss = 0.51299960\n",
      "Validation score: 0.831169\n",
      "Iteration 5, loss = 0.50732569\n",
      "Validation score: 0.701299\n",
      "Iteration 10, loss = 0.48100539\n",
      "Validation score: 0.766234\n",
      "Iteration 6, loss = 0.49305322\n",
      "Validation score: 0.701299\n",
      "Iteration 11, loss = 0.51590912Iteration 1, loss = 0.64245766\n",
      "\n",
      "Validation score: 0.792208Validation score: 0.740260\n",
      "\n",
      "Iteration 7, loss = 0.49487010\n",
      "Validation score: 0.714286\n",
      "Iteration 12, loss = 0.49974426\n",
      "Iteration 2, loss = 0.52700220\n",
      "Validation score: 0.792208\n",
      "Validation score: 0.701299\n",
      "Iteration 8, loss = 0.48731351\n",
      "Validation score: 0.701299\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.49457069\n",
      "Iteration 3, loss = 0.48589028\n",
      "Validation score: 0.792208\n",
      "Validation score: 0.714286\n",
      "Iteration 14, loss = 0.49372055\n",
      "Validation score: 0.779221\n",
      "Iteration 4, loss = 0.48069448\n",
      "Validation score: 0.701299\n",
      "Iteration 15, loss = 0.50720953\n",
      "Validation score: 0.779221\n",
      "Iteration 5, loss = 0.44521909\n",
      "Validation score: 0.675325\n",
      "Iteration 16, loss = 0.50815497\n",
      "Validation score: 0.792208\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.44327580\n",
      "Validation score: 0.662338\n",
      "Iteration 1, loss = 0.60625366\n",
      "Validation score: 0.727273\n",
      "Iteration 7, loss = 0.44224418\n",
      "Validation score: 0.714286\n",
      "Iteration 1, loss = 0.59769477\n",
      "Iteration 2, loss = 0.55719296Validation score: 0.779221\n",
      "\n",
      "Validation score: 0.753247\n",
      "Iteration 8, loss = 0.44543028\n",
      "Iteration 2, loss = 0.52047548\n",
      "Validation score: 0.714286\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.53995774\n",
      "Validation score: 0.779221\n",
      "Validation score: 0.766234\n",
      "Iteration 3, loss = 0.51135936\n",
      "Validation score: 0.740260\n",
      "Iteration 4, loss = 0.53287356\n",
      "Validation score: 0.753247\n",
      "Iteration 4, loss = 0.48793255\n",
      "Validation score: 0.740260\n",
      "Iteration 5, loss = 0.52316836\n",
      "Validation score: 0.740260\n",
      "Iteration 5, loss = 0.49921149\n",
      "Validation score: 0.740260\n",
      "Iteration 1, loss = 0.60245994\n",
      "Validation score: 0.740260\n",
      "Iteration 6, loss = 0.51437891\n",
      "Validation score: 0.766234\n",
      "Iteration 6, loss = 0.46644931\n",
      "Validation score: 0.714286\n",
      "Iteration 2, loss = 0.54296115\n",
      "Validation score: 0.753247\n",
      "Iteration 7, loss = 0.50302437\n",
      "Validation score: 0.727273\n",
      "Iteration 7, loss = 0.49313088\n",
      "Validation score: 0.740260\n",
      "Iteration 3, loss = 0.51384925\n",
      "Iteration 8, loss = 0.51435738\n",
      "Validation score: 0.779221\n",
      "Validation score: 0.779221\n",
      "Iteration 8, loss = 0.46657572\n",
      "Validation score: 0.740260\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.50172676\n",
      "Iteration 4, loss = 0.49536848\n",
      "Validation score: 0.740260\n",
      "Validation score: 0.792208\n",
      "Iteration 10, loss = 0.49834045Iteration 5, loss = 0.49235949\n",
      "\n",
      "Validation score: 0.727273Validation score: 0.740260\n",
      "\n",
      "Iteration 1, loss = 0.61013560\n",
      "Iteration 2, loss = 0.58638466\n",
      "Iteration 6, loss = 0.47493478\n",
      "Iteration 3, loss = 0.58000463\n",
      "Iteration 11, loss = 0.49270209\n",
      "Validation score: 0.727273\n",
      "Validation score: 0.753247\n",
      "Iteration 4, loss = 0.57783117\n",
      "Iteration 1, loss = 0.64843131\n",
      "Iteration 5, loss = 0.57802439\n",
      "Iteration 2, loss = 0.59788239Iteration 7, loss = 0.46892520\n",
      "\n",
      "Iteration 12, loss = 0.48931726\n",
      "Validation score: 0.701299\n",
      "Validation score: 0.766234\n",
      "Iteration 6, loss = 0.57767958\n",
      "Iteration 3, loss = 0.58364725\n",
      "Iteration 7, loss = 0.57811713\n",
      "Iteration 4, loss = 0.57965375\n",
      "Iteration 8, loss = 0.45888788\n",
      "Iteration 8, loss = 0.57787555\n",
      "Iteration 5, loss = 0.57983364\n",
      "Validation score: 0.779221\n",
      "Iteration 13, loss = 0.48785039\n",
      "Validation score: 0.753247\n",
      "Iteration 9, loss = 0.57895234\n",
      "Iteration 6, loss = 0.57946249\n",
      "Iteration 10, loss = 0.57760414\n",
      "Iteration 7, loss = 0.57968151\n",
      "Iteration 14, loss = 0.47984569Iteration 8, loss = 0.57974140\n",
      "\n",
      "Iteration 11, loss = 0.57718897\n",
      "Validation score: 0.766234Iteration 9, loss = 0.45723098\n",
      "\n",
      "Validation score: 0.766234\n",
      "Iteration 9, loss = 0.57998226\n",
      "Iteration 12, loss = 0.57740833\n",
      "Iteration 10, loss = 0.57954429\n",
      "Iteration 13, loss = 0.57709348\n",
      "Iteration 11, loss = 0.57943236\n",
      "Iteration 14, loss = 0.57699830\n",
      "Iteration 10, loss = 0.45526678\n",
      "Iteration 15, loss = 0.48608223\n",
      "Validation score: 0.753247\n",
      "Validation score: 0.779221\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.57876754\n",
      "Iteration 15, loss = 0.57909196\n",
      "Iteration 13, loss = 0.57893352\n",
      "Iteration 16, loss = 0.57726386\n",
      "Iteration 14, loss = 0.57868595\n",
      "Iteration 11, loss = 0.45285603\n",
      "Iteration 17, loss = 0.57784531\n",
      "Validation score: 0.740260\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.57877398\n",
      "Iteration 18, loss = 0.57721432\n",
      "Iteration 16, loss = 0.57892366\n",
      "Iteration 19, loss = 0.57728325\n",
      "Iteration 17, loss = 0.57866774\n",
      "Iteration 20, loss = 0.57679116\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "Iteration 18, loss = 0.57843130\n",
      "Iteration 21, loss = 0.57654870\n",
      "Iteration 19, loss = 0.58081315\n",
      "Iteration 22, loss = 0.57670930\n",
      "Iteration 20, loss = 0.57882445\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "Iteration 23, loss = 0.57654986\n",
      "Iteration 21, loss = 0.57832838\n",
      "Iteration 24, loss = 0.57650159\n",
      "Iteration 1, loss = 0.67160139\n",
      "Iteration 22, loss = 0.57821282\n",
      "Validation score: 0.740260\n",
      "Iteration 25, loss = 0.57654574\n",
      "Iteration 23, loss = 0.57821616\n",
      "Iteration 26, loss = 0.57657461\n",
      "Iteration 1, loss = 0.60870944\n",
      "Validation score: 0.740260\n",
      "Iteration 24, loss = 0.57819007\n",
      "Iteration 2, loss = 0.55531940\n",
      "Validation score: 0.766234Iteration 25, loss = 0.57816147\n",
      "\n",
      "Iteration 27, loss = 0.57646333\n",
      "Iteration 26, loss = 0.57833046\n",
      "Iteration 2, loss = 0.54913134\n",
      "Iteration 28, loss = 0.57642718\n",
      "Validation score: 0.818182\n",
      "Iteration 27, loss = 0.57816394\n",
      "Iteration 3, loss = 0.51701624\n",
      "Iteration 29, loss = 0.57640148\n",
      "Validation score: 0.766234\n",
      "Iteration 28, loss = 0.57811419\n",
      "Iteration 30, loss = 0.57650124\n",
      "Iteration 29, loss = 0.57819985\n",
      "Iteration 31, loss = 0.57636563\n",
      "Iteration 30, loss = 0.57804371\n",
      "Iteration 4, loss = 0.50887331\n",
      "Validation score: 0.753247\n",
      "Iteration 32, loss = 0.57646329\n",
      "Iteration 3, loss = 0.53800879Iteration 31, loss = 0.57802390\n",
      "\n",
      "Validation score: 0.805195\n",
      "Iteration 1, loss = 0.60697305\n",
      "Iteration 33, loss = 0.57634639\n",
      "Iteration 32, loss = 0.57862150\n",
      "Iteration 2, loss = 0.58704701\n",
      "Iteration 34, loss = 0.57639281Iteration 5, loss = 0.49451774\n",
      "\n",
      "Iteration 33, loss = 0.57802176\n",
      "Iteration 3, loss = 0.57981864\n",
      "Iteration 35, loss = 0.57632762\n",
      "Validation score: 0.740260\n",
      "Iteration 34, loss = 0.57825938\n",
      "Iteration 4, loss = 0.51081421\n",
      "Iteration 4, loss = 0.57839311Validation score: 0.805195\n",
      "Iteration 36, loss = 0.57629463\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Iteration 35, loss = 0.57796164\n",
      "Iteration 37, loss = 0.57626746\n",
      "Iteration 6, loss = 0.50370720\n",
      "Iteration 36, loss = 0.57794929\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Validation score: 0.766234\n",
      "Iteration 38, loss = 0.57627461Iteration 37, loss = 0.57793179\n",
      "\n",
      "Iteration 5, loss = 0.50417346\n",
      "Validation score: 0.792208\n",
      "Iteration 38, loss = 0.57789721\n",
      "Iteration 39, loss = 0.57627829\n",
      "Iteration 1, loss = 0.76164701\n",
      "\n",
      "Iteration 40, loss = 0.57626657Iteration 7, loss = 0.49564029\n",
      "\n",
      "Iteration 39, loss = 0.57789867\n",
      "Validation score: 0.779221\n",
      "Iteration 5, loss = 0.57814298\n",
      "Iteration 41, loss = 0.57626719\n",
      "Iteration 40, loss = 0.57789451\n",
      "Iteration 6, loss = 0.48663118\n",
      "Iteration 6, loss = 0.57720297\n",
      "Validation score: 0.805195\n",
      "Iteration 42, loss = 0.57625496\n",
      "Iteration 41, loss = 0.57792597\n",
      "Iteration 7, loss = 0.57715759\n",
      "Iteration 8, loss = 0.50430152\n",
      "Iteration 43, loss = 0.57625157Validation score: 0.753247\n",
      "Iteration 8, loss = 0.57741513\n",
      "\n",
      "Iteration 42, loss = 0.57788363\n",
      "Iteration 9, loss = 0.57737478\n",
      "Iteration 7, loss = 0.48288035Iteration 44, loss = 0.57624719\n",
      "\n",
      "Iteration 43, loss = 0.57788144\n",
      "Validation score: 0.779221\n",
      "Iteration 10, loss = 0.57735784Iteration 2, loss = 0.60459559\n",
      "Iteration 44, loss = 0.57787909\n",
      "Iteration 45, loss = 0.57626776\n",
      "Iteration 3, loss = 0.57925940\n",
      "Iteration 9, loss = 0.48410373\n",
      "Iteration 45, loss = 0.57789073\n",
      "Validation score: 0.779221\n",
      "Iteration 46, loss = 0.57625447\n",
      "Iteration 8, loss = 0.47170663\n",
      "Iteration 4, loss = 0.57774950\n",
      "Iteration 46, loss = 0.57788710\n",
      "Iteration 5, loss = 0.57485649Iteration 47, loss = 0.57624204\n",
      "\n",
      "Validation score: 0.792208Iteration 6, loss = 0.57375675\n",
      "\n",
      "Iteration 48, loss = 0.57623043\n",
      "Iteration 47, loss = 0.57786351\n",
      "Iteration 10, loss = 0.48760168\n",
      "Iteration 7, loss = 0.57344593\n",
      "Validation score: 0.740260\n",
      "Iteration 49, loss = 0.57622604\n",
      "Iteration 11, loss = 0.57697473\n",
      "Iteration 12, loss = 0.57687227\n",
      "Iteration 9, loss = 0.45853267\n",
      "Validation score: 0.818182\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.48201965\n",
      "Iteration 13, loss = 0.57705465\n",
      "Validation score: 0.753247\n",
      "Iteration 14, loss = 0.57686861\n",
      "Iteration 15, loss = 0.57666415\n",
      "Iteration 12, loss = 0.48225553\n",
      "Iteration 8, loss = 0.57334975\n",
      "Validation score: 0.753247\n",
      "Iteration 9, loss = 0.57315337\n",
      "Iteration 50, loss = 0.57633346\n",
      "Iteration 10, loss = 0.57342513\n",
      "\n",
      "Iteration 51, loss = 0.57624305\n",
      "Iteration 16, loss = 0.57657607\n",
      "Iteration 13, loss = 0.47280166\n",
      "Iteration 1, loss = 0.64845755\n",
      "Iteration 52, loss = 0.57621887\n",
      "Validation score: 0.740260\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369Iteration 48, loss = 0.57786142\n",
      "\n",
      "Iteration 17, loss = 0.57640535\n",
      "Iteration 2, loss = 0.58879012\n",
      "Iteration 49, loss = 0.57788645Iteration 18, loss = 0.57733875\n",
      "Iteration 53, loss = 0.57621423\n",
      "\n",
      "Iteration 1, loss = 0.57865058\n",
      "Iteration 3, loss = 0.57514963\n",
      "Iteration 19, loss = 0.57634174\n",
      "Iteration 2, loss = 0.57591730Iteration 50, loss = 0.57785400\n",
      "Iteration 54, loss = 0.57623458\n",
      "Iteration 14, loss = 0.46674140\n",
      "Iteration 20, loss = 0.57639739Iteration 4, loss = 0.57360600\n",
      "Validation score: 0.727273\n",
      "Validation score did not improve more than tol=0.000356 for 6 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.57785210\n",
      "Iteration 11, loss = 0.57400590\n",
      "Iteration 55, loss = 0.57621999\n",
      "Iteration 5, loss = 0.57330649\n",
      "Iteration 52, loss = 0.57791713\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369\n",
      "Iteration 12, loss = 0.57325705\n",
      "Iteration 56, loss = 0.57620798\n",
      "\n",
      "Iteration 13, loss = 0.57302389Iteration 6, loss = 0.57413029\n",
      "Iteration 53, loss = 0.57785008\n",
      "\n",
      "Iteration 3, loss = 0.57526799Iteration 57, loss = 0.57621441\n",
      "Iteration 54, loss = 0.57784866\n",
      "Iteration 21, loss = 0.57681678\n",
      "Iteration 58, loss = 0.57620562\n",
      "Iteration 7, loss = 0.57440187\n",
      "Iteration 22, loss = 0.57651933Iteration 55, loss = 0.57784872\n",
      "\n",
      "Iteration 59, loss = 0.57620803\n",
      "Iteration 8, loss = 0.57291777\n",
      "Iteration 1, loss = 0.58708502Iteration 56, loss = 0.57784446\n",
      "\n",
      "Iteration 14, loss = 0.57286410\n",
      "\n",
      "Iteration 60, loss = 0.57620272\n",
      "Iteration 15, loss = 0.57289605\n",
      "Iteration 57, loss = 0.57784866\n",
      "Iteration 4, loss = 0.57530173Iteration 16, loss = 0.57333763\n",
      "\n",
      "Iteration 61, loss = 0.57620527\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "Iteration 58, loss = 0.57784564\n",
      "Iteration 62, loss = 0.57620109\n",
      "Iteration 5, loss = 0.57527597\n",
      "Iteration 23, loss = 0.57617665\n",
      "Iteration 63, loss = 0.57620076\n",
      "Iteration 59, loss = 0.57784020\n",
      "Iteration 6, loss = 0.57574395Iteration 24, loss = 0.57636031Iteration 60, loss = 0.57784006Iteration 64, loss = 0.57619740\n",
      "\n",
      "Iteration 17, loss = 0.57265894\n",
      "Iteration 61, loss = 0.57783874Iteration 18, loss = 0.57293688\n",
      "\n",
      "\n",
      "\n",
      "Iteration 65, loss = 0.57619711\n",
      "Iteration 9, loss = 0.57317298\n",
      "Iteration 25, loss = 0.57626314Iteration 62, loss = 0.57784275\n",
      "\n",
      "Iteration 7, loss = 0.57511635\n",
      "Iteration 66, loss = 0.57619511\n",
      "Iteration 10, loss = 0.57350426\n",
      "Iteration 26, loss = 0.57620332\n",
      "Iteration 63, loss = 0.57784008\n",
      "Iteration 8, loss = 0.57499552Iteration 67, loss = 0.57619586\n",
      "Iteration 11, loss = 0.57453498\n",
      "Iteration 64, loss = 0.57783525\n",
      "Iteration 19, loss = 0.57316720\n",
      "Iteration 68, loss = 0.57619388\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "Iteration 12, loss = 0.57312005\n",
      "Iteration 65, loss = 0.57783619\n",
      "Iteration 20, loss = 0.57333824\n",
      "Iteration 13, loss = 0.57303338Iteration 69, loss = 0.57619175\n",
      "\n",
      "Iteration 21, loss = 0.57280114Iteration 66, loss = 0.57783525\n",
      "Iteration 70, loss = 0.57619132Iteration 2, loss = 0.57557988\n",
      "\n",
      "Iteration 27, loss = 0.57613357\n",
      "Iteration 67, loss = 0.57783332\n",
      "Iteration 14, loss = 0.57327921\n",
      "Iteration 28, loss = 0.57610702\n",
      "Iteration 3, loss = 0.57272928\n",
      "Iteration 71, loss = 0.57618993\n",
      "Iteration 68, loss = 0.57783306\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "Iteration 15, loss = 0.57256481\n",
      "Iteration 29, loss = 0.57604181\n",
      "Iteration 72, loss = 0.57619429\n",
      "Iteration 22, loss = 0.57228954\n",
      "Iteration 16, loss = 0.57285405\n",
      "Iteration 69, loss = 0.57783090\n",
      "Iteration 73, loss = 0.57619017\n",
      "\n",
      "Iteration 30, loss = 0.57599647\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "Iteration 17, loss = 0.57264254Iteration 70, loss = 0.57783066\n",
      "\n",
      "Iteration 74, loss = 0.57619152Iteration 9, loss = 0.57525920\n",
      "\n",
      "Iteration 23, loss = 0.57236337\n",
      "Iteration 71, loss = 0.57783087\n",
      "Iteration 18, loss = 0.57254392\n",
      "Iteration 10, loss = 0.57562093\n",
      "Iteration 75, loss = 0.57618995\n",
      "Iteration 24, loss = 0.57235401\n",
      "Iteration 19, loss = 0.57344164\n",
      "Iteration 72, loss = 0.57783037\n",
      "Iteration 11, loss = 0.57506221\n",
      "Iteration 25, loss = 0.57223857\n",
      "Iteration 76, loss = 0.57618979\n",
      "Iteration 20, loss = 0.57236276Iteration 73, loss = 0.57783018\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "Iteration 12, loss = 0.57550136Iteration 31, loss = 0.57596121\n",
      "\n",
      "Iteration 77, loss = 0.57618942Iteration 74, loss = 0.57783002\n",
      "\n",
      "Iteration 32, loss = 0.57610327\n",
      "Iteration 13, loss = 0.57502550\n",
      "Iteration 4, loss = 0.57212324\n",
      "Iteration 21, loss = 0.57263644\n",
      "Iteration 78, loss = 0.57619138\n",
      "Iteration 75, loss = 0.57783109Iteration 33, loss = 0.57599823\n",
      "\n",
      "Iteration 14, loss = 0.57477346\n",
      "Iteration 5, loss = 0.57246432\n",
      "Iteration 79, loss = 0.57618892Iteration 34, loss = 0.57614756\n",
      "\n",
      "Iteration 15, loss = 0.57536556Iteration 22, loss = 0.57242883Iteration 76, loss = 0.57782966\n",
      "\n",
      "Iteration 6, loss = 0.57210397\n",
      "Iteration 35, loss = 0.57586812\n",
      "Iteration 80, loss = 0.57618941\n",
      "Iteration 7, loss = 0.57192212\n",
      "\n",
      "Iteration 36, loss = 0.57583628\n",
      "Iteration 77, loss = 0.57783062\n",
      "Iteration 23, loss = 0.57230176\n",
      "Iteration 8, loss = 0.57178947Iteration 81, loss = 0.57618885\n",
      "\n",
      "Iteration 26, loss = 0.57219817Iteration 16, loss = 0.57485632\n",
      "\n",
      "Iteration 78, loss = 0.57782965\n",
      "Iteration 82, loss = 0.57618918\n",
      "Iteration 27, loss = 0.57247681\n",
      "Iteration 24, loss = 0.57249785\n",
      "Iteration 79, loss = 0.57783109\n",
      "Iteration 17, loss = 0.57460773\n",
      "Iteration 83, loss = 0.57618796Iteration 28, loss = 0.57221932\n",
      "\n",
      "Iteration 18, loss = 0.57452342Iteration 80, loss = 0.57782944\n",
      "\n",
      "Iteration 25, loss = 0.57266235Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "\n",
      "Iteration 29, loss = 0.57225511\n",
      "Iteration 84, loss = 0.57618793\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015Iteration 81, loss = 0.57782912\n",
      "Iteration 26, loss = 0.57236962\n",
      "\n",
      "Iteration 19, loss = 0.57445152\n",
      "Iteration 37, loss = 0.57578450\n",
      "Iteration 27, loss = 0.57235864Iteration 20, loss = 0.57444794\n",
      "Iteration 85, loss = 0.57618768\n",
      "Iteration 82, loss = 0.57782887\n",
      "\n",
      "Iteration 30, loss = 0.57220263\n",
      "Iteration 21, loss = 0.57494178Iteration 83, loss = 0.57782870\n",
      "Iteration 86, loss = 0.57618771\n",
      "\n",
      "Iteration 38, loss = 0.57577017\n",
      "Iteration 28, loss = 0.57252450\n",
      "Iteration 22, loss = 0.57449913Iteration 84, loss = 0.57782962\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Iteration 9, loss = 0.57218176Iteration 87, loss = 0.57618771\n",
      "\n",
      "Iteration 23, loss = 0.57444694\n",
      "Iteration 29, loss = 0.57234968Iteration 85, loss = 0.57782842\n",
      "\n",
      "Iteration 39, loss = 0.57576881Iteration 88, loss = 0.57618751Iteration 10, loss = 0.57152746\n",
      "\n",
      "Iteration 31, loss = 0.57212358\n",
      "Iteration 24, loss = 0.57445413\n",
      "Iteration 86, loss = 0.57782848\n",
      "Iteration 30, loss = 0.57233475\n",
      "Iteration 11, loss = 0.57211952\n",
      "\n",
      "Iteration 89, loss = 0.57618782\n",
      "Iteration 31, loss = 0.57234625\n",
      "Iteration 87, loss = 0.57782851\n",
      "Iteration 40, loss = 0.57578360\n",
      "Iteration 12, loss = 0.57202323\n",
      "Iteration 25, loss = 0.57450452Iteration 90, loss = 0.57618761\n",
      "\n",
      "Iteration 41, loss = 0.57574788Iteration 88, loss = 0.57782840\n",
      "Iteration 13, loss = 0.57167247\n",
      "Iteration 32, loss = 0.57220711\n",
      "Iteration 91, loss = 0.57618762Iteration 26, loss = 0.57473054\n",
      "Iteration 32, loss = 0.57210320\n",
      "Iteration 89, loss = 0.57782859\n",
      "Iteration 33, loss = 0.57238272\n",
      "Iteration 27, loss = 0.57442348\n",
      "Iteration 33, loss = 0.57214484\n",
      "\n",
      "Iteration 90, loss = 0.57782833Iteration 34, loss = 0.57219755\n",
      "Iteration 28, loss = 0.57453582\n",
      "\n",
      "Iteration 14, loss = 0.57364323\n",
      "Iteration 42, loss = 0.57574750Iteration 35, loss = 0.57219899\n",
      "Iteration 29, loss = 0.57479301\n",
      "Iteration 34, loss = 0.57207272Iteration 91, loss = 0.57782832\n",
      "\n",
      "Iteration 15, loss = 0.57191221\n",
      "\n",
      "\n",
      "Iteration 30, loss = 0.57435442Iteration 92, loss = 0.57782865\n",
      "\n",
      "Iteration 16, loss = 0.57130432\n",
      "Iteration 43, loss = 0.57574121\n",
      "Iteration 92, loss = 0.57618826\n",
      "Iteration 93, loss = 0.57782837\n",
      "Iteration 31, loss = 0.57447622\n",
      "Iteration 17, loss = 0.57108647\n",
      "Iteration 44, loss = 0.57574218\n",
      "Iteration 36, loss = 0.57215564Iteration 93, loss = 0.57618756\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Iteration 94, loss = 0.57782827Iteration 32, loss = 0.57440133\n",
      "\n",
      "Iteration 45, loss = 0.57574581\n",
      "Iteration 94, loss = 0.57618748\n",
      "Iteration 37, loss = 0.57211512Iteration 33, loss = 0.57438588\n",
      "Iteration 18, loss = 0.57198875\n",
      "\n",
      "Iteration 95, loss = 0.57782831\n",
      "Iteration 46, loss = 0.57574010\n",
      "Iteration 34, loss = 0.57434204\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Iteration 19, loss = 0.57090605\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "Iteration 38, loss = 0.57217365\n",
      "Iteration 96, loss = 0.57782827\n",
      "Iteration 95, loss = 0.57618760\n",
      "Iteration 47, loss = 0.57575180\n",
      "Iteration 35, loss = 0.57217540\n",
      "Iteration 39, loss = 0.57212875Iteration 35, loss = 0.57428853\n",
      "Iteration 97, loss = 0.57782824\n",
      "\n",
      "Iteration 20, loss = 0.57095428\n",
      "Iteration 48, loss = 0.57573316\n",
      "Iteration 36, loss = 0.57222022\n",
      "Iteration 36, loss = 0.57429089\n",
      "Iteration 40, loss = 0.57211458\n",
      "Iteration 98, loss = 0.57782850Iteration 21, loss = 0.57086183\n",
      "\n",
      "Iteration 49, loss = 0.57572514\n",
      "Iteration 37, loss = 0.57222367\n",
      "Iteration 37, loss = 0.57432418\n",
      "Iteration 41, loss = 0.57210949\n",
      "Iteration 22, loss = 0.57074193\n",
      "Iteration 99, loss = 0.57782810\n",
      "Iteration 38, loss = 0.57208752\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Iteration 1, loss = 0.81321805\n",
      "Iteration 96, loss = 0.57618739\n",
      "Iteration 38, loss = 0.57431167\n",
      "Iteration 42, loss = 0.57212724Iteration 23, loss = 0.57065620\n",
      "Iteration 100, loss = 0.57782797\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "\n",
      "Iteration 50, loss = 0.57580264\n",
      "Iteration 39, loss = 0.57205548\n",
      "Iteration 2, loss = 0.62362411Iteration 97, loss = 0.57618737\n",
      "\n",
      "Iteration 39, loss = 0.57431073\n",
      "Iteration 101, loss = 0.57782785\n",
      "Iteration 24, loss = 0.57069306\n",
      "Iteration 51, loss = 0.57579416\n",
      "Iteration 98, loss = 0.57618732\n",
      "Iteration 43, loss = 0.57211523\n",
      "Iteration 40, loss = 0.57428152\n",
      "Iteration 102, loss = 0.57782796Iteration 25, loss = 0.57074882\n",
      "\n",
      "Iteration 52, loss = 0.57573064\n",
      "Iteration 99, loss = 0.57618729\n",
      "Iteration 41, loss = 0.57430017\n",
      "Iteration 26, loss = 0.57093062\n",
      "Iteration 53, loss = 0.57572796\n",
      "Iteration 103, loss = 0.57782787\n",
      "Iteration 100, loss = 0.57618726\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 44, loss = 0.57209602Iteration 3, loss = 0.58192844\n",
      "Iteration 42, loss = 0.57427670\n",
      "\n",
      "Iteration 54, loss = 0.57570995\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369\n",
      "Iteration 27, loss = 0.57063661\n",
      "Iteration 104, loss = 0.57782795\n",
      "Iteration 101, loss = 0.57618724\n",
      "Iteration 45, loss = 0.57211553\n",
      "Iteration 43, loss = 0.57430654\n",
      "Iteration 4, loss = 0.57401101\n",
      "Iteration 55, loss = 0.57571049\n",
      "Iteration 28, loss = 0.57060176\n",
      "Iteration 105, loss = 0.57782786\n",
      "Iteration 46, loss = 0.57213704Iteration 102, loss = 0.57618723\n",
      "Iteration 44, loss = 0.57426887\n",
      "Iteration 5, loss = 0.57221719\n",
      "Iteration 56, loss = 0.57570808\n",
      "Iteration 106, loss = 0.57782790\n",
      "Iteration 47, loss = 0.57208913\n",
      "Iteration 45, loss = 0.57429215Iteration 29, loss = 0.57054620\n",
      "\n",
      "Iteration 6, loss = 0.57213537\n",
      "Iteration 57, loss = 0.57570699\n",
      "Iteration 48, loss = 0.57209135Iteration 46, loss = 0.57426557\n",
      "\n",
      "\n",
      "Iteration 7, loss = 0.57194316Iteration 30, loss = 0.57059195\n",
      "\n",
      "Iteration 58, loss = 0.57573178\n",
      "Iteration 40, loss = 0.57204754\n",
      "Iteration 47, loss = 0.57426449Iteration 49, loss = 0.57208431\n",
      "\n",
      "Iteration 8, loss = 0.57194333Iteration 103, loss = 0.57618726Iteration 31, loss = 0.57052792\n",
      "\n",
      "\n",
      "Iteration 59, loss = 0.57570527\n",
      "Iteration 41, loss = 0.57204085Iteration 48, loss = 0.57427432\n",
      "\n",
      "Iteration 50, loss = 0.57210688\n",
      "Iteration 9, loss = 0.57285032Iteration 104, loss = 0.57618724Iteration 32, loss = 0.57062677\n",
      "\n",
      "\n",
      "Iteration 60, loss = 0.57570602\n",
      "Iteration 49, loss = 0.57427488Iteration 42, loss = 0.57203440\n",
      "\n",
      "Iteration 51, loss = 0.57211469Iteration 107, loss = 0.57782784\n",
      "Iteration 105, loss = 0.57618726\n",
      "Iteration 33, loss = 0.57081148\n",
      "Iteration 61, loss = 0.57570866\n",
      "\n",
      "Iteration 50, loss = 0.57425815Iteration 10, loss = 0.57296958\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369\n",
      "Iteration 106, loss = 0.57618722Iteration 108, loss = 0.57782788\n",
      "\n",
      "Iteration 62, loss = 0.57570416\n",
      "Iteration 52, loss = 0.57208893\n",
      "Iteration 34, loss = 0.57050944Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369\n",
      "\n",
      "Iteration 51, loss = 0.57425985\n",
      "Iteration 109, loss = 0.57782782\n",
      "Iteration 63, loss = 0.57570737\n",
      "Iteration 11, loss = 0.57163242\n",
      "Iteration 107, loss = 0.57618720\n",
      "Iteration 35, loss = 0.57045017\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Iteration 52, loss = 0.57425591Iteration 53, loss = 0.57207898\n",
      "\n",
      "Iteration 64, loss = 0.57570289Iteration 12, loss = 0.57190891\n",
      "\n",
      "Iteration 110, loss = 0.57782781Iteration 36, loss = 0.57043225Iteration 108, loss = 0.57618719\n",
      "\n",
      "Iteration 54, loss = 0.57208176\n",
      "\n",
      "Iteration 53, loss = 0.57425608\n",
      "Iteration 13, loss = 0.57376266Iteration 65, loss = 0.57570451\n",
      "\n",
      "Iteration 55, loss = 0.57207691\n",
      "Iteration 109, loss = 0.57618719Iteration 37, loss = 0.57044903\n",
      "\n",
      "Iteration 54, loss = 0.57425922\n",
      "Iteration 111, loss = 0.57782799\n",
      "Iteration 66, loss = 0.57570263\n",
      "Iteration 38, loss = 0.57043303\n",
      "Iteration 110, loss = 0.57618718\n",
      "Iteration 55, loss = 0.57427207\n",
      "Iteration 112, loss = 0.57782779\n",
      "Iteration 14, loss = 0.57157495\n",
      "Iteration 43, loss = 0.57205393\n",
      "Iteration 67, loss = 0.57570156\n",
      "Iteration 56, loss = 0.57207602\n",
      "Iteration 39, loss = 0.57045635Iteration 111, loss = 0.57618717\n",
      "Iteration 56, loss = 0.57425386\n",
      "\n",
      "Iteration 113, loss = 0.57782786\n",
      "Iteration 15, loss = 0.57447846\n",
      "Iteration 44, loss = 0.57209929\n",
      "Iteration 68, loss = 0.57570127\n",
      "Iteration 57, loss = 0.57425361Iteration 57, loss = 0.57207540\n",
      "\n",
      "Iteration 112, loss = 0.57618716\n",
      "Iteration 40, loss = 0.57042356\n",
      "Iteration 114, loss = 0.57782777\n",
      "Iteration 16, loss = 0.57197747\n",
      "Iteration 45, loss = 0.57203101\n",
      "Iteration 58, loss = 0.57208733Iteration 69, loss = 0.57571199\n",
      "Iteration 58, loss = 0.57426034\n",
      "\n",
      "Iteration 113, loss = 0.57618718\n",
      "Iteration 41, loss = 0.57045743\n",
      "Iteration 115, loss = 0.57782775\n",
      "Iteration 46, loss = 0.57205156\n",
      "Iteration 17, loss = 0.57160740Iteration 59, loss = 0.57425567\n",
      "Iteration 70, loss = 0.57570245\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "\n",
      "Iteration 114, loss = 0.57618717\n",
      "Iteration 42, loss = 0.57043400\n",
      "Iteration 116, loss = 0.57782776\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 71, loss = 0.57569848\n",
      "Iteration 60, loss = 0.57427046\n",
      "Iteration 115, loss = 0.57618717\n",
      "Iteration 43, loss = 0.57048077\n",
      "Iteration 59, loss = 0.57208442\n",
      "Iteration 72, loss = 0.57569845\n",
      "Iteration 117, loss = 0.57782775\n",
      "Iteration 61, loss = 0.57425192\n",
      "Iteration 116, loss = 0.57618713\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 44, loss = 0.57042783\n",
      "Iteration 73, loss = 0.57569878\n",
      "Iteration 18, loss = 0.57135307\n",
      "Iteration 60, loss = 0.57207366\n",
      "Iteration 74, loss = 0.57569823\n",
      "Iteration 19, loss = 0.57107876\n",
      "Iteration 61, loss = 0.57207973\n",
      "Iteration 117, loss = 0.57618714\n",
      "Iteration 45, loss = 0.57041253\n",
      "Iteration 75, loss = 0.57569824\n",
      "Iteration 20, loss = 0.57097462\n",
      "Iteration 62, loss = 0.57207256\n",
      "Iteration 118, loss = 0.57618713\n",
      "Iteration 46, loss = 0.57040276\n",
      "Iteration 76, loss = 0.57569820\n",
      "Iteration 21, loss = 0.57087890\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "Iteration 63, loss = 0.57208350\n",
      "Iteration 119, loss = 0.57618713Iteration 47, loss = 0.57039893\n",
      "\n",
      "Iteration 77, loss = 0.57570117\n",
      "Iteration 47, loss = 0.57217388\n",
      "Iteration 22, loss = 0.57080488\n",
      "Iteration 64, loss = 0.57207168\n",
      "Iteration 48, loss = 0.57041758Iteration 78, loss = 0.57569773Iteration 120, loss = 0.57618713\n",
      "\n",
      "\n",
      "Iteration 48, loss = 0.57204380\n",
      "Iteration 23, loss = 0.57077157\n",
      "Iteration 65, loss = 0.57206972\n",
      "Iteration 79, loss = 0.57569779\n",
      "Iteration 121, loss = 0.57618712Iteration 49, loss = 0.57039932\n",
      "\n",
      "Iteration 49, loss = 0.57204679\n",
      "Iteration 24, loss = 0.57075657\n",
      "Iteration 66, loss = 0.57207301\n",
      "Iteration 62, loss = 0.57425258\n",
      "Iteration 80, loss = 0.57569793\n",
      "Iteration 50, loss = 0.57038886Iteration 122, loss = 0.57618713\n",
      "\n",
      "Iteration 50, loss = 0.57205887\n",
      "Iteration 25, loss = 0.57079085\n",
      "Iteration 67, loss = 0.57206814\n",
      "Iteration 81, loss = 0.57569917\n",
      "Iteration 123, loss = 0.57618713\n",
      "Iteration 51, loss = 0.57201095\n",
      "Iteration 63, loss = 0.57425519\n",
      "Iteration 82, loss = 0.57569728\n",
      "Iteration 124, loss = 0.57618712\n",
      "Iteration 68, loss = 0.57206718\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "Iteration 118, loss = 0.57782774\n",
      "Iteration 64, loss = 0.57425101\n",
      "Iteration 26, loss = 0.57080323\n",
      "Iteration 83, loss = 0.57569737\n",
      "Iteration 125, loss = 0.57618712\n",
      "Iteration 52, loss = 0.57199787Iteration 65, loss = 0.57426724\n",
      "Iteration 119, loss = 0.57782774\n",
      "Iteration 84, loss = 0.57569709Iteration 126, loss = 0.57618715\n",
      "\n",
      "Iteration 69, loss = 0.57206724\n",
      "Iteration 27, loss = 0.57070339\n",
      "Iteration 85, loss = 0.57569700Iteration 127, loss = 0.57618713\n",
      "Iteration 66, loss = 0.57425273\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "\n",
      "Iteration 120, loss = 0.57782773\n",
      "Iteration 70, loss = 0.57206733\n",
      "Iteration 128, loss = 0.57618714Iteration 86, loss = 0.57569841Iteration 28, loss = 0.57070297\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015\n",
      "Iteration 67, loss = 0.57424965\n",
      "\n",
      "Iteration 121, loss = 0.57782774\n",
      "Iteration 71, loss = 0.57206786\n",
      "Iteration 87, loss = 0.57569707\n",
      "Iteration 68, loss = 0.57425022\n",
      "Iteration 29, loss = 0.57071246\n",
      "Iteration 129, loss = 0.57618713Iteration 122, loss = 0.57782773\n",
      "Iteration 72, loss = 0.57206656\n",
      "\n",
      "Iteration 51, loss = 0.57038512\n",
      "Iteration 88, loss = 0.57569676\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369\n",
      "Iteration 69, loss = 0.57425037\n",
      "Iteration 73, loss = 0.57206655\n",
      "Iteration 130, loss = 0.57618714\n",
      "Iteration 123, loss = 0.57782773\n",
      "Iteration 89, loss = 0.57569684\n",
      "Iteration 70, loss = 0.57424953Iteration 30, loss = 0.57065917\n",
      "\n",
      "Iteration 131, loss = 0.57618711Iteration 124, loss = 0.57782773\n",
      "\n",
      "Iteration 90, loss = 0.57569687\n",
      "Iteration 52, loss = 0.57038530\n",
      "Iteration 71, loss = 0.57424946Iteration 31, loss = 0.57068382\n",
      "\n",
      "Iteration 74, loss = 0.57206686\n",
      "Iteration 125, loss = 0.57782772\n",
      "Iteration 132, loss = 0.57618711\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 91, loss = 0.57569670\n",
      "Iteration 72, loss = 0.57425267\n",
      "Iteration 75, loss = 0.57206649\n",
      "Iteration 32, loss = 0.57067165Iteration 126, loss = 0.57782773\n",
      "\n",
      "Iteration 92, loss = 0.57569676\n",
      "Iteration 53, loss = 0.57038384\n",
      "\n",
      "Iteration 73, loss = 0.57424940\n",
      "Iteration 127, loss = 0.57782772\n",
      "Iteration 93, loss = 0.57569671\n",
      "Iteration 53, loss = 0.57203249\n",
      "Iteration 54, loss = 0.57038020\n",
      "Iteration 74, loss = 0.57424941\n",
      "Iteration 33, loss = 0.57092973\n",
      "Iteration 94, loss = 0.57569677\n",
      "Iteration 128, loss = 0.57782773\n",
      "Iteration 76, loss = 0.57206659\n",
      "Iteration 54, loss = 0.57200478\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369\n",
      "Iteration 75, loss = 0.57424922\n",
      "Iteration 55, loss = 0.57038372\n",
      "Iteration 34, loss = 0.57058920\n",
      "Iteration 95, loss = 0.57569660\n",
      "Iteration 77, loss = 0.57206613Iteration 1, loss = 0.76786075\n",
      "\n",
      "Iteration 129, loss = 0.57782772\n",
      "Iteration 55, loss = 0.57198205\n",
      "Iteration 76, loss = 0.57424927\n",
      "Iteration 56, loss = 0.57038139\n",
      "Iteration 35, loss = 0.57059415Iteration 96, loss = 0.57569662\n",
      "\n",
      "Iteration 78, loss = 0.57206847Iteration 2, loss = 0.62335605\n",
      "\n",
      "Iteration 56, loss = 0.57198108\n",
      "Iteration 130, loss = 0.57782773\n",
      "Iteration 77, loss = 0.57424916Iteration 57, loss = 0.57038620\n",
      "\n",
      "Iteration 97, loss = 0.57569656\n",
      "Iteration 36, loss = 0.57082279\n",
      "Iteration 3, loss = 0.58996435\n",
      "Iteration 79, loss = 0.57206611\n",
      "Iteration 57, loss = 0.57198110\n",
      "Iteration 131, loss = 0.57782772\n",
      "Iteration 58, loss = 0.57038045\n",
      "Iteration 78, loss = 0.57424947\n",
      "Iteration 98, loss = 0.57569657\n",
      "Iteration 37, loss = 0.57055549\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Iteration 4, loss = 0.58481921\n",
      "Iteration 58, loss = 0.57197956\n",
      "Iteration 132, loss = 0.57782772\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.Iteration 80, loss = 0.57206833\n",
      "\n",
      "Iteration 79, loss = 0.57424892Iteration 99, loss = 0.57569685\n",
      "\n",
      "Iteration 38, loss = 0.57053921\n",
      "Iteration 59, loss = 0.57037680\n",
      "Iteration 59, loss = 0.57197962\n",
      "Iteration 81, loss = 0.57206782Iteration 5, loss = 0.58296927\n",
      "\n",
      "Iteration 100, loss = 0.57569648Iteration 80, loss = 0.57424925\n",
      "\n",
      "Iteration 60, loss = 0.57037711\n",
      "Iteration 60, loss = 0.57198048\n",
      "Iteration 82, loss = 0.57206572\n",
      "Iteration 39, loss = 0.57055548\n",
      "Iteration 81, loss = 0.57424879\n",
      "Iteration 101, loss = 0.57569650\n",
      "Iteration 6, loss = 0.58366224\n",
      "Iteration 61, loss = 0.57038060\n",
      "Iteration 40, loss = 0.57053168Iteration 102, loss = 0.57569649\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 82, loss = 0.57424917\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015\n",
      "\n",
      "Iteration 7, loss = 0.58262946\n",
      "Iteration 103, loss = 0.57569642\n",
      "Iteration 83, loss = 0.57206586Iteration 83, loss = 0.57424903\n",
      "\n",
      "Iteration 62, loss = 0.57037806\n",
      "Iteration 8, loss = 0.58244234Iteration 1, loss = 0.57743994\n",
      "\n",
      "Iteration 104, loss = 0.57569664\n",
      "Iteration 84, loss = 0.57206563\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015\n",
      "Iteration 84, loss = 0.57424857\n",
      "Iteration 41, loss = 0.57053479\n",
      "Iteration 63, loss = 0.57037573\n",
      "Iteration 2, loss = 0.57617691Iteration 9, loss = 0.58309496\n",
      "\n",
      "Iteration 105, loss = 0.57569645\n",
      "Iteration 85, loss = 0.57206563Iteration 85, loss = 0.57424862\n",
      "\n",
      "Iteration 42, loss = 0.57058158\n",
      "Iteration 3, loss = 0.57560630Iteration 64, loss = 0.57037293\n",
      "Iteration 106, loss = 0.57569651\n",
      "\n",
      "Iteration 86, loss = 0.57424849Iteration 86, loss = 0.57206550\n",
      "\n",
      "Iteration 43, loss = 0.57051622Iteration 10, loss = 0.58274050\n",
      "\n",
      "Iteration 107, loss = 0.57569641\n",
      "Iteration 65, loss = 0.57037215\n",
      "Iteration 4, loss = 0.57530959\n",
      "Iteration 87, loss = 0.57206594Iteration 87, loss = 0.57424857\n",
      "\n",
      "Iteration 11, loss = 0.58641498Iteration 44, loss = 0.57055453\n",
      "\n",
      "Iteration 108, loss = 0.57569641\n",
      "Iteration 66, loss = 0.57038772\n",
      "Iteration 5, loss = 0.57535065\n",
      "Iteration 88, loss = 0.57424873\n",
      "Iteration 88, loss = 0.57206547\n",
      "Iteration 45, loss = 0.57051548\n",
      "Iteration 109, loss = 0.57569640\n",
      "Iteration 12, loss = 0.58372006\n",
      "Iteration 67, loss = 0.57037159\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "Iteration 6, loss = 0.57551063\n",
      "Iteration 89, loss = 0.57424882\n",
      "Iteration 89, loss = 0.57206551\n",
      "Iteration 46, loss = 0.57051064\n",
      "Iteration 61, loss = 0.57199480\n",
      "Iteration 110, loss = 0.57569640\n",
      "Iteration 68, loss = 0.57037177Iteration 13, loss = 0.58291428\n",
      "\n",
      "Iteration 7, loss = 0.57473803\n",
      "Iteration 90, loss = 0.57424855\n",
      "Iteration 90, loss = 0.57206542\n",
      "Iteration 47, loss = 0.57050730\n",
      "Iteration 62, loss = 0.57197860\n",
      "Iteration 111, loss = 0.57569645\n",
      "Iteration 14, loss = 0.58238686\n",
      "Iteration 91, loss = 0.57206537\n",
      "Iteration 8, loss = 0.57531739Iteration 91, loss = 0.57424842\n",
      "\n",
      "Iteration 63, loss = 0.57197793\n",
      "Iteration 69, loss = 0.57037053\n",
      "Iteration 112, loss = 0.57569639\n",
      "Iteration 48, loss = 0.57050359\n",
      "Iteration 15, loss = 0.58183735Iteration 92, loss = 0.57424848\n",
      "\n",
      "Iteration 9, loss = 0.57495454\n",
      "Iteration 113, loss = 0.57569639\n",
      "Iteration 70, loss = 0.57037044\n",
      "Iteration 93, loss = 0.57424844\n",
      "Iteration 64, loss = 0.57197481\n",
      "Iteration 16, loss = 0.58161069\n",
      "Iteration 10, loss = 0.57469215\n",
      "Iteration 114, loss = 0.57569638\n",
      "Iteration 49, loss = 0.57051603\n",
      "Iteration 92, loss = 0.57206571\n",
      "Iteration 65, loss = 0.57199483\n",
      "Iteration 94, loss = 0.57424836\n",
      "Iteration 11, loss = 0.57492962\n",
      "Iteration 71, loss = 0.57037026\n",
      "Iteration 115, loss = 0.57569650\n",
      "Iteration 93, loss = 0.57206525Iteration 50, loss = 0.57049312\n",
      "\n",
      "Iteration 66, loss = 0.57197164\n",
      "Iteration 95, loss = 0.57424864\n",
      "Iteration 17, loss = 0.58399282\n",
      "Iteration 12, loss = 0.57431265\n",
      "Iteration 72, loss = 0.57037009\n",
      "Iteration 116, loss = 0.57569637\n",
      "Iteration 94, loss = 0.57206547Iteration 51, loss = 0.57049479\n",
      "\n",
      "Iteration 96, loss = 0.57424844\n",
      "Iteration 18, loss = 0.58289266\n",
      "Iteration 13, loss = 0.57438213\n",
      "Iteration 73, loss = 0.57036996\n",
      "Iteration 117, loss = 0.57569639\n",
      "Iteration 52, loss = 0.57056479Iteration 95, loss = 0.57206514\n",
      "\n",
      "Iteration 97, loss = 0.57424842\n",
      "Iteration 14, loss = 0.57431184\n",
      "Iteration 67, loss = 0.57197087\n",
      "Iteration 118, loss = 0.57569636\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 19, loss = 0.58131835\n",
      "Iteration 96, loss = 0.57206523\n",
      "Iteration 53, loss = 0.57050704\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369\n",
      "Iteration 74, loss = 0.57036997\n",
      "Iteration 98, loss = 0.57424832\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 15, loss = 0.57423081\n",
      "Iteration 119, loss = 0.57569636\n",
      "Iteration 20, loss = 0.58171952\n",
      "Iteration 97, loss = 0.57206513\n",
      "Iteration 54, loss = 0.57048407\n",
      "Iteration 75, loss = 0.57037127\n",
      "Iteration 99, loss = 0.57424826\n",
      "Iteration 68, loss = 0.57196910\n",
      "Iteration 16, loss = 0.57419647\n",
      "Iteration 120, loss = 0.57569635\n",
      "Iteration 21, loss = 0.58155517\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "Iteration 98, loss = 0.57206513\n",
      "Iteration 100, loss = 0.57424827\n",
      "Iteration 76, loss = 0.57036949\n",
      "Iteration 55, loss = 0.57048644\n",
      "Iteration 69, loss = 0.57197058\n",
      "Iteration 17, loss = 0.57464837\n",
      "Iteration 121, loss = 0.57569636\n",
      "Iteration 22, loss = 0.58133147\n",
      "Iteration 99, loss = 0.57206518\n",
      "Iteration 101, loss = 0.57424827\n",
      "Iteration 70, loss = 0.57198019\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "Iteration 18, loss = 0.57406720\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.009222\n",
      "Iteration 56, loss = 0.57048309\n",
      "Iteration 77, loss = 0.57037021\n",
      "Iteration 122, loss = 0.57569635\n",
      "Iteration 23, loss = 0.58139567\n",
      "Iteration 102, loss = 0.57424825\n",
      "Iteration 100, loss = 0.57206510\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 57, loss = 0.57048314\n",
      "Iteration 19, loss = 0.57388757\n",
      "Iteration 78, loss = 0.57036932\n",
      "Iteration 24, loss = 0.58129353\n",
      "Iteration 103, loss = 0.57424825\n",
      "Iteration 101, loss = 0.57206507\n",
      "Iteration 123, loss = 0.57569636\n",
      "Iteration 20, loss = 0.57389408\n",
      "Iteration 25, loss = 0.58137259Iteration 104, loss = 0.57424833\n",
      "\n",
      "Iteration 79, loss = 0.57036993\n",
      "Iteration 58, loss = 0.57048850\n",
      "Iteration 102, loss = 0.57206507\n",
      "Iteration 21, loss = 0.57384948\n",
      "Iteration 124, loss = 0.57569636\n",
      "Iteration 105, loss = 0.57424826\n",
      "Iteration 26, loss = 0.58153380\n",
      "Iteration 80, loss = 0.57036940\n",
      "Iteration 59, loss = 0.57048984\n",
      "Iteration 103, loss = 0.57206507\n",
      "Iteration 22, loss = 0.57387833Iteration 125, loss = 0.57569636\n",
      "\n",
      "Iteration 106, loss = 0.57424824Iteration 27, loss = 0.58107827\n",
      "\n",
      "Iteration 81, loss = 0.57036875\n",
      "Iteration 60, loss = 0.57049340Iteration 104, loss = 0.57206513\n",
      "\n",
      "Iteration 23, loss = 0.57398169\n",
      "Iteration 107, loss = 0.57424823Iteration 82, loss = 0.57036835Iteration 28, loss = 0.58114244\n",
      "\n",
      "\n",
      "Iteration 61, loss = 0.57047931\n",
      "Iteration 24, loss = 0.57372135\n",
      "Iteration 108, loss = 0.57424825Iteration 29, loss = 0.58110322\n",
      "\n",
      "Iteration 83, loss = 0.57036822\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015\n",
      "Iteration 62, loss = 0.57048286\n",
      "Iteration 105, loss = 0.57206506\n",
      "Iteration 25, loss = 0.57376449\n",
      "Iteration 30, loss = 0.58102248\n",
      "Iteration 109, loss = 0.57424828\n",
      "Iteration 106, loss = 0.57206508\n",
      "Iteration 63, loss = 0.57047773\n",
      "Iteration 84, loss = 0.57036817\n",
      "Iteration 26, loss = 0.57368269\n",
      "Iteration 110, loss = 0.57424822\n",
      "Iteration 107, loss = 0.57206506\n",
      "Iteration 64, loss = 0.57047851\n",
      "Iteration 31, loss = 0.58101878\n",
      "Iteration 85, loss = 0.57036808\n",
      "Iteration 27, loss = 0.57378737\n",
      "Iteration 111, loss = 0.57424826\n",
      "Iteration 126, loss = 0.57569635\n",
      "Iteration 32, loss = 0.58104477\n",
      "Iteration 28, loss = 0.57372039Iteration 108, loss = 0.57206506\n",
      "\n",
      "Iteration 65, loss = 0.57047751\n",
      "Iteration 86, loss = 0.57036807\n",
      "Iteration 112, loss = 0.57424821\n",
      "Iteration 127, loss = 0.57569635Iteration 71, loss = 0.57197121Iteration 33, loss = 0.58098370\n",
      "Iteration 109, loss = 0.57206506\n",
      "Iteration 29, loss = 0.57365297\n",
      "Iteration 113, loss = 0.57424821\n",
      "Iteration 87, loss = 0.57036804\n",
      "\n",
      "Iteration 34, loss = 0.58098199Iteration 30, loss = 0.57360261\n",
      "\n",
      "Iteration 66, loss = 0.57047503Iteration 114, loss = 0.57424821\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "\n",
      "Iteration 72, loss = 0.57196604\n",
      "Iteration 31, loss = 0.57360655\n",
      "Iteration 35, loss = 0.58111492\n",
      "Iteration 88, loss = 0.57036807\n",
      "Iteration 110, loss = 0.57206519\n",
      "Iteration 115, loss = 0.57424821\n",
      "Iteration 67, loss = 0.57047491\n",
      "\n",
      "Iteration 73, loss = 0.57196636\n",
      "Iteration 32, loss = 0.57368031\n",
      "Iteration 89, loss = 0.57036798Iteration 36, loss = 0.58096078\n",
      "\n",
      "Iteration 111, loss = 0.57206505Iteration 68, loss = 0.57048055\n",
      "\n",
      "Iteration 116, loss = 0.57424821\n",
      "Iteration 128, loss = 0.57569635Iteration 33, loss = 0.57377646\n",
      "\n",
      "Iteration 74, loss = 0.57196554\n",
      "Iteration 90, loss = 0.57036797Iteration 37, loss = 0.58098087\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Iteration 69, loss = 0.57047371Iteration 112, loss = 0.57206511\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "Iteration 117, loss = 0.57424827\n",
      "Iteration 34, loss = 0.57353379Iteration 129, loss = 0.57569636\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.001844\n",
      "Iteration 75, loss = 0.57196527\n",
      "Iteration 38, loss = 0.58092895Iteration 113, loss = 0.57206504\n",
      "Iteration 91, loss = 0.57036795\n",
      "\n",
      "Iteration 70, loss = 0.57047545\n",
      "Iteration 118, loss = 0.57424822\n",
      "Iteration 130, loss = 0.57569635\n",
      "Iteration 35, loss = 0.57350605Iteration 76, loss = 0.57196518\n",
      "\n",
      "Iteration 114, loss = 0.57206504\n",
      "Iteration 92, loss = 0.57036792\n",
      "Iteration 39, loss = 0.58092510\n",
      "Iteration 71, loss = 0.57047302\n",
      "Iteration 119, loss = 0.57424821\n",
      "Iteration 131, loss = 0.57569635\n",
      "Iteration 77, loss = 0.57196495\n",
      "Iteration 36, loss = 0.57350209\n",
      "Iteration 115, loss = 0.57206503Iteration 93, loss = 0.57036818\n",
      "\n",
      "Iteration 72, loss = 0.57047445Iteration 40, loss = 0.58091054\n",
      "\n",
      "Iteration 120, loss = 0.57424821\n",
      "Iteration 78, loss = 0.57196620Iteration 37, loss = 0.57350269\n",
      "\n",
      "Iteration 41, loss = 0.58090792Iteration 116, loss = 0.57206503\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "\n",
      "Iteration 73, loss = 0.57047256Iteration 121, loss = 0.57424822\n",
      "\n",
      "Iteration 38, loss = 0.57351814\n",
      "Iteration 42, loss = 0.58090526Iteration 79, loss = 0.57196472\n",
      "\n",
      "Iteration 122, loss = 0.57424820\n",
      "Iteration 117, loss = 0.57206503\n",
      "Iteration 74, loss = 0.57047254\n",
      "Iteration 39, loss = 0.57349417\n",
      "Iteration 80, loss = 0.57196428\n",
      "Iteration 43, loss = 0.58090430\n",
      "Iteration 123, loss = 0.57424821\n",
      "Iteration 118, loss = 0.57206502\n",
      "Iteration 40, loss = 0.57349545Iteration 75, loss = 0.57047233\n",
      "Iteration 81, loss = 0.57196409\n",
      "\n",
      "Iteration 44, loss = 0.58090117\n",
      "Iteration 124, loss = 0.57424824\n",
      "Iteration 119, loss = 0.57206504\n",
      "Iteration 82, loss = 0.57196390\n",
      "Iteration 45, loss = 0.58089858\n",
      "Iteration 125, loss = 0.57424821\n",
      "Iteration 76, loss = 0.57047216Iteration 83, loss = 0.57196384\n",
      "\n",
      "Iteration 120, loss = 0.57206502\n",
      "Iteration 41, loss = 0.57356060\n",
      "Iteration 46, loss = 0.58089742\n",
      "Iteration 132, loss = 0.57569635\n",
      "Iteration 126, loss = 0.57424822\n",
      "Iteration 84, loss = 0.57196453\n",
      "Iteration 121, loss = 0.57206503\n",
      "Iteration 77, loss = 0.57047324\n",
      "Iteration 42, loss = 0.57353285\n",
      "Iteration 133, loss = 0.57569636\n",
      "Iteration 127, loss = 0.57424820\n",
      "Iteration 47, loss = 0.58092240\n",
      "Iteration 85, loss = 0.57196519Iteration 122, loss = 0.57206503\n",
      "\n",
      "Iteration 94, loss = 0.57036783Iteration 43, loss = 0.57352547\n",
      "\n",
      "Iteration 78, loss = 0.57047171\n",
      "Iteration 134, loss = 0.57569634\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 128, loss = 0.57424820\n",
      "Iteration 48, loss = 0.58088927\n",
      "Iteration 123, loss = 0.57206502\n",
      "Iteration 86, loss = 0.57196438\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015\n",
      "Iteration 44, loss = 0.57350724\n",
      "Iteration 95, loss = 0.57036785\n",
      "Iteration 79, loss = 0.57047202\n",
      "Iteration 129, loss = 0.57424821\n",
      "Iteration 49, loss = 0.58088846\n",
      "Iteration 124, loss = 0.57206503\n",
      "Iteration 87, loss = 0.57196310\n",
      "Iteration 45, loss = 0.57348918\n",
      "Iteration 96, loss = 0.57036780\n",
      "Iteration 130, loss = 0.57424820\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 80, loss = 0.57047143\n",
      "Iteration 88, loss = 0.57196314\n",
      "Iteration 50, loss = 0.58088526\n",
      "Iteration 125, loss = 0.57206503\n",
      "Iteration 46, loss = 0.57351042\n",
      "Iteration 97, loss = 0.57036801\n",
      "Iteration 81, loss = 0.57047144\n",
      "Iteration 51, loss = 0.58090087\n",
      "Iteration 126, loss = 0.57206502\n",
      "Iteration 89, loss = 0.57196325\n",
      "Iteration 47, loss = 0.57347189\n",
      "Iteration 98, loss = 0.57036774\n",
      "Iteration 82, loss = 0.57047416\n",
      "Iteration 52, loss = 0.58090515\n",
      "Iteration 90, loss = 0.57196306\n",
      "Iteration 127, loss = 0.57206502\n",
      "Iteration 48, loss = 0.57347193\n",
      "Iteration 99, loss = 0.57036774\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 83, loss = 0.57047356\n",
      "Iteration 53, loss = 0.58091221\n",
      "Iteration 91, loss = 0.57196301\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369\n",
      "Iteration 49, loss = 0.57345860\n",
      "Iteration 84, loss = 0.57047130Iteration 92, loss = 0.57196298\n",
      "\n",
      "Iteration 100, loss = 0.57036765\n",
      "Iteration 54, loss = 0.58087420\n",
      "Iteration 128, loss = 0.57206502\n",
      "Iteration 50, loss = 0.57346036\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000369\n",
      "Iteration 93, loss = 0.57196294\n",
      "Iteration 85, loss = 0.57047276\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015\n",
      "Iteration 101, loss = 0.57036767\n",
      "Iteration 129, loss = 0.57206502\n",
      "Iteration 51, loss = 0.57345506\n",
      "Iteration 55, loss = 0.58087803\n",
      "Iteration 94, loss = 0.57196304Iteration 86, loss = 0.57047077\n",
      "\n",
      "Iteration 52, loss = 0.57345386Iteration 102, loss = 0.57036764\n",
      "Iteration 130, loss = 0.57206502\n",
      "\n",
      "Iteration 56, loss = 0.58087639\n",
      "Iteration 95, loss = 0.57196277\n",
      "Iteration 87, loss = 0.57047091\n",
      "Iteration 131, loss = 0.57206502Iteration 53, loss = 0.57345306\n",
      "Iteration 103, loss = 0.57036763\n",
      "\n",
      "Iteration 57, loss = 0.58087426\n",
      "Iteration 96, loss = 0.57196269\n",
      "Iteration 88, loss = 0.57047085\n",
      "Iteration 132, loss = 0.57206503Iteration 54, loss = 0.57345241\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 104, loss = 0.57036764\n",
      "Iteration 97, loss = 0.57196275\n",
      "Iteration 55, loss = 0.57345302\n",
      "Iteration 58, loss = 0.58087134\n",
      "Iteration 89, loss = 0.57047079\n",
      "Iteration 98, loss = 0.57196268\n",
      "Iteration 59, loss = 0.58087121\n",
      "Iteration 90, loss = 0.57047083\n",
      "Iteration 56, loss = 0.57345097\n",
      "Iteration 105, loss = 0.57036766\n",
      "Iteration 99, loss = 0.57196260\n",
      "Iteration 91, loss = 0.57047080Iteration 60, loss = 0.58087375\n",
      "\n",
      "Iteration 57, loss = 0.57344988\n",
      "Iteration 100, loss = 0.57196256\n",
      "Iteration 106, loss = 0.57036761\n",
      "Iteration 61, loss = 0.58086969\n",
      "Iteration 92, loss = 0.57047111\n",
      "Iteration 58, loss = 0.57345113\n",
      "Iteration 101, loss = 0.57196253\n",
      "Iteration 107, loss = 0.57036761\n",
      "Iteration 62, loss = 0.58086911\n",
      "Iteration 93, loss = 0.57047083\n",
      "Iteration 59, loss = 0.57344759\n",
      "Iteration 102, loss = 0.57196258\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 108, loss = 0.57036764\n",
      "Iteration 63, loss = 0.58086897\n",
      "Iteration 60, loss = 0.57344941\n",
      "Iteration 94, loss = 0.57047094\n",
      "Iteration 64, loss = 0.58086795\n",
      "Iteration 103, loss = 0.57196244\n",
      "Iteration 109, loss = 0.57036760\n",
      "Iteration 61, loss = 0.57344697\n",
      "Iteration 95, loss = 0.57047068\n",
      "Iteration 65, loss = 0.58086737\n",
      "Iteration 104, loss = 0.57196248\n",
      "Iteration 110, loss = 0.57036763\n",
      "Iteration 62, loss = 0.57344593\n",
      "Iteration 66, loss = 0.58086850\n",
      "Iteration 96, loss = 0.57047069\n",
      "Iteration 105, loss = 0.57196248\n",
      "Iteration 111, loss = 0.57036759\n",
      "Iteration 63, loss = 0.57344531\n",
      "Iteration 67, loss = 0.58087632\n",
      "Iteration 97, loss = 0.57047065\n",
      "Iteration 106, loss = 0.57196245\n",
      "Iteration 112, loss = 0.57036758Iteration 68, loss = 0.58086628\n",
      "\n",
      "Iteration 64, loss = 0.57344452\n",
      "Iteration 98, loss = 0.57047072\n",
      "Iteration 69, loss = 0.58086787\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "Iteration 113, loss = 0.57036759\n",
      "Iteration 65, loss = 0.57345863Iteration 107, loss = 0.57196244\n",
      "\n",
      "Iteration 99, loss = 0.57047116\n",
      "Iteration 70, loss = 0.58086512\n",
      "Iteration 66, loss = 0.57344450\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000074\n",
      "Iteration 114, loss = 0.57036757\n",
      "Iteration 100, loss = 0.57047108\n",
      "Iteration 115, loss = 0.57036762\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 67, loss = 0.57344063\n",
      "Iteration 108, loss = 0.57196250\n",
      "Iteration 71, loss = 0.58086654\n",
      "Iteration 68, loss = 0.57344264Iteration 116, loss = 0.57036756Iteration 101, loss = 0.57047101\n",
      "\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 72, loss = 0.58086898Iteration 109, loss = 0.57196242\n",
      "\n",
      "Iteration 69, loss = 0.57344375Iteration 117, loss = 0.57036757\n",
      "\n",
      "Iteration 102, loss = 0.57047054\n",
      "Iteration 110, loss = 0.57196245\n",
      "Iteration 73, loss = 0.58086732\n",
      "Iteration 70, loss = 0.57344015\n",
      "Iteration 118, loss = 0.57036756\n",
      "Iteration 111, loss = 0.57196243\n",
      "Iteration 103, loss = 0.57047056\n",
      "Iteration 112, loss = 0.57196246\n",
      "Iteration 71, loss = 0.57344075Iteration 119, loss = 0.57036756\n",
      "\n",
      "Iteration 104, loss = 0.57047068\n",
      "Iteration 74, loss = 0.58086446Iteration 113, loss = 0.57196241\n",
      "Iteration 120, loss = 0.57036756\n",
      "\n",
      "Iteration 72, loss = 0.57344032\n",
      "Iteration 105, loss = 0.57047069\n",
      "Iteration 121, loss = 0.57036756\n",
      "Iteration 73, loss = 0.57343959\n",
      "Iteration 114, loss = 0.57196239\n",
      "Iteration 106, loss = 0.57047053\n",
      "Iteration 74, loss = 0.57343945Iteration 115, loss = 0.57196242\n",
      "\n",
      "Iteration 122, loss = 0.57036755\n",
      "Iteration 75, loss = 0.58086494\n",
      "Iteration 107, loss = 0.57047058\n",
      "Iteration 116, loss = 0.57196238\n",
      "Iteration 75, loss = 0.57343973\n",
      "Iteration 123, loss = 0.57036756\n",
      "Iteration 76, loss = 0.58086652\n",
      "Iteration 108, loss = 0.57047055\n",
      "Iteration 117, loss = 0.57196244\n",
      "Iteration 76, loss = 0.57343971\n",
      "Iteration 124, loss = 0.57036756\n",
      "Iteration 77, loss = 0.58086409\n",
      "Iteration 109, loss = 0.57047052Iteration 118, loss = 0.57196241\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "\n",
      "Iteration 77, loss = 0.57343940\n",
      "Iteration 125, loss = 0.57036756\n",
      "Iteration 119, loss = 0.57196239Iteration 78, loss = 0.57343992\n",
      "Iteration 110, loss = 0.57047052\n",
      "\n",
      "Iteration 126, loss = 0.57036755\n",
      "Iteration 127, loss = 0.57036755\n",
      "Iteration 79, loss = 0.57343941\n",
      "Iteration 120, loss = 0.57196235\n",
      "Iteration 111, loss = 0.57047056\n",
      "Iteration 78, loss = 0.58086387\n",
      "Iteration 80, loss = 0.57343962Iteration 128, loss = 0.57036755\n",
      "\n",
      "Iteration 112, loss = 0.57047051Iteration 121, loss = 0.57196235\n",
      "\n",
      "Iteration 79, loss = 0.58086371Iteration 129, loss = 0.57036755\n",
      "Iteration 81, loss = 0.57343871\n",
      "\n",
      "Iteration 122, loss = 0.57196234Iteration 113, loss = 0.57047049\n",
      "\n",
      "Iteration 130, loss = 0.57036755\n",
      "Iteration 82, loss = 0.57343873\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015Iteration 80, loss = 0.58086359\n",
      "\n",
      "Iteration 114, loss = 0.57047051\n",
      "Iteration 123, loss = 0.57196234\n",
      "Iteration 131, loss = 0.57036755\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 83, loss = 0.57343854\n",
      "Iteration 124, loss = 0.57196234\n",
      "Iteration 115, loss = 0.57047053\n",
      "Iteration 84, loss = 0.57343820\n",
      "Iteration 81, loss = 0.58086465\n",
      "Iteration 125, loss = 0.57196236Iteration 116, loss = 0.57047049\n",
      "\n",
      "Iteration 82, loss = 0.58086383\n",
      "Iteration 85, loss = 0.57343827\n",
      "Iteration 117, loss = 0.57047049\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 126, loss = 0.57196235\n",
      "Iteration 83, loss = 0.58086345\n",
      "Iteration 86, loss = 0.57343856\n",
      "Iteration 118, loss = 0.57047047\n",
      "Iteration 127, loss = 0.57196233\n",
      "Iteration 84, loss = 0.58086440Iteration 87, loss = 0.57343814\n",
      "\n",
      "Iteration 119, loss = 0.57047047\n",
      "Iteration 128, loss = 0.57196233\n",
      "Iteration 85, loss = 0.58086308Iteration 88, loss = 0.57343820\n",
      "\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000015\n",
      "Iteration 120, loss = 0.57047047\n",
      "Iteration 129, loss = 0.57196233\n",
      "Iteration 89, loss = 0.57343830\n",
      "Iteration 86, loss = 0.58086299\n",
      "Iteration 121, loss = 0.57047047\n",
      "Iteration 130, loss = 0.57196233\n",
      "Iteration 90, loss = 0.57343830\n",
      "Iteration 122, loss = 0.57047047\n",
      "Iteration 87, loss = 0.58086298\n",
      "Iteration 91, loss = 0.57343806\n",
      "Iteration 131, loss = 0.57196234\n",
      "Iteration 123, loss = 0.57047046\n",
      "Iteration 88, loss = 0.58086299\n",
      "Iteration 92, loss = 0.57343820\n",
      "Iteration 124, loss = 0.57047046\n",
      "Iteration 132, loss = 0.57196233\n",
      "Iteration 89, loss = 0.58086296\n",
      "Iteration 93, loss = 0.57343803\n",
      "Iteration 125, loss = 0.57047046\n",
      "Iteration 133, loss = 0.57196233\n",
      "Iteration 90, loss = 0.58086289\n",
      "Iteration 94, loss = 0.57343803\n",
      "Iteration 126, loss = 0.57047047\n",
      "Iteration 134, loss = 0.57196233\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 95, loss = 0.57343830\n",
      "Iteration 91, loss = 0.58086311\n",
      "Iteration 127, loss = 0.57047046\n",
      "Iteration 96, loss = 0.57343795\n",
      "Iteration 92, loss = 0.58086289\n",
      "Iteration 128, loss = 0.57047047\n",
      "Iteration 97, loss = 0.57343793\n",
      "Iteration 129, loss = 0.57047046\n",
      "Iteration 93, loss = 0.58086283\n",
      "Iteration 98, loss = 0.57343820\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 130, loss = 0.57047046\n",
      "Iteration 94, loss = 0.58086295\n",
      "Iteration 99, loss = 0.57343787\n",
      "Iteration 131, loss = 0.57047046\n",
      "Iteration 95, loss = 0.58086295\n",
      "Iteration 100, loss = 0.57343787\n",
      "Iteration 132, loss = 0.57047046\n",
      "Iteration 96, loss = 0.58086280\n",
      "Iteration 101, loss = 0.57343786\n",
      "Iteration 133, loss = 0.57047045\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 102, loss = 0.57343785\n",
      "Iteration 97, loss = 0.58086277\n",
      "Iteration 103, loss = 0.57343783\n",
      "Iteration 98, loss = 0.58086272\n",
      "Iteration 104, loss = 0.57343783\n",
      "Iteration 99, loss = 0.58086270\n",
      "Iteration 105, loss = 0.57343786\n",
      "Iteration 100, loss = 0.58086268\n",
      "Iteration 106, loss = 0.57343782\n",
      "Iteration 101, loss = 0.58086291\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 107, loss = 0.57343785\n",
      "Iteration 108, loss = 0.57343781\n",
      "Iteration 102, loss = 0.58086264\n",
      "Iteration 109, loss = 0.57343781\n",
      "Iteration 103, loss = 0.58086264\n",
      "Iteration 110, loss = 0.57343786\n",
      "Iteration 104, loss = 0.58086263\n",
      "Iteration 111, loss = 0.57343780\n",
      "Iteration 105, loss = 0.58086263\n",
      "Iteration 112, loss = 0.57343779\n",
      "Iteration 106, loss = 0.58086265\n",
      "Iteration 113, loss = 0.57343778\n",
      "Iteration 107, loss = 0.58086262\n",
      "Iteration 114, loss = 0.57343783\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 108, loss = 0.58086263Iteration 115, loss = 0.57343779\n",
      "\n",
      "Iteration 116, loss = 0.57343777Iteration 109, loss = 0.58086262\n",
      "\n",
      "Iteration 110, loss = 0.58086262Iteration 117, loss = 0.57343777\n",
      "\n",
      "Iteration 118, loss = 0.57343776\n",
      "Iteration 111, loss = 0.58086267\n",
      "Iteration 119, loss = 0.57343776\n",
      "Iteration 112, loss = 0.58086272\n",
      "Iteration 120, loss = 0.57343777\n",
      "Iteration 113, loss = 0.58086259\n",
      "Iteration 121, loss = 0.57343776\n",
      "Iteration 114, loss = 0.58086267\n",
      "Iteration 122, loss = 0.57343776\n",
      "Iteration 115, loss = 0.58086258\n",
      "Iteration 123, loss = 0.57343776\n",
      "Iteration 124, loss = 0.57343777\n",
      "Iteration 116, loss = 0.58086258\n",
      "Iteration 125, loss = 0.57343776\n",
      "Iteration 117, loss = 0.58086263\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 126, loss = 0.57343776\n",
      "Iteration 118, loss = 0.58086258\n",
      "Iteration 127, loss = 0.57343777\n",
      "Iteration 119, loss = 0.58086257\n",
      "Iteration 128, loss = 0.57343775\n",
      "Iteration 129, loss = 0.57343775\n",
      "Iteration 120, loss = 0.58086257\n",
      "Iteration 130, loss = 0.57343779\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 121, loss = 0.58086257\n",
      "Iteration 122, loss = 0.58086257\n",
      "Iteration 123, loss = 0.58086256\n",
      "Iteration 124, loss = 0.58086256\n",
      "Iteration 125, loss = 0.58086259\n",
      "Iteration 126, loss = 0.58086257\n",
      "Iteration 127, loss = 0.58086257\n",
      "Iteration 128, loss = 0.58086257\n",
      "Iteration 129, loss = 0.58086256\n",
      "Iteration 130, loss = 0.58086258\n",
      "Iteration 131, loss = 0.58086256\n",
      "Iteration 132, loss = 0.58086256\n",
      "Iteration 133, loss = 0.58086258\n",
      "Training loss did not improve more than tol=0.000846 for 15 consecutive epochs. Learning rate too small. Stopping.\n",
      "Iteration 1, loss = 0.66207507\n",
      "Iteration 2, loss = 0.56150863\n",
      "Iteration 3, loss = 0.56103216\n",
      "Iteration 4, loss = 0.52951004\n",
      "Iteration 5, loss = 0.51558006\n",
      "Iteration 6, loss = 0.54535040\n",
      "Iteration 7, loss = 0.53867802\n",
      "Iteration 8, loss = 0.51290901\n",
      "Iteration 9, loss = 0.54600783\n",
      "Iteration 10, loss = 0.54011427\n",
      "Iteration 11, loss = 0.53066139\n",
      "Iteration 12, loss = 0.51017142\n",
      "Iteration 13, loss = 0.51952298\n",
      "Iteration 14, loss = 0.53047933\n",
      "Iteration 15, loss = 0.53940839\n",
      "Iteration 16, loss = 0.52918044\n",
      "Iteration 17, loss = 0.52718952\n",
      "Iteration 18, loss = 0.51848860\n",
      "Iteration 19, loss = 0.51791507\n",
      "Iteration 20, loss = 0.51254144\n",
      "Iteration 21, loss = 0.52515253\n",
      "Iteration 22, loss = 0.51775049\n",
      "Iteration 23, loss = 0.53402728\n",
      "Training loss did not improve more than tol=0.000998 for 10 consecutive epochs. Stopping.\n",
      "Best MLP Params:  {'activation': 'identity', 'alpha': 2.8213778924638245e-05, 'batch_size': 32, 'beta_1': 0.841690405652771, 'beta_2': 0.7878132469258807, 'early_stopping': False, 'epsilon': 4.4675189318962583e-07, 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling', 'learning_rate_init': 0.06531631329451161, 'max_fun': 27, 'max_iter': 3827, 'momentum': 0.5963384844554342, 'n_iter_no_change': 10, 'nesterovs_momentum': False, 'power_t': 0.22218707338020585, 'shuffle': True, 'solver': 'adam', 'tol': 0.0009979351585820793, 'validation_fraction': 0.2819987418371761, 'verbose': True, 'warm_start': False}\n",
      "Best MLP Score:  0.7648221343873518\n"
     ]
    }
   ],
   "source": [
    "# Encontrando os melhores hiperparametros\n",
    "\n",
    "mlp_model = RandomizedSearchCV(MLPClassifier(), param_distributions=param_dist, n_iter=20, cv=10, error_score='raise', n_jobs=-1)\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "best_model = mlp_model.best_estimator_\n",
    "\n",
    "print(\"Best MLP Params: \", mlp_model.best_params_)\n",
    "print(\"Best MLP Score: \", mlp_model.best_score_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia:  0.6964285714285714\n"
     ]
    }
   ],
   "source": [
    "# Acuracia dos modelos\n",
    "\n",
    "pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Acuracia: \", accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1907,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "no-recurrence-events       0.67      0.94      0.78        33\n",
      "   recurrence-events       0.80      0.35      0.48        23\n",
      "\n",
      "            accuracy                           0.70        56\n",
      "           macro avg       0.74      0.64      0.63        56\n",
      "        weighted avg       0.73      0.70      0.66        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "\n",
    "labels = ['no-recurrence-events', 'recurrence-events']\n",
    "print(classification_report(y_test,pred, zero_division = 1, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1908,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6964285714285714"
      ]
     },
     "execution_count": 1908,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAycAAAJjCAYAAAAWF25nAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8QUlEQVR4nO3dfdzX8/3///uhpNKJ5CSn+bTvlFVS6OOsdUI+NPRjRWwYNqZsc/Zxsn34bKbMlOiEmNOmDcMYtWzRsJxsYk6mfTJCk0gJU5Q6fn/4dnwdnzrSmd5Pdr1eLi6Xer3e7/fxeB+Oy7P37Xi9Xu93VXV1dXUAAAAqbINKDwAAAJCIEwAAoBDiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKUL/SA8DaevLJJ1NdXZ0NN9yw0qMAALACixcvTlVVVTp37rzS24kTPvOqq6uzePHizJo1q9KjAKwTrVu3rvQIAOtUdXX1Kt1OnPCZt+GGG2bWrFmZevAZlR4FYJ04qPp//u+fplZ0DoB15ZlnGqzS7VxzAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAvBxVVXZ84zjc8r0e/P9BU/lpL/clY5HHbzCmzZosnG+++J96XTsoet5SIC1s3Tp0owZc1t23nlAmjTpljZt+ua004blnXf+WenR+BdXv9ID8K+luro6VVVVlR4D6tTzgu9l77NOyOTzR2TWn5/JF/t0z2HjhqZ66dI8e/P4mts13KRZBtx1RVr827YVnBZgzfz0p2PzX/91Zf7zP4/OvvvununTX8l5543Js8++kN/9brR/q6kYccJ68/zzz+e8887LzTffXOlRYIXqN2qYPU49Jo9d/vNMufhnSZIZ9z+arXZtn67fPbomTnY8uFcOHPGDNGi6cSXHBVgjS5cuzcUX35iTTjosF110SpJkv/3+PS1bNs+AAd/P1KnTsttuX6rwlPyrcloX683EiRPz5JNPVnoMqNOSDxbl2r2OzMPDrqu9fdHi1G+4UZJko+ZNc8SvR+WlB/6cm/7jm5UYE2CtvPPOezn66D456qj/qLW9XbsdkiQvvPCPCkwFH3HkBOD/ql66NG888z81f994i5bZ5bjD0ma/vXLPSecnSRYveD9XfOkrmTt9Rpq33qZSowKssU02aZoRI/5zue133vmHJEn79m3W80Tw/6zWkZNevXplxIgRufjii7PXXntl5513zgknnJCXXnqp5jZTpkzJUUcdlV133TX//u//njPOOCOvvfbaKj32kCFDcuyxx2bnnXfOD37wgyTJ/Pnzc/7552evvfZKx44dc/jhh+eRRx6pdd9Fixblsssuy7777pudd945Bx10UH7961/Xeuxzzjmn1n3uuOOOtG3bNv/4x0e/HRg5cmR69+6dUaNGpWvXrtlnn33y9ttvr9Vcbdu2zbhx4/KDH/wgXbt2TefOnfO9730vb775Zq3b3XnnnTn00EPTqVOn9OjRI8OGDcuiRYtq9k+fPj0nnXRSunTpki5dumTQoEGZOXPmJ35Pk2TSpEk57LDD0rFjx+y999658MILs2DBgiTJE088kbZt22by5Mm17jNt2rS0bds2v//975MkH3zwQX7605+me/fu6dChQw4++OBMmDCh1n0+6Wdj5MiRGTVqVM33ZeTIkUk++nk5/PDD07lz5+y+++45+eST88ILL6zSc4NPU4cBX8mZrz+c/X5yZp6f8ECevuk3SZKlixdn7vQZFZ4OYN167LFn85Of3JiDD+6WDh3+T6XH4V/Yap/WNXbs2Lz44ou56KKLcuGFF+bZZ5/N2WefneSjF9nHH398ttpqq1x66aU599xz8+STT+aII47I3LlzP/Gxx40bl44dO+aKK65Iv3798sEHH+TYY4/Nfffdl9NOOy2jRo1Kq1at8s1vfrNWCJx55pm5/vrr079//1x11VXZZ599cs455+See+5Zrec2a9asPPDAAxk+fHjOPffcNG/efK3mSpLhw4dn6dKlufTSS3PWWWdl8uTJGTJkSK3nfPbZZ6d9+/YZNWpUTjzxxPz85z/PhRdemCSZMWNGBgwYkLlz5+biiy/O4MGDM3PmzBx55JGf+D29++67M2jQoLRp0yajR4/OKaeckt/85jcZOHBgqqur06VLl2y//fYZP358rfvdc8892WSTTdK9e/dUV1dn0KBBufnmm3PcccflyiuvTOfOnXPaaaflzjvvrHW/lf1s9O/fP/369UuS3HLLLenfv39mzpyZgQMHpkOHDrnyyiszePDgzJgxIyeeeGKWLl26Wv/vYF179U9P5/ovfy0TTrkg2+3dJV+beE2lRwL4VEyZ8pcccMB38m//tnWuv/6/Kz0O/+JW+7SuZs2a5Yorrki9evWSJK+88kpGjhyZt956K0OHDs0+++yTYcOG1dy+S5cu6dOnT6699tqcddZZK33srbfeOmeeeWbN32+99db87W9/y6233ppOnTolSb785S/n6KOPztChQ3P77bdn+vTpuffee/P9738/xx57bJJkzz33zKuvvprHHnssBx100Co/tw8//DBnn312dtttt7Wea5kdd9wxF110Uc3fn3766UycODHJRxekjR49Ovvtt19NjCTJwoULM378+CxevDijRo1Ko0aNcsMNN6RJkyY1z2+//fbLNddcU/Pi/3+rrq7O0KFD061btwwdOrRm+w477JBvfOMbeeCBB9KjR48ccsghue666/L++++nYcOGqa6uzoQJE3LAAQekQYMGmTJlSh566KEMHz48ffr0SZJ069YtCxcuzNChQ3PQQQelfv2PfoxW9rPRqlWrtGrVKkmyyy67JEnGjx+f999/PyeddFK23HLLJEmrVq1y3333ZcGCBTXPFyrhrRdn5q0XZ+aVhx7PB+/8M4eO/Wm277ZbXnno8UqPBrDO3HLL7/KNb/woO+64fSZOHJmWLTep9Ej8i1vtIycdO3asefGZpOYF5/PPP585c+YsFwPbb799OnfunD/96U9JPgqAj//38d+Q77TTTrXu+8gjj2TzzTdP+/bta26/ZMmS9OzZM88++2zefvvtTJ06NUmy//7717rvyJEj8+Mf/3h1n95yM6zpXMsseyG+TKtWrbJw4cIkHx0VmTt3bnr37l3rNieccELuuOOObLjhhnn00UfTtWvXNGzYsOZrNWnSJLvttlsefvjhJMmSJUuW+56++OKLmT17dnr16lVr3+67754mTZpkypQpSZJDDjkkCxYsqDm164knnsisWbPSt2/fmudaVVWV7t2713qcXr16Zc6cOXn++edr5q7rZ2PZ8/3fOnXqlI022ij9+vXL4MGD89BDD6Vdu3Y57bTThAkV0XizFtn56L5pvPmmtba/9sRzSZKmW29RibEAPhVDh/48Rx75g+y5Z8c8+ODPstVWm1V6JFj9IyeNGjWq9fcNNviob5a9KN1ss+V/sDfbbLM899xH/7i3b9++1r5TTjkl3/nOd5IkjRs3rrVv/vz5mTNnznL3WWbOnDmZP39+kqRly5ar+UxWbOONl39r0DWZa9kpYSv6flVXV9c8TrLy2efPn58JEyYsd41Hkmy66UcvoHr37p1XX321Zvuhhx6a/v37J0l+9KMf5Uc/+tFy933jjTeSJK1bt07nzp0zfvz4HHjggRk/fny23377dOnSpebrLzsFbEXeeOONmnir62ejrlO0tt1229x00025+uqrc9ttt2Xs2LFp1qxZjjrqqJx66qneY531rn6jhjl07E9z37nD8sefXF2z/Qv7750kef3p/6nrrgCfKVdddXv+8z8vzxFH9M7YsRekQYMNKz0SJFmH79a1ySabJMlyF3snH71Yb9GiRZLktttuq7Vviy3q/k1k06ZNs8MOO9Q6Lenjtt122zRr1ixJMm/evJrf1CfJCy+8kPnz52fXXXdN8tHRhY9bdlH4mliVuVbFx2f/uLfeeivPPfdcOnfunKZNm2avvfbKcccdt9z9l51OdeWVV9a6gL5FixY1RyvOOuusdO3adbn7Loun5KOjJxdddFHefffdTJw4MUceeWSt59q4ceOMHTt2hc+hdevWq/Rc67Lzzjtn1KhRWbRoUaZOnZpbbrklY8aMSbt27XLggQeu1WPD6npn5mt58trb8uXzB2XJ4g8z+8nnsn233bLPOSfmiWt+lTenebMG4LNv9uw3c9ppl2aHHbbOKacckSee+Fut/V/4wrbZfPMWFZqOf3XrLE4aNGiQzTffPPfcc08OPfTQmu0zZ87MX/7ylxxzzDFJPjr1Z1V17do1f/jDH9KyZctsvfXWNdvHjBmTadOmZejQoTXxcf/99+eoo46quc3QoUPz+uuv54477kiTJk0ye/bsWo+97HSwNbEqc62KNm3apEWLFpk8eXLNaVRJctddd+WSSy7Jww8/nK5du+bvf/97dtppp5oYqa6uzplnnpnWrVtnp512Stu2bZd77CVLlqRly5b5xz/+kRNOOKFm+xtvvJGzzjorAwYMyPbbb58k6dOnT4YMGZLLL788c+fOzSGHHFLruV533XWprq7OzjvvXLP99ttvz+9///taF/d/kmVHUpa54YYbcuONN+bee+9NgwYNsueee6ZDhw757W9/m1mzZq3y48K6dM/JP8xbL87Mricenuatt8k7M1/L5PNH5OGh11Z6NIB1YsKEKVm48IO89NKsdOu2/Oc1XX/9f+cb3zi4ApPBOoyTqqqqnH766Tn33HNzxhln5JBDDslbb72VUaNGpXnz5iv8zf8nOeyww3LTTTfluOOOy7e//e1stdVWefjhh/Ozn/0sX//617PhhhumXbt2OeCAA3LJJZfk/fffz0477ZQHH3wwkydPrnnr2p49e+aqq67KVVddlU6dOuX+++/Po48+usbPdVXmWhX16tXLd77znVxwwQVp2bJlevXqlRkzZmTEiBH52te+lubNm2fgwIEZMGBATjrppBx55JHZaKONcsstt2TSpEkZMWLESh/7tNNOy/nnn5969eqlZ8+eeeedd3LFFVfk9ddfr3VK2rJ35vrFL36Rzp071zoa0r179+y+++4ZOHBgBg4cmC984Qt5+umnM2LEiHTr1q3m1LJVsexI0T333JNOnTpljz32yNChQzNo0KB8/etfT7169XLzzTenQYMG6dmz5yo/LqxLSxcvzkNDxuShIWM+8bZvv/xqflS1/C8HAEp2/PF9c/zxfT/5hlAB6/RDGA877LBsvPHGueqqqzJo0KA0adIk3bp1y+mnn57NN998tR+vcePGGTduXIYNG5ZLLrkk7777brbZZpucccYZOf7442tud8kll2TUqFG58cYb89Zbb+ULX/hCRowYkf322y9JctJJJ2XevHm59tprs3jx4vTo0SODBw/OySefvEbPc1XnWhVf+9rX0rhx41x77bW55ZZb0qpVq3zrW9/Kt771rSRJu3btMm7cuAwfPjxnnXVWqqurs+OOO2b06NHZd999V/rY/fv3z8Ybb5xrrrkmt9xySxo3bpwuXbpk6NCh2W677Wrdtm/fvpk0aVIOPrj2b0o22GCDXH311bn88stz1VVXZe7cudlyyy1z3HHHZdCgQav1XPfff//cddddOeecc9KvX7/88Ic/zJgxYzJ69OicfvrpWbJkSTp06JDrrrsubdr4ACgAgH81VdXLrs6Gz6hnnnkmL7/8cqYefEalRwFYJ/67etmbL6z5KcgAJXnmmQZJPvkSj9V+K2EAAIBPgzgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAoQv1KDwDryuUt5lR6BIB14r9r/rRrBacAWJeeWaVbOXICAIXZdNNNKz0CQEU4csLnQuvWrfPmf7Wv9BgA68RmF/41m266aeY++q1KjwKwTrz8cre0bt36E2/nyAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAAEARxAkAAFCE+pUeAKBU/5j/QXa+5InccdxO6fF/NqnZ/uWRT2fKS+8sd/vHTu2U3bZruh4nBFhzP7v1qVx+4+N56dV3sv1WTTPo610y8KjOqaqqqvRo/AsTJ6xX1dXVFj0+E2a+9UEO/Nmzefv9JbW2V1dX5+nX3stp3bdOv06b1dq30xaN1+eIAGvsml89lZPOuzenHN0lfff9Yh56fGa+++NJef+DD3PG8V0rPR7/wsQJ683zzz+f8847LzfffHOlR4E6LV1anbGPv5Gz7p6R6hXsf2Hu+3n3gyU5sN2m2aN1s/U+H8C6cP3tz2SfXbfNiP/aL0my756tM33GvIy+6UlxQkW55oT1ZuLEiXnyyScrPQas1NOvvZeBt/89R++2RW48asfl9v/l1feSJLtss/H6Hg1gnXn/gw/TrEmDWts23aRR5s5fWKGJ4CPiBOBjtm+xUaafu1uG9W2Txhsuv0Q+Neu9NNmoXv7z7hnZ4rxH0/jsKfnKz/6a/3ljQQWmBVgz3z1mt9z7xxm56a6/5u13P8i9D83I2F8/m6/3bV/p0fgXV1Sc9OrVK0OGDMmxxx6bnXfeOT/4wQ8yf/78nH/++dlrr73SsWPHHH744XnkkUdq3W/RokW57LLLsu+++2bnnXfOQQcdlF//+te1Hvecc86pdZ877rgjbdu2zT/+8Y8kyciRI9O7d++MGjUqXbt2zT777JO33357hTMlWaW52rZtm3HjxuUHP/hBunbtms6dO+d73/te3nzzzVq3u/POO3PooYemU6dO6dGjR4YNG5ZFixbV7J8+fXpOOumkdOnSJV26dMmgQYMyc+bMVfqeTpo0KYcddlg6duyYvffeOxdeeGEWLPjoRdQTTzyRtm3bZvLkybXuM23atLRt2za///3vkyQffPBBfvrTn6Z79+7p0KFDDj744EyYMGG5/3cjRozIxRdfnL322is777xzTjjhhLz00ks1399Ro0bVfF9GjhyZJJkyZUoOP/zwdO7cObvvvntOPvnkvPDCC6v03ODTsGnjDbPtJhvVuf8vr/4z//xgSVo0qp/bj9spVx/+xfz9zYXpPvqZzHr7g/U4KcCaO/IrO+Xovu1zzFnj02K3y3PgN3+Vvbtsm8u+36vSo/Evrqg4SZJx48alY8eOueKKK9KvX78ce+yxue+++3Laaadl1KhRadWqVb75zW/WCoEzzzwz119/ffr375+rrroq++yzT84555zcc889q/W1Z82alQceeCDDhw/Pueeem+bNm69wpg8++GCV5kqS4cOHZ+nSpbn00ktz1llnZfLkyRkyZEit53v22Wenffv2GTVqVE488cT8/Oc/z4UXXpgkmTFjRgYMGJC5c+fm4osvzuDBgzNz5swceeSRmTt37kqfz913351BgwalTZs2GT16dE455ZT85je/ycCBA1NdXZ0uXbpk++23z/jx42vd75577skmm2yS7t27p7q6OoMGDcrNN9+c4447LldeeWU6d+6c0047LXfeeWet+40dOzYvvvhiLrroolx44YV59tlnc/bZZydJ+vfvn379+iVJbrnllvTv3z8zZ87MwIED06FDh1x55ZUZPHhwZsyYkRNPPDFLly5drf93sL5c2Kd1Jg/smGF926Rbm+b5+q5b5LcndsjbCz/MiIdmVXo8gFXy/w28I7dN/J9c/J/dM/nnAzLivP3y+LOzc/j37kp19YquuIP1o7gL4rfeeuuceeaZSZJbb701f/vb33LrrbemU6dOSZIvf/nLOfroozN06NDcfvvtmT59eu699958//vfz7HHHpsk2XPPPfPqq6/msccey0EHHbTKX/vDDz/M2Wefnd12263OmVZ1rmV23HHHXHTRRTV/f/rppzNx4sQkydKlSzN69Ojst99+NTGSJAsXLsz48eOzePHijBo1Ko0aNcoNN9yQJk2a1Dy//fbbL9dcc03Ni///rbq6OkOHDk23bt0ydOjQmu077LBDvvGNb+SBBx5Ijx49csghh+S6667L+++/n4YNG6a6ujoTJkzIAQcckAYNGmTKlCl56KGHMnz48PTp0ydJ0q1btyxcuDBDhw7NQQcdlPr1P/oxatasWa644orUq1cvSfLKK69k5MiReeutt9KqVau0atUqSbLLLrskScaPH5/3338/J510UrbccsskSatWrXLfffdlwYIFNc8XStJp6+V/Ltu0bJidtmycp2a9V4GJAFbPw0+8mokPzcjVF/5Hvtn/o9cx3btunzbbNc9BJ96e8X94IQf1/D8VnpJ/VcUdOdlpp51q/vzII49k8803T/v27fPhhx/mww8/zJIlS9KzZ888++yzefvttzN16tQkyf7771/rcUaOHJkf//jHa/X169q2KnMts+yF+DKtWrXKwoUfXWw2Y8aMzJ07N7179651mxNOOCF33HFHNtxwwzz66KPp2rVrGjZsWPO1mjRpkt122y0PP/xwkmTJkiU1+z788MMsXbo0L774YmbPnp1evXrV2rf77runSZMmmTJlSpLkkEMOyYIFC2pO7XriiScya9as9O3bt+a5VlVVpXv37rUep1evXpkzZ06ef/75mrk7duxYEybLnmuSmuf7v3Xq1CkbbbRR+vXrl8GDB+ehhx5Ku3btctpppwkTivThkurc+OfX88gKPuNk4eKl2bzJhhWYCmD1vDzro9cpe3fZttb2L++2XZLkr8+v/MwM+DQVd+SkceP/9zkB8+fPz5w5c9K+/YovzpozZ07mz5+fJGnZsuU6+fobb7z8O/B8fKZVnWvZKWGNGjWqtW+DDTaoOVy6KrPPnz8/EyZMWO4ajyTZdNNNkyS9e/fOq6++WrP90EMPTf/+/ZMkP/rRj/KjH/1oufu+8cYbSZLWrVunc+fOGT9+fA488MCMHz8+22+/fbp06VLz9ZedArYib7zxRk28rei5JqnzFK1tt902N910U66++urcdtttGTt2bJo1a5ajjjoqp556qs9DoTj161Xlx797JVs32ygPfmfnmu1P/OOf+fubC/OfPbep4HQAq6Zdm49edzz0+Mzs9IX/9xpkyhMfvZZos13ziswFSYFx8nFNmzbNDjvsUOu0pI/bdttt06zZR58zMG/evJrf1CfJCy+8kPnz52fXXXdN8tHRhY9bdlH4pzXXqvj47B/31ltv5bnnnkvnzp3TtGnT7LXXXjnuuOOWu/+y06muvPLKWhfQt2jRouZoxVlnnZWuXZd/v/Jl8ZR8dPTkoosuyrvvvpuJEyfmyCOPrPVcGzdunLFjx67wObRu3XqVnmtddt5554waNSqLFi3K1KlTc8stt2TMmDFp165dDjzwwLV6bPg0nP8f2+e4Xz6fY3/xP/n6rlvk5bc+yA/vfSW7bLNxjt19y0qPB/CJOn9py3z1P3bMGT+ZnLfe/iD/3mmr/PXvb+ZHI6dk1/Zb5tDey7+NOqwvxZ3W9XFdu3bNa6+9lpYtW6Zjx441/02ZMiXXXHNN6tWrVxMf999/f637Dh06NIMHD06SNGnSJLNnz661f9npYJ/WXKuiTZs2adGixXLvlnXXXXflxBNPzOLFi9O1a9f8/e9/z0477VTzdTp06JAbbrih5t202rZtW2uObbfdNm3atEnLli3zj3/8o9a+LbfcMsOGDctzzz1X8/X69OmT6urqXH755Zk7d24OOeSQWs91wYIFqa6urvU406dPz+jRo/Phhx+u8vdt2ZGUZW644Yb07NkzixYtSoMGDbLnnnvWnIo3a5YLiynTMbttmV8e3TbTXl+Yw26Ylv/67cs56Eub5t6TOqTeBo72AZ8N44YenNO+sVuuuvkvOeCEX+XyGx/PNw7rmMk/PzL16xf98pDPuaKPnBx22GG56aabctxxx+Xb3/52ttpqqzz88MP52c9+lq9//evZcMMN065duxxwwAG55JJL8v7772ennXbKgw8+mMmTJ9e8dW3Pnj1z1VVX5aqrrkqnTp1y//3359FHH/1U51oV9erVy3e+851ccMEFadmyZXr16pUZM2ZkxIgR+drXvpbmzZtn4MCBGTBgQE466aQceeSR2WijjXLLLbdk0qRJGTFixEof+7TTTsv555+fevXqpWfPnnnnnXdyxRVX5PXXX691Stqyd+b6xS9+kc6dO9c6GtK9e/fsvvvuGThwYAYOHJgvfOELefrppzNixIh069at5tSyVbHsSNE999yTTp06ZY899sjQoUMzaNCgfP3rX0+9evVy8803p0GDBunZs+cqPy58Wnr8n02yZNg+y20/fJfNc/gum1dgIoB1o0GDernge91ywfe6VXoUqKXoOGncuHHGjRuXYcOG5ZJLLsm7776bbbbZJmeccUaOP/74mttdcsklGTVqVG688ca89dZb+cIXvpARI0Zkv/32S5KcdNJJmTdvXq699tosXrw4PXr0yODBg3PyySd/qnOtiq997Wtp3Lhxrr322txyyy1p1apVvvWtb+Vb3/pWkqRdu3YZN25chg8fnrPOOivV1dXZcccdM3r06Oy7774rfez+/ftn4403zjXXXJNbbrkljRs3TpcuXTJ06NBst912tW7bt2/fTJo0KQcffHCt7RtssEGuvvrqXH755bnqqqsyd+7cbLnlljnuuOMyaNCg1Xqu+++/f+66666cc8456devX374wx9mzJgxGT16dE4//fQsWbIkHTp0yHXXXZc2bdqs1mMDAPDZV1Xtzaz5jHvmmWeSJO1/P7DCkwCsG5td+NckydxHv1XhSQDWjfHTu6V169bp2LHjSm/npEIAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAI4gQAACiCOAEAAIogTgAAgCKIEwAAoAjiBAAAKII4AQAAiiBOAACAIogTAACgCOIEAAAogjgBAACKIE4AAIAiiBMAAKAIVdXV1dWVHgLWxhNPPJHq6uo0aNCg0qMArBMvv/xypUcAWKc233zzbLjhhunSpctKb1d/Pc0Dn5qqqqpKjwCwTrVu3brSIwCsU4sXL16l12yOnAAAAEVwzQkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEcQJAABQBHECAAAUQZwAAABFECcAn2DBggU1f7733ntz/fXX56WXXqrcQABrybpGqcQJQB1efPHF9O7dO1dffXWS5LLLLsupp56aiy++OH379s3UqVMrPCHA6rGuUTpxAlCHoUOHpn79+tl3332zaNGi/OIXv8iBBx6Yxx9/PN26dctll11W6REBVot1jdKJE4A6PP744znjjDPSsWPH/OlPf8q7776bI444Ik2aNMmAAQPy7LPPVnpEgNViXaN04gSgDosXL06zZs2SJA8++GAaNWqUXXfdNUmyZMmS1K9fv5LjAaw26xqlEycAddhxxx3zu9/9LnPmzMnEiROzzz77pH79+lm8eHHGjRuXHXfcsdIjAqwW6xqlq6qurq6u9BAAJZoyZUoGDRqUDz74IA0aNMhNN92Ujh07plevXnnzzTczZsyY7LXXXpUeE2CVWdconTgBWImZM2fmmWeeSadOnbLNNtskSW688cbsscceadu2bYWnA1h91jVK5rQugDqMGjUqDRo0SJ8+fWr+AU+SY489NhtvvHEuuOCCCk4HsPqsa5ROnADUYfTo0Xn99ddXuO+pp57Kr371q/U8EcDasa5ROm/JAPAxAwYMyFNPPZUkqa6uzhFHHFHnbTt27Li+xgJYY9Y1PktccwLwMX//+98zceLEVFdXZ/To0enXr19atWpV6zYbbLBBmjVrlv333z9bbLFFhSYFWDXWNT5LxAlAHUaNGpX+/ftnyy23rPQoAOuEdY3SiROAT/D2229n4cKFWbp06XL7tt566wpMBLB2rGuUyjUnAHV45ZVXctZZZ9Wcq70i06ZNW48TAawd6xqlEycAdbjgggvy0ksv5ZRTTkmrVq2ywQbe4BD4bLOuUTqndQHUoVOnThk8eHAOOuigSo8CsE5Y1yidXAaoQ5MmTdK8efNKjwGwzljXKJ04AahD3759M27cuDjADHxeWNconWtOAOrQqFGjTJ06Nb17907Hjh3TsGHDWvurqqoyZMiQCk0HsPqsa5TONScAdejVq9dK91dVVeW+++5bT9MArD3rGqUTJwAAQBFccwLwCZYuXZq//e1vefDBB/PPf/4z8+fPr/RIAGvFukapXHMCsBJ33XVXhg0bljfeeCNVVVW57bbbMnLkyGy44YYZNmxYGjRoUOkRAVaLdY2SOXICUIcJEybk7LPPzh577JHhw4fXvLtN796988ADD+SKK66o8IQAq8e6RukcOQGow5gxYzJgwID88Ic/zJIlS2q2f/WrX828efNy66235tRTT63cgACrybpG6Rw5AajDjBkz0rt37xXu69SpU15//fX1PBHA2rGuUTpxAlCHli1b5oUXXljhvhdeeCEtW7ZczxMBrB3rGqUTJwB16NOnT0aMGJGJEydm0aJFST76DIBnn302V1xxRQ444IAKTwiweqxrlM7nnADUYdGiRRk4cGD++Mc/ZoMNNsjSpUuz8cYbZ8GCBdltt93ys5/9bLlPVwYomXWN0okTgE8wZcqUPProo5k/f36aNm2arl27pnv37qmqqqr0aABrxLpGqcQJQB0mTZqUHj16pH59b2wIfD5Y1yidOAGoQ7t27bLJJpukT58+6du3bzp16lTpkQDWinWN0okTgDpMmzYt99xzT37729/mtddey/bbb59DDjkkhxxySLbbbrtKjwew2qxrlE6cAKyCqVOnZvz48bn33nszb9687LLLLunbt28GDBhQ6dEA1oh1jRKJE4DV8M9//jOXXXZZfvnLX2bp0qWZNm1apUcCWCvWNUriaiiAT7Bo0aJMnjw548ePzwMPPJClS5emZ8+e6du3b6VHA1gj1jVK5cgJQB0eeOCBjB8/Pvfdd1/ee++9mlMe+vTpk+bNm1d6PIDVZl2jdOIEoA7t2rWrdbHo9ttvX+mRANaKdY3SiROAOjz55JPp3LlzpccAWGesa5ROnAB8ggceeCAPP/xw3njjjZx++umZNm1a2rdvn2222abSowGsEesapXJBPEAdFi5cmEGDBuXhhx9OkyZN8t577+Wb3/xmfvnLX+a5557LTTfdlC9+8YuVHhNglVnXKN0GlR4AoFSXXnpp/vrXv+aGG27Io48+mmUHmi+++OJsueWWufzyyys8IcDqsa5ROnECUIff/va3Of3007PHHnukqqqqZvsWW2yRk08+OVOnTq3gdACrz7pG6cQJQB3eeeedOs+/bt68eRYsWLCeJwJYO9Y1SidOAOrwxS9+MXffffcK991///3OywY+c6xrlM4F8QB1OPnkk3PKKadk/vz56dmzZ6qqqvLnP/85d9xxR26++eYMGzas0iMCrBbrGqXzVsIAK3H33Xdn2LBhmT17ds22li1b5tRTT03//v0rOBnAmrGuUTJxArAKXnzxxcyfPz/NmjVLmzZtssEGzooFPtusa5TITyHAKvi3f/u3/OpXv0rjxo39Aw58LljXKJGfRIBVsHTp0tx555156623Kj0KwDphXaNE4gRgFTkLFvi8sa5RGnECAAAUQZwArIKqqqpsvfXWadCgQaVHAVgnrGuUyLt1AQAARfAhjAArMW/evFx77bV5+OGHM2fOnFxzzTWZNGlS2rVrl/3226/S4wGsNusaJXNaF0AdZs6cmUMOOSS33nprttxyy8ydOzdLlizJjBkz8t3vfjd/+MMfKj0iwGqxrlE6R04A6nDxxRenZcuW+fnPf57GjRunQ4cOSZJhw4blgw8+yJgxY9KjR4/KDgmwGqxrlM6RE4A6PPLIIxk4cGCaNWuWqqqqWvuOOOKIPP/88xWaDGDNWNconTgBWIn69Vd8gHnRokXL/cMO8FlgXaNk4gSgDrvttluuuuqqLFiwoGZbVVVVli5dml/+8pfp0qVLBacDWH3WNUrnrYQB6jB9+vQceeSRadSoUf793/89EyZMSJ8+ffLCCy/k5Zdfzi9+8YvstNNOlR4TYJVZ1yidOAFYiRkzZmTUqFF57LHHMn/+/DRt2jS77757Bg0alLZt21Z6PIDVZl2jZOIE4BMsWbIk9erVS5IsXLgwH374YZo2bVrhqQDWnHWNUrnmBKAOixcvzn//93/n8MMPr9n25JNPZs8998zFF1+cpUuXVnA6gNVnXaN04gSgDiNHjsxvfvObfOUrX6nZ9qUvfSlnnnlmbr311lxzzTUVnA5g9VnXKJ3TugDq0LNnz5x00kkZMGDAcvtuuummjB07Nr/73e8qMBnAmrGuUTpHTgDq8NZbb2W77bZb4b42bdpk9uzZ63kigLVjXaN04gSgDm3atMm99967wn33339/WrduvZ4nAlg71jVKt+KPCAUgxxxzTM4555zMnz8/++23X1q2bJl58+Zl8uTJ+e1vf5uLLrqo0iMCrBbrGqVzzQnASowbNy5XXHFF5s6dW7OtRYsW+c53vpOjjjqqgpMBrBnrGiUTJwCfoLq6OjNmzMj8+fPTrFmztGnTJhts4KxY4LPLukapxAkAAFAE15wA1GHevHkZPHhw/vCHP2ThwoX537/LqaqqynPPPVeh6QBWn3WN0okTgDpccMEFmTx5cr7yla+kVatWTnkAPvOsa5TOaV0AdejSpUvOPvvsHHHEEZUeBWCdsK5ROrkMUIcNN9ywzg8rA/gssq5ROnECUIfevXvnnnvuqfQYAOuMdY3SueYEoA5f+tKXctlll2XmzJnp1KlTGjZsWGt/VVVVBg0aVKHpAFafdY3SueYEoA7t2rVb6f6qqqpMmzZtPU0DsPasa5ROnAAAAEVwzQnAKnj33XfzwgsvZNGiRVmyZEmlxwFYa9Y1SiROAFbiscceS//+/dO1a9ccfPDBef7553PGGWfkJz/5SaVHA1gj1jVKJk4A6vDII4/khBNOSMOGDXPmmWfWfJJyu3btMnbs2Fx//fUVnhBg9VjXKJ1rTgDqcMQRR6RVq1a5/PLL8+GHH6ZDhw65/fbb0759+1x66aWZNGlSJkyYUOkxAVaZdY3SOXICUIdp06blq1/9apKP3sHm4/bee++8+uqrlRgLYI1Z1yidOAGoQ9OmTTNnzpwV7nvttdfStGnT9TwRwNqxrlE6cQJQh3333TfDhw/PM888U7Otqqoqs2fPzpgxY9KjR4/KDQewBqxrlM41JwB1ePvtt3PMMcdk+vTp2WyzzTJnzpzssMMOmT17drbaaquMGzcum266aaXHBFhl1jVKJ04AVmLRokW588478+ijj2b+/Plp2rRpunbtmsMOOyyNGjWq9HgAq826RsnECUAdzjvvvPTr1y+dOnWq9CgA64R1jdK55gSgDr/5zW/y3nvvVXoMgHXGukbpxAlAHTp37pzHHnus0mMArDPWNUpXv9IDAJSqbdu2ufbaazNx4sS0a9cujRs3rrW/qqoqQ4YMqdB0AKvPukbpXHMCUIdevXqtdH9VVVXuu+++9TQNwNqzrlE6cQJQh3feeSfNmjWr9BgA64x1jdK55gSgDl/5ylcyYcKESo8BsM5Y1yidOAGow6JFi9KiRYtKjwGwzljXKJ0L4gHqcMwxx+Syyy5Lw4YN065dOx9OBnzmWdconWtOAOqw//77Z9asWVmyZMkK91dVVeW5555bz1MBrDnrGqVz5ASgDoccckilRwBYp6xrlM6REwAAoAiOnADUYdasWZ94m6233no9TAKwbljXKJ0jJwB1aNeuXaqqqlZ6m2nTpq2naQDWnnWN0jlyAlCHIUOGLPeP+IIFC/L444/nsccey5AhQyo0GcCasa5ROkdOANbARRddlDfffDPDhg2r9CgA64R1jRL4EEaANdCrV6/84Q9/qPQYAOuMdY0SiBOANfDUU0+lfn1nxgKfH9Y1SuAnEKAO55577nLbli5dmtmzZ+fPf/5z+vXrV4GpANacdY3SueYEoA69evVabltVVVWaNGmSHj165Nvf/nYaNWpUgckA1ox1jdKJEwAAoAiuOQFYiQkTJuT888+v+fsTTzyRfv365f7776/gVABrzrpGycQJQB3uvPPOnH766Zk/f37Ntk022SSbb755TjnllEyaNKlywwGsAesapXNaF0AdDj744Oyzzz45++yzl9t38cUX57HHHssdd9xRgckA1ox1jdI5cgJQh1deeSXdu3df4b4vf/nLefHFF9fzRABrx7pG6cQJQB0233zzPP300yvc97e//S0tWrRYzxMBrB3rGqXzOScAdTjooINy5ZVXpnHjxundu3c23XTTzJs3L5MnT87IkSNz9NFHV3pEgNViXaN0rjkBqMPixYtzxhln5He/+12qqqpqtldXV+eAAw7I0KFDfZoy8JliXaN04gTgE0yfPj1Tp07N22+/naZNm2bXXXdNu3btKj0WwBqzrlEqcQKwCt5999288cYb2W677VKvXr3Uq1ev0iMBrBXrGiVyQTzASjz22GPp379/unbtmoMPPjjPP/98zjjjjPzkJz+p9GgAa8S6RsnECUAdHnnkkZxwwglp2LBhzjzzzCw70NyuXbuMHTs2119/fYUnBFg91jVK57QugDocccQRadWqVS6//PJ8+OGH6dChQ26//fa0b98+l156aSZNmpQJEyZUekyAVWZdo3SOnADUYdq0afnqV7+aJLXe1SZJ9t5777z66quVGAtgjVnXKJ04AahD06ZNM2fOnBXue+2119K0adP1PBHA2rGuUTpxAlCHfffdN8OHD88zzzxTs62qqiqzZ8/OmDFj0qNHj8oNB7AGrGuUzjUnAHV4++23c8wxx2T69OnZbLPNMmfOnOywww6ZPXt2ttpqq4wbNy6bbrpppccEWGXWNUonTgBWYtGiRbnzzjvz6KOPZv78+WnatGm6du2aww47LI0aNar0eACr5bzzzstXv/rVTJ8+3bpGkcQJQB3OO++89OvXL506dar0KADrRKdOnXLllVdmr732qvQosEKuOQGow29+85u89957lR4DYJ3p3LlzHn300UqPAXWqX+kBAErVuXPnPPbYY37DCHxutG3bNtddd13uvffetGvXLo0bN661v6qqKkOGDKnQdCBOAOrUtm3bXHvttZk4caJ/xIHPhd///vfZYostsnjx4lrv2LXM//7sE1jfXHMCUIdevXqtdH9VVVXuu+++9TQNAHz+iRMAAKAILogHWAXV1dUZNWpUnZ+sDACsPXECsAqWLl2a0aNH54033qj0KADwuSVOAFaRs2AB4NMlTgBWkXexAYBPlzgBWEWOnADAp8u7dQEAAEXwIYwAKzFv3rxcd911+dOf/pR33nknLVq0yG677ZZvfOMbadmyZaXHA4DPFUdOAOowe/bsDBgwIHPnzs0uu+ySzTffPHPmzMmTTz6ZFi1a5LbbbsuWW25Z6TEB4HPDkROAOlxyySWpV69eJkyYkO22265m+8yZM3P88cdn+PDh+clPflLBCQHg88UF8QB1+OMf/5jvfve7tcIkSbbbbrsMGjQoDz74YIUmA4DPJ3ECUIclS5akRYsWK9y36aab5p///Od6nggAPt/ECUAd2rZtm7vvvnuF++66667suOOO63kiAPh8c80JQB0GDhyYE044IW+//Xb69OlTc0H8+PHj88c//jEjRoyo9IgA8Lni3boAVuLOO+/M0KFD8+abb9Zs22yzzXLGGWfk0EMPreBkAPD5I04APsH8+fPz/PPPp379+mnevHkaNGiQDTb46KzYrbfeusLTAcDnh9O6AOrw8ssv5+yzz85TTz1V522mTZu2HicCgM83cQJQhx//+Md56aWXcsopp6RVq1Y1R0sAgE+H07oA6tCpU6cMHjw4Bx10UKVHAYB/CX4NCFCHJk2apHnz5pUeAwD+ZYgTgDr07ds348aNiwPMALB+uOYEoA6NGjXK1KlT07t373Ts2DENGzastb+qqipDhgyp0HQA8PnjmhOAOvTq1Wul+6uqqnLfffetp2kA4PNPnAAAAEVwzQkAAFAEcQIAABRBnAAAAEUQJwAAQBHECQAAUARxAgAAFEGcAAAARRAnAABAEf5/vguQiHiCf0UAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "cm = ConfusionMatrix(best_model, classes=labels)\n",
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
